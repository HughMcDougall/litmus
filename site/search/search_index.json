{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"doctest/","title":"Welcome to MkDocs","text":""},{"location":"doctest/#litmus.fitting_methods","title":"litmus.fitting_methods","text":"<p>Contains fitting procedures to be executed by the litmus class object</p> <p>HM 24</p>"},{"location":"doctest/#litmus.fitting_methods.has_jaxns","title":"has_jaxns  <code>module-attribute</code>","text":"<pre><code>has_jaxns = find_spec('jaxns') is not None\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.has_polychord","title":"has_polychord  <code>module-attribute</code>","text":"<pre><code>has_polychord = find_spec('pypolychord') is not None\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.correl_jax_jitted","title":"correl_jax_jitted  <code>module-attribute</code>","text":"<pre><code>correl_jax_jitted = jit(\n    correl_jax, static_argnames=[\"Nterp\"]\n)\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.correlfunc_jax_vmapped","title":"correlfunc_jax_vmapped  <code>module-attribute</code>","text":"<pre><code>correlfunc_jax_vmapped = vmap(\n    correlfunc_jax,\n    in_axes=(0, None, None, None, None, None),\n)\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.correl_func_boot_jax","title":"correl_func_boot_jax  <code>module-attribute</code>","text":"<pre><code>correl_func_boot_jax = jit(\n    correl_func_boot_jax,\n    static_argnames=[\"Nterp\", \"N1\", \"N2\"],\n)\n</code></pre> <p>Finds the best fit lag for a single bootstrapped (sub-sampled &amp; jittered) linterp correlation function</p>"},{"location":"doctest/#litmus.fitting_methods.correl_func_boot_jax_nomap","title":"correl_func_boot_jax_nomap  <code>module-attribute</code>","text":"<pre><code>correl_func_boot_jax_nomap = jit(\n    correl_func_boot_jax,\n    static_argnames=[\"Nterp\", \"N1\", \"N2\"],\n)\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.true_lag","title":"true_lag  <code>module-attribute</code>","text":"<pre><code>true_lag = lag\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.Nlags","title":"Nlags  <code>module-attribute</code>","text":"<pre><code>Nlags = 512\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.lags","title":"lags  <code>module-attribute</code>","text":"<pre><code>lags = linspace(-ptp() / 2, ptp() / 2, Nlags)\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.correls_jax","title":"correls_jax  <code>module-attribute</code>","text":"<pre><code>correls_jax = correlfunc_jax_vmapped(\n    lags, X1, Y1, X2, Y2, Nterp\n)\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.jax_samples","title":"jax_samples  <code>module-attribute</code>","text":"<pre><code>jax_samples = correl_func_boot_jax_wrapper_nomap(\n    lags, X1, Y1, X2, Y2, E1, E2, Nterp=Nterp, Nboot=Nboot\n)\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.z_med","title":"z_med  <code>module-attribute</code>","text":"<pre><code>z_med = res_med - true_lag / res_p2 - res_med\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.range","title":"range  <code>module-attribute</code>","text":"<pre><code>range = array([-true_lag, true_lag * 3])\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.mock","title":"mock  <code>module-attribute</code>","text":"<pre><code>mock = mock(seed=2, lag=100, season=0)\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.mock01","title":"mock01  <code>module-attribute</code>","text":"<pre><code>mock01 = lc_1\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.mock02","title":"mock02  <code>module-attribute</code>","text":"<pre><code>mock02 = lc_2\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.lag_true","title":"lag_true  <code>module-attribute</code>","text":"<pre><code>lag_true = lag\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.test_statmodel","title":"test_statmodel  <code>module-attribute</code>","text":"<pre><code>test_statmodel = dummy_statmodel()\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.test_ICCF","title":"test_ICCF  <code>module-attribute</code>","text":"<pre><code>test_ICCF = ICCF(\n    Nboot=128,\n    Nterp=1024,\n    Nlags=1024,\n    stat_model=test_statmodel,\n    debug=True,\n)\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.ICCF_samples","title":"ICCF_samples  <code>module-attribute</code>","text":"<pre><code>ICCF_samples = get_samples()['lag']\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.test_prior_sampler","title":"test_prior_sampler  <code>module-attribute</code>","text":"<pre><code>test_prior_sampler = prior_sampling(\n    stat_model=test_statmodel\n)\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.test_samples","title":"test_samples  <code>module-attribute</code>","text":"<pre><code>test_samples = get_samples(512, importance_sampling=True)\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.fitting_procedure","title":"fitting_procedure","text":"<p>               Bases: <code>logger</code></p> <p>Generic class for lag fitting procedures. Contains parent methods for setting properties</p> Source code in <code>litmus/fitting_methods.py</code> <pre><code>class fitting_procedure(logger):\n    \"\"\"\n    Generic class for lag fitting procedures. Contains parent methods for setting properties\n    \"\"\"\n\n    def __init__(self, stat_model: stats_model,\n                 out_stream=sys.stdout,\n                 err_stream=sys.stderr,\n                 verbose=True,\n                 debug=True,\n                 **fit_params):\n\n        logger.__init__(out_stream=out_stream,\n                        err_stream=err_stream,\n                        verbose=verbose,\n                        debug=debug,\n                        )\n\n        if not hasattr(self, \"_default_params\"):\n            self._default_params = {}\n\n        self.stat_model = stat_model\n\n        logger.__init__(self)\n\n        self.name = \"Base Fitting Procedure\"\n\n        self.is_ready = False\n        self.has_prefit = False\n        self.has_run = False\n\n        self.fitting_params = {} | self._default_params\n        self.set_config(**(self._default_params | fit_params))\n\n        self.seed = _utils.randint() if \"seed\" not in fit_params.keys() else fit_params['seed']\n        self._tempseed = self.seed\n        self._data = None\n\n    # ----------------------\n    def __getattribute__(self, key):\n        \"\"\"\n        Trying to set or get anything with a key in `fitting_params` or `results` will re-direct straight\n        to the corresponding dict entry.\n        \"\"\"\n        if key not in [\"_default_params\", \"fitting_params\"] \\\n                and hasattr(self, \"_default_params\") \\\n                and hasattr(self, \"fitting_params\") \\\n                and key in self._default_params.keys():\n            return self.fitting_params[key]\n        else:\n            return super().__getattribute__(key)\n\n    def __setattr__(self, key, value):\n        \"\"\"\n        Trying to set or get anything with a key in `fitting_params` or `results` will re-direct straight\n        to the corresponding dict entry.\n        \"\"\"\n        if key not in [\"_default_params\", \"fitting_params\"] \\\n                and hasattr(self, \"_default_params\") \\\n                and hasattr(self, \"fitting_params\") \\\n                and key in self._default_params.keys():\n            self.fitting_params[key] = value\n            self.is_ready = False\n            if self.has_run: self.msg_err(\n                \"Warning! Fitting parameter changed after a run. Can lead to unusual behaviour.\")\n        else:\n            super().__setattr__(key, value)\n\n    # ----------------------\n\n    def reset(self) -&gt; None:\n        \"\"\"\n        Clears all memory and resets params to defaults\n        \"\"\"\n        self.set_config(**self._default_params)\n\n        self.has_run, self.is_ready = False, False\n\n        return\n\n    def set_config(self, **fit_params) -&gt; None:\n        \"\"\"\n        Configure fitting parameters for fitting_method() object\n        Accepts any parameters present with a name in fitting_method.fitting_params\n        Unlisted parameters will be ignored.\n        \"\"\"\n\n        if self.debug: print(\"Doing config with keys\", fit_params.keys())\n\n        badkeys = [key for key in fit_params.keys() if key not in self._default_params.keys()]\n\n        for key, val in zip(fit_params.keys(), fit_params.values()):\n            if key in badkeys: continue\n\n            # If something's changed, flag as having not run\n            currval = self.__getattribute__(key)\n            if self.has_run and val != currval: self.has_run = False\n\n            self.__setattr__(key, val)\n            # self.fitting_params |= {key: val}\n            if self.debug: print(\"\\t set attr\", key, file=self.out_stream)\n\n        if len(badkeys) &gt; 0:\n            self.msg_err(\"Tried to configure bad keys:\", *badkeys, delim='\\t')\n        return\n\n    def readyup(self) -&gt; None:\n        \"\"\"\n        Performs pre-fit preparation calcs. Should only be called if not self.is_ready()\n        \"\"\"\n        self.is_ready = True\n\n    # ----------------------\n    # Main methods\n\n    def prefit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None) -&gt; None:\n        \"\"\"\n        Fit lags\n        :param lc_1: Lightcurve 1 (Main)\n        :param lc_2: Lightcurve 2 (Response)\n        :param seed: A random seed for feeding to the fitting process. If none, will select randomly\n        \"\"\"\n\n        self.has_prefit = True\n\n    def fit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None) -&gt; None:\n        \"\"\"\n        Fit lags\n        :param lc_1: Lightcurve 1 (Main)\n        :param lc_2: Lightcurve 2 (Response)\n        :param seed: A random seed for feeding to the fitting process. If none, will select randomly\n        \"\"\"\n\n        # Sanity checks inherited by all subclasses\n        if not self.is_ready: self.readyup()\n        if isinstance(seed, int):\n            self._tempseed = seed\n            self._tempseed = _utils.randint()\n            self._tempseed = _utils.randint()\n        seed = self._tempseed\n\n        data = self.stat_model.lc_to_data(lc_1, lc_2)\n        self._data = data\n\n        # An error message raised if this fitting procedure doesn't have .fit()\n        if self.__class__.fit == fitting_procedure.fit:\n            self.msg_err(\"Fitting \\\"%s\\\" method does not have method .fit() implemented\" % self.name)\n\n        return\n\n    def get_samples(self, N: int = None, seed: int = None, importance_sampling: bool = False) -&gt; {str: [float]}:\n        '''\n        Returns MCMC-like posterior samples\n        :param N: Number of samples to return. If None, return all\n        :param seed: Random seed for any stochastic elements\n        :param importance_sampling: If true, will weight the results by\n        :return:\n        '''\n\n        if not self.is_ready: self.readyup()\n        if isinstance(seed, int):\n            self._tempseed = seed\n            self._tempseed = _utils.randint()\n        seed = self._tempseed\n\n        if self.__class__.fit == fitting_procedure.fit:\n            self.msg_err(\"Fitting \\\"%s\\\" method does not have method .get_samples() implemented\" % self.name)\n\n    def get_evidence(self, seed: int = None, return_type='linear') -&gt; [float, float, float]:\n        \"\"\"\n        Returns the estimated evidence for the fit model.\n        if return_type = 'linear', returns as array-like [Z,-dZ-,dZ+]\n        if return_type = 'log', returns as array-like [logZ,-dlogZ-,dlogZ+]\n        \"\"\"\n\n        assert return_type in ['linear', 'log'], \"Return type must be 'linear' or 'log'\"\n\n        if not self.is_ready: self.readyup()\n        if not self.has_run: self.msg_err(\"Warning! Tried to call get_evidence without running first!\")\n\n        if isinstance(seed, int):\n            self._tempseed = seed\n            self._tempseed = _utils.randint()\n        seed = self._tempseed\n\n        if self.__class__.get_evidence == fitting_procedure.get_evidence:\n            self.msg_err(\"Fitting \\\"%s\\\" method does not have method .get_evidence() implemented\" % self.name)\n            return np.array([0.0, 0.0, 0.0])\n\n    def get_information(self, seed: int = None) -&gt; [float, float, float]:\n        \"\"\"\n        Returns an estimate of the information (KL divergence relative to prior). Returns as array-like [I,dI-,dI+]\n        \"\"\"\n\n        if not self.is_ready: self.readyup()\n        if isinstance(seed, int):\n            self._tempseed = seed\n            self._tempseed = _utils.randint()\n        seed = self._tempseed\n\n        if self.__class__.get_information == fitting_procedure.get_information:\n            self.msg_err(\"Fitting \\\"%s\\\" method does not have method .get_information() implemented\" % self.name)\n\n            return np.array([0.0, 0.0, 0.0])\n\n    def get_peaks(self, seed=None):\n        \"\"\"\n        Returns the maximum posterior position in parameter space\n        \"\"\"\n\n        if not self.is_ready: self.readyup()\n        if isinstance(seed, int):\n            self._tempseed = seed\n            self._tempseed = _utils.randint()\n        seed = self._tempseed\n\n        if self.__class__.get_peaks == fitting_procedure.get_peaks:\n            self.msg_err(\"Fitting \\\"%s\\\" method does not have method .get_peaks() implemented\" % self.name)\n\n            return {}, np.array([])\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.fitting_procedure.stat_model","title":"stat_model  <code>instance-attribute</code>","text":"<pre><code>stat_model = stat_model\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.fitting_procedure.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = 'Base Fitting Procedure'\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.fitting_procedure.is_ready","title":"is_ready  <code>instance-attribute</code>","text":"<pre><code>is_ready = False\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.fitting_procedure.has_prefit","title":"has_prefit  <code>instance-attribute</code>","text":"<pre><code>has_prefit = False\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.fitting_procedure.has_run","title":"has_run  <code>instance-attribute</code>","text":"<pre><code>has_run = False\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.fitting_procedure.fitting_params","title":"fitting_params  <code>instance-attribute</code>","text":"<pre><code>fitting_params = {} | _default_params\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.fitting_procedure.seed","title":"seed  <code>instance-attribute</code>","text":"<pre><code>seed = (\n    randint()\n    if \"seed\" not in keys()\n    else fit_params[\"seed\"]\n)\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.fitting_procedure.reset","title":"reset","text":"<pre><code>reset() -&gt; None\n</code></pre> <p>Clears all memory and resets params to defaults</p> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"\n    Clears all memory and resets params to defaults\n    \"\"\"\n    self.set_config(**self._default_params)\n\n    self.has_run, self.is_ready = False, False\n\n    return\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.fitting_procedure.set_config","title":"set_config","text":"<pre><code>set_config(**fit_params) -&gt; None\n</code></pre> <p>Configure fitting parameters for fitting_method() object Accepts any parameters present with a name in fitting_method.fitting_params Unlisted parameters will be ignored.</p> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def set_config(self, **fit_params) -&gt; None:\n    \"\"\"\n    Configure fitting parameters for fitting_method() object\n    Accepts any parameters present with a name in fitting_method.fitting_params\n    Unlisted parameters will be ignored.\n    \"\"\"\n\n    if self.debug: print(\"Doing config with keys\", fit_params.keys())\n\n    badkeys = [key for key in fit_params.keys() if key not in self._default_params.keys()]\n\n    for key, val in zip(fit_params.keys(), fit_params.values()):\n        if key in badkeys: continue\n\n        # If something's changed, flag as having not run\n        currval = self.__getattribute__(key)\n        if self.has_run and val != currval: self.has_run = False\n\n        self.__setattr__(key, val)\n        # self.fitting_params |= {key: val}\n        if self.debug: print(\"\\t set attr\", key, file=self.out_stream)\n\n    if len(badkeys) &gt; 0:\n        self.msg_err(\"Tried to configure bad keys:\", *badkeys, delim='\\t')\n    return\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.fitting_procedure.readyup","title":"readyup","text":"<pre><code>readyup() -&gt; None\n</code></pre> <p>Performs pre-fit preparation calcs. Should only be called if not self.is_ready()</p> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def readyup(self) -&gt; None:\n    \"\"\"\n    Performs pre-fit preparation calcs. Should only be called if not self.is_ready()\n    \"\"\"\n    self.is_ready = True\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.fitting_procedure.prefit","title":"prefit","text":"<pre><code>prefit(\n    lc_1: lightcurve, lc_2: lightcurve, seed: int = None\n) -&gt; None\n</code></pre> <p>Fit lags :param lc_1: Lightcurve 1 (Main) :param lc_2: Lightcurve 2 (Response) :param seed: A random seed for feeding to the fitting process. If none, will select randomly</p> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def prefit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None) -&gt; None:\n    \"\"\"\n    Fit lags\n    :param lc_1: Lightcurve 1 (Main)\n    :param lc_2: Lightcurve 2 (Response)\n    :param seed: A random seed for feeding to the fitting process. If none, will select randomly\n    \"\"\"\n\n    self.has_prefit = True\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.fitting_procedure.fit","title":"fit","text":"<pre><code>fit(\n    lc_1: lightcurve, lc_2: lightcurve, seed: int = None\n) -&gt; None\n</code></pre> <p>Fit lags :param lc_1: Lightcurve 1 (Main) :param lc_2: Lightcurve 2 (Response) :param seed: A random seed for feeding to the fitting process. If none, will select randomly</p> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def fit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None) -&gt; None:\n    \"\"\"\n    Fit lags\n    :param lc_1: Lightcurve 1 (Main)\n    :param lc_2: Lightcurve 2 (Response)\n    :param seed: A random seed for feeding to the fitting process. If none, will select randomly\n    \"\"\"\n\n    # Sanity checks inherited by all subclasses\n    if not self.is_ready: self.readyup()\n    if isinstance(seed, int):\n        self._tempseed = seed\n        self._tempseed = _utils.randint()\n        self._tempseed = _utils.randint()\n    seed = self._tempseed\n\n    data = self.stat_model.lc_to_data(lc_1, lc_2)\n    self._data = data\n\n    # An error message raised if this fitting procedure doesn't have .fit()\n    if self.__class__.fit == fitting_procedure.fit:\n        self.msg_err(\"Fitting \\\"%s\\\" method does not have method .fit() implemented\" % self.name)\n\n    return\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.fitting_procedure.get_samples","title":"get_samples","text":"<pre><code>get_samples(\n    N: int = None,\n    seed: int = None,\n    importance_sampling: bool = False,\n) -&gt; {str: [float]}\n</code></pre> <p>Returns MCMC-like posterior samples :param N: Number of samples to return. If None, return all :param seed: Random seed for any stochastic elements :param importance_sampling: If true, will weight the results by :return:</p> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def get_samples(self, N: int = None, seed: int = None, importance_sampling: bool = False) -&gt; {str: [float]}:\n    '''\n    Returns MCMC-like posterior samples\n    :param N: Number of samples to return. If None, return all\n    :param seed: Random seed for any stochastic elements\n    :param importance_sampling: If true, will weight the results by\n    :return:\n    '''\n\n    if not self.is_ready: self.readyup()\n    if isinstance(seed, int):\n        self._tempseed = seed\n        self._tempseed = _utils.randint()\n    seed = self._tempseed\n\n    if self.__class__.fit == fitting_procedure.fit:\n        self.msg_err(\"Fitting \\\"%s\\\" method does not have method .get_samples() implemented\" % self.name)\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.fitting_procedure.get_evidence","title":"get_evidence","text":"<pre><code>get_evidence(\n    seed: int = None, return_type=\"linear\"\n) -&gt; [float, float, float]\n</code></pre> <p>Returns the estimated evidence for the fit model. if return_type = 'linear', returns as array-like [Z,-dZ-,dZ+] if return_type = 'log', returns as array-like [logZ,-dlogZ-,dlogZ+]</p> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def get_evidence(self, seed: int = None, return_type='linear') -&gt; [float, float, float]:\n    \"\"\"\n    Returns the estimated evidence for the fit model.\n    if return_type = 'linear', returns as array-like [Z,-dZ-,dZ+]\n    if return_type = 'log', returns as array-like [logZ,-dlogZ-,dlogZ+]\n    \"\"\"\n\n    assert return_type in ['linear', 'log'], \"Return type must be 'linear' or 'log'\"\n\n    if not self.is_ready: self.readyup()\n    if not self.has_run: self.msg_err(\"Warning! Tried to call get_evidence without running first!\")\n\n    if isinstance(seed, int):\n        self._tempseed = seed\n        self._tempseed = _utils.randint()\n    seed = self._tempseed\n\n    if self.__class__.get_evidence == fitting_procedure.get_evidence:\n        self.msg_err(\"Fitting \\\"%s\\\" method does not have method .get_evidence() implemented\" % self.name)\n        return np.array([0.0, 0.0, 0.0])\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.fitting_procedure.get_information","title":"get_information","text":"<pre><code>get_information(seed: int = None) -&gt; [float, float, float]\n</code></pre> <p>Returns an estimate of the information (KL divergence relative to prior). Returns as array-like [I,dI-,dI+]</p> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def get_information(self, seed: int = None) -&gt; [float, float, float]:\n    \"\"\"\n    Returns an estimate of the information (KL divergence relative to prior). Returns as array-like [I,dI-,dI+]\n    \"\"\"\n\n    if not self.is_ready: self.readyup()\n    if isinstance(seed, int):\n        self._tempseed = seed\n        self._tempseed = _utils.randint()\n    seed = self._tempseed\n\n    if self.__class__.get_information == fitting_procedure.get_information:\n        self.msg_err(\"Fitting \\\"%s\\\" method does not have method .get_information() implemented\" % self.name)\n\n        return np.array([0.0, 0.0, 0.0])\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.fitting_procedure.get_peaks","title":"get_peaks","text":"<pre><code>get_peaks(seed=None)\n</code></pre> <p>Returns the maximum posterior position in parameter space</p> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def get_peaks(self, seed=None):\n    \"\"\"\n    Returns the maximum posterior position in parameter space\n    \"\"\"\n\n    if not self.is_ready: self.readyup()\n    if isinstance(seed, int):\n        self._tempseed = seed\n        self._tempseed = _utils.randint()\n    seed = self._tempseed\n\n    if self.__class__.get_peaks == fitting_procedure.get_peaks:\n        self.msg_err(\"Fitting \\\"%s\\\" method does not have method .get_peaks() implemented\" % self.name)\n\n        return {}, np.array([])\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.ICCF","title":"ICCF","text":"<p>               Bases: <code>fitting_procedure</code></p> <p>Fit lags using interpolated cross correlation function in the style of pyCCF. Note that this is not a Bayesian fitter and gives only approximate measures of the lag todo     - Add p value, false positive and evidence estimates (?)</p> Source code in <code>litmus/fitting_methods.py</code> <pre><code>class ICCF(fitting_procedure):\n    \"\"\"\n    Fit lags using interpolated cross correlation function in the style of pyCCF.\n    Note that this is not a Bayesian fitter and gives only approximate measures of the lag\n    todo\n        - Add p value, false positive and evidence estimates (?)\n    \"\"\"\n\n    def __init__(self, stat_model: stats_model,\n                 out_stream=sys.stdout, err_stream=sys.stderr,\n                 verbose=True, debug=False, **fit_params):\n\n        args_in = {**locals(), **fit_params}\n        del args_in['self']\n        del args_in['__class__']\n        del args_in['fit_params']\n\n        if not hasattr(self, '_default_params'):\n            self._default_params = {}\n\n        self._default_params |= {\n            'Nboot': 512,\n            'Nterp': 2014,\n            'Nlags': 512,\n        }\n\n        super().__init__(**args_in)\n\n        self.name = \"ICCF Fitting Procedure\"\n        self.lags = np.zeros(self.Nterp)\n\n        # -----------------------------------\n        self.samples = np.zeros(self.Nboot)\n        self.correl_curve = np.zeros(self.Nterp)\n        self.lag_mean = 0.0\n        self.lag_err = 0.0\n\n    # -------------------------\n    def set_config(self, **fit_params):\n        super().set_config(**fit_params)\n\n    def readyup(self):\n        super().readyup()\n        '''\n        # self.lags = jnp.linspace(*self.stat_model.prior_ranges['lag'], self.Nlags)\n        self.lags = np.random.randn(self.Nlags) * self.stat_model.prior_ranges['lag'].ptp() + \\\n                    self.stat_model.prior_ranges['lag'][0]\n        '''\n        self.is_ready = True\n        self.has_prefit = False\n\n    # -------------------------\n    def fit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None):\n        # -------------------\n        fitting_procedure.fit(**locals())\n        seed = self._tempseed\n        # -------------------\n\n        # Unpack lightcurve\n        X1, Y1, E1 = lc_1.T, lc_1.Y, lc_1.E\n        X2, Y2, E2 = lc_2.T, lc_2.Y, lc_2.E\n\n        # Get interpolated correlation for all un-bootstrapped data\n        self.correls = correlfunc_jax_vmapped(self.lags, X1, Y1, X2, Y2, self.Nterp)\n\n        # Do bootstrap fitting\n        lagrange = jnp.linspace(*self.stat_model.prior_ranges['lag'], self.Nlags)\n        jax_samples = correl_func_boot_jax_wrapper_nomap(lagrange, X1, Y1, X2, Y2, E1, E2,\n                                                         Nterp=self.Nterp,\n                                                         Nboot=self.Nboot)\n\n        # Store Results\n        self.samples = jax_samples\n        self.lag_mean, self.lag_err = jax_samples.mean(), jax_samples.std()\n\n        self.has_run = True\n\n    def get_samples(self, N: int = None, seed: int = None, importance_sampling: bool = False) -&gt; {str: [float]}:\n        # -------------------\n        fitting_procedure.get_samples(**locals())\n        seed = self._tempseed\n\n        if importance_sampling:\n            self.msg_err(\"Warning! Cannot use important sampling with ICCF. Try implementing manually\")\n            return\n        # -------------------\n\n        # Return entire sample chain or sub-set of samples\n        if N is None:\n            return ({'lag': self.samples})\n        else:\n            if N &gt; self.Nboot:\n                self.msg_err(\n                    \"Warning, tried to get %i sub-samples from %i boot-strap iterations in ICCF\" % (N, self.Nboot),\n                )\n            return ({'lag': np.random.choice(a=self.samples, size=N, replace=True)})\n\n    def get_peaks(self, seed: int = None) -&gt; ({float: [float]}, [float]):\n        # -------------------\n        fitting_procedure.get_peaks(**locals())\n        seed = self._tempseed\n        # --------------\n        out = self.lags[np.argmax(self.correls)]\n        return ({'lag': np.array([out])})\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.ICCF.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = 'ICCF Fitting Procedure'\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.ICCF.lags","title":"lags  <code>instance-attribute</code>","text":"<pre><code>lags = zeros(Nterp)\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.ICCF.samples","title":"samples  <code>instance-attribute</code>","text":"<pre><code>samples = zeros(Nboot)\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.ICCF.correl_curve","title":"correl_curve  <code>instance-attribute</code>","text":"<pre><code>correl_curve = zeros(Nterp)\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.ICCF.lag_mean","title":"lag_mean  <code>instance-attribute</code>","text":"<pre><code>lag_mean = 0.0\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.ICCF.lag_err","title":"lag_err  <code>instance-attribute</code>","text":"<pre><code>lag_err = 0.0\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.ICCF.set_config","title":"set_config","text":"<pre><code>set_config(**fit_params)\n</code></pre> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def set_config(self, **fit_params):\n    super().set_config(**fit_params)\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.ICCF.readyup","title":"readyup","text":"<pre><code>readyup()\n</code></pre> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def readyup(self):\n    super().readyup()\n    '''\n    # self.lags = jnp.linspace(*self.stat_model.prior_ranges['lag'], self.Nlags)\n    self.lags = np.random.randn(self.Nlags) * self.stat_model.prior_ranges['lag'].ptp() + \\\n                self.stat_model.prior_ranges['lag'][0]\n    '''\n    self.is_ready = True\n    self.has_prefit = False\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.ICCF.fit","title":"fit","text":"<pre><code>fit(lc_1: lightcurve, lc_2: lightcurve, seed: int = None)\n</code></pre> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def fit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None):\n    # -------------------\n    fitting_procedure.fit(**locals())\n    seed = self._tempseed\n    # -------------------\n\n    # Unpack lightcurve\n    X1, Y1, E1 = lc_1.T, lc_1.Y, lc_1.E\n    X2, Y2, E2 = lc_2.T, lc_2.Y, lc_2.E\n\n    # Get interpolated correlation for all un-bootstrapped data\n    self.correls = correlfunc_jax_vmapped(self.lags, X1, Y1, X2, Y2, self.Nterp)\n\n    # Do bootstrap fitting\n    lagrange = jnp.linspace(*self.stat_model.prior_ranges['lag'], self.Nlags)\n    jax_samples = correl_func_boot_jax_wrapper_nomap(lagrange, X1, Y1, X2, Y2, E1, E2,\n                                                     Nterp=self.Nterp,\n                                                     Nboot=self.Nboot)\n\n    # Store Results\n    self.samples = jax_samples\n    self.lag_mean, self.lag_err = jax_samples.mean(), jax_samples.std()\n\n    self.has_run = True\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.ICCF.get_samples","title":"get_samples","text":"<pre><code>get_samples(\n    N: int = None,\n    seed: int = None,\n    importance_sampling: bool = False,\n) -&gt; {str: [float]}\n</code></pre> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def get_samples(self, N: int = None, seed: int = None, importance_sampling: bool = False) -&gt; {str: [float]}:\n    # -------------------\n    fitting_procedure.get_samples(**locals())\n    seed = self._tempseed\n\n    if importance_sampling:\n        self.msg_err(\"Warning! Cannot use important sampling with ICCF. Try implementing manually\")\n        return\n    # -------------------\n\n    # Return entire sample chain or sub-set of samples\n    if N is None:\n        return ({'lag': self.samples})\n    else:\n        if N &gt; self.Nboot:\n            self.msg_err(\n                \"Warning, tried to get %i sub-samples from %i boot-strap iterations in ICCF\" % (N, self.Nboot),\n            )\n        return ({'lag': np.random.choice(a=self.samples, size=N, replace=True)})\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.ICCF.get_peaks","title":"get_peaks","text":"<pre><code>get_peaks(seed: int = None) -&gt; ({float: [float]}, [float])\n</code></pre> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def get_peaks(self, seed: int = None) -&gt; ({float: [float]}, [float]):\n    # -------------------\n    fitting_procedure.get_peaks(**locals())\n    seed = self._tempseed\n    # --------------\n    out = self.lags[np.argmax(self.correls)]\n    return ({'lag': np.array([out])})\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.prior_sampling","title":"prior_sampling","text":"<p>               Bases: <code>fitting_procedure</code></p> <p>Randomly samples from the prior and weights with importance sampling. The crudest available sampler. For test purposes only, not suggested for actual use.</p> Source code in <code>litmus/fitting_methods.py</code> <pre><code>class prior_sampling(fitting_procedure):\n    \"\"\"\n    Randomly samples from the prior and weights with importance sampling.\n    The crudest available sampler. For test purposes only, not suggested for actual use.\n    \"\"\"\n\n    def __init__(self, stat_model: stats_model,\n                 out_stream=sys.stdout, err_stream=sys.stderr,\n                 verbose=True, debug=False, **fit_params):\n\n        # ------------------------------------\n        args_in = {**locals(), **fit_params}\n        del args_in['self']\n        del args_in['__class__']\n        del args_in['fit_params']\n\n        if not hasattr(self, '_default_params'):\n            self._default_params = {}\n        self._default_params |= {\n            'Nsamples': 4096\n        }\n\n        super().__init__(**args_in)\n        # ------------------------------------\n\n        self.name = \"Prior Sampling Fitting Procedure\"\n\n        self.samples = np.zeros(self.Nsamples)\n        self.log_likes = np.zeros(self.Nsamples)\n        self.weights = np.zeros(self.Nsamples)\n\n    # --------------\n    def fit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None):\n        # -------------------\n        fitting_procedure.fit(**locals())\n        seed = self._tempseed\n        # -------------------\n\n        # Generate samples &amp; calculate likelihoods todo - These currently give posterior densities and not log likes\n        data = self.stat_model.lc_to_data(lc_1, lc_2)\n        samples = self.stat_model.prior_sample(num_samples=self.Nsamples, seed=seed)\n        log_density = self.stat_model.log_density(data=data, params=samples)\n        log_prior = self.stat_model.log_prior(params=samples)\n        log_likes = log_density - log_prior\n        likes = np.exp(log_likes)\n\n        # Store results\n\n        self.log_prior = log_prior\n        self.log_likes = log_likes\n        self.log_density = log_density\n        self.samples = samples\n        self.weights = likes / likes.sum()\n\n        # Mark as having completed a run\n        self.has_run = True\n\n    def get_samples(self, N: int = None, seed: int = None, importance_sampling: bool = True) -&gt; {str: [float]}:\n        # -------------------\n        fitting_procedure.get_samples(**locals())\n        seed = self._tempseed\n        # -------------------\n\n        if N is None:\n            N = self.Nsamples\n        else:\n            if N &gt; self.Nsamples:\n                self.msg_err(\"Warning, tried to get %i sub-samples from %i samples\" % (N, self.Nsamples))\n\n        if importance_sampling:\n            weights = self.weights / self.weights.sum()\n        else:\n            weights = None\n\n        I = np.random.choice(a=np.arange(self.Nsamples), size=N, replace=True,\n                             p=weights)\n        return ({\n            key: val[I] for key, val in zip(self.samples.keys(), self.samples.values())\n        })\n\n    def get_evidence(self, seed=None, return_type='linear') -&gt; [float, float, float]:\n        # -------------------\n        fitting_procedure.get_evidence(**locals())\n        seed = self._tempseed\n        # -------------------\n        density = np.exp(self.log_density - self.log_density.max())\n\n        Z = density.mean() * self.stat_model.prior_volume * np.exp(self.log_density.max())\n        uncert = density.std() / np.sqrt(self.Nsamples) * self.stat_model.prior_volume\n\n        if return_type == 'linear':\n            np.array([Z, -uncert, uncert])\n        elif return_type == 'log':\n            np.array([np.log(Z), np.log(1 - uncert / Z), np.log(1 + uncert / Z)])\n\n    def get_information(self, seed: int = None) -&gt; [float, float, float]:\n        # -------------------\n        fitting_procedure.get_information(**locals())\n        seed = self._tempseed\n        # -------------------\n        info_partial = np.random.choice(self.log_density - self.log_prior, self.Nsamples,\n                                        p=self.weights)\n        info = info_partial.mean() * self.stat_model.prior_volume\n        uncert = info_partial.std() / np.sqrt(self.Nsamples) * self.stat_model.prior_volume\n\n        return (np.array([info, -uncert, uncert]))\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.prior_sampling.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = 'Prior Sampling Fitting Procedure'\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.prior_sampling.samples","title":"samples  <code>instance-attribute</code>","text":"<pre><code>samples = zeros(Nsamples)\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.prior_sampling.log_likes","title":"log_likes  <code>instance-attribute</code>","text":"<pre><code>log_likes = zeros(Nsamples)\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.prior_sampling.weights","title":"weights  <code>instance-attribute</code>","text":"<pre><code>weights = zeros(Nsamples)\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.prior_sampling.fit","title":"fit","text":"<pre><code>fit(lc_1: lightcurve, lc_2: lightcurve, seed: int = None)\n</code></pre> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def fit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None):\n    # -------------------\n    fitting_procedure.fit(**locals())\n    seed = self._tempseed\n    # -------------------\n\n    # Generate samples &amp; calculate likelihoods todo - These currently give posterior densities and not log likes\n    data = self.stat_model.lc_to_data(lc_1, lc_2)\n    samples = self.stat_model.prior_sample(num_samples=self.Nsamples, seed=seed)\n    log_density = self.stat_model.log_density(data=data, params=samples)\n    log_prior = self.stat_model.log_prior(params=samples)\n    log_likes = log_density - log_prior\n    likes = np.exp(log_likes)\n\n    # Store results\n\n    self.log_prior = log_prior\n    self.log_likes = log_likes\n    self.log_density = log_density\n    self.samples = samples\n    self.weights = likes / likes.sum()\n\n    # Mark as having completed a run\n    self.has_run = True\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.prior_sampling.get_samples","title":"get_samples","text":"<pre><code>get_samples(\n    N: int = None,\n    seed: int = None,\n    importance_sampling: bool = True,\n) -&gt; {str: [float]}\n</code></pre> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def get_samples(self, N: int = None, seed: int = None, importance_sampling: bool = True) -&gt; {str: [float]}:\n    # -------------------\n    fitting_procedure.get_samples(**locals())\n    seed = self._tempseed\n    # -------------------\n\n    if N is None:\n        N = self.Nsamples\n    else:\n        if N &gt; self.Nsamples:\n            self.msg_err(\"Warning, tried to get %i sub-samples from %i samples\" % (N, self.Nsamples))\n\n    if importance_sampling:\n        weights = self.weights / self.weights.sum()\n    else:\n        weights = None\n\n    I = np.random.choice(a=np.arange(self.Nsamples), size=N, replace=True,\n                         p=weights)\n    return ({\n        key: val[I] for key, val in zip(self.samples.keys(), self.samples.values())\n    })\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.prior_sampling.get_evidence","title":"get_evidence","text":"<pre><code>get_evidence(\n    seed=None, return_type=\"linear\"\n) -&gt; [float, float, float]\n</code></pre> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def get_evidence(self, seed=None, return_type='linear') -&gt; [float, float, float]:\n    # -------------------\n    fitting_procedure.get_evidence(**locals())\n    seed = self._tempseed\n    # -------------------\n    density = np.exp(self.log_density - self.log_density.max())\n\n    Z = density.mean() * self.stat_model.prior_volume * np.exp(self.log_density.max())\n    uncert = density.std() / np.sqrt(self.Nsamples) * self.stat_model.prior_volume\n\n    if return_type == 'linear':\n        np.array([Z, -uncert, uncert])\n    elif return_type == 'log':\n        np.array([np.log(Z), np.log(1 - uncert / Z), np.log(1 + uncert / Z)])\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.prior_sampling.get_information","title":"get_information","text":"<pre><code>get_information(seed: int = None) -&gt; [float, float, float]\n</code></pre> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def get_information(self, seed: int = None) -&gt; [float, float, float]:\n    # -------------------\n    fitting_procedure.get_information(**locals())\n    seed = self._tempseed\n    # -------------------\n    info_partial = np.random.choice(self.log_density - self.log_prior, self.Nsamples,\n                                    p=self.weights)\n    info = info_partial.mean() * self.stat_model.prior_volume\n    uncert = info_partial.std() / np.sqrt(self.Nsamples) * self.stat_model.prior_volume\n\n    return (np.array([info, -uncert, uncert]))\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.nested_sampling","title":"nested_sampling","text":"<p>               Bases: <code>fitting_procedure</code></p> <p>Fits the Bayesian model with Nested Sampling by using JAXNS. Highly accurate evidence / posterior distributions, but can be slow for models with more than a few parameters. Use only if hessian_scan and SVI_scan fail.</p> Source code in <code>litmus/fitting_methods.py</code> <pre><code>class nested_sampling(fitting_procedure):\n    \"\"\"\n    Fits the Bayesian model with Nested Sampling by using JAXNS. Highly accurate evidence / posterior distributions,\n    but can be slow for models with more than a few parameters. Use only if hessian_scan and SVI_scan fail.\n    \"\"\"\n\n    def __init__(self, stat_model: stats_model,\n                 out_stream=sys.stdout, err_stream=sys.stderr,\n                 verbose=True, debug=False, **fit_params):\n\n        args_in = {**locals(), **fit_params}\n        del args_in['self']\n        del args_in['__class__']\n        del args_in['fit_params']\n\n        if not hasattr(self, '_default_params'):\n            self._default_params = {}\n        self._default_params |= {\n            'num_live_points': 500,\n            'max_samples': 10_000,\n            'num_parallel_samplers': 1,\n            'evidence_uncert': 1E-3,\n            'live_evidence_frac': np.log(1 + 1e-3),\n        }\n\n        super().__init__(**args_in)\n\n        self.name = \"Nested Sampling Fitting Procedure\"\n\n        self.sampler = None\n\n        self._jaxnsmodel = None\n        self._jaxnsresults = None\n        self._jaxnstermination = None\n        self._jaxnsresults = None\n\n        self.logevidence = jnp.zeros(3)\n        self.priorvolume = 0.0\n\n    def prefit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None):\n        # ---------------------\n        fitting_procedure.prefit(**locals())\n        if seed is None: seed = _utils.randint()\n        # ---------------------\n\n        # Get uniform prior bounds\n        bounds = np.array([self.stat_model.prior_ranges[key] for key in self.stat_model.paramnames()])\n        lo, hi = jnp.array(bounds[:, 0]), jnp.array(bounds[:, 1])\n\n        data = self.stat_model.lc_to_data(lc_1, lc_2)\n\n        # Construct jaxns friendly prior &amp; likelihood\n        def prior_model():\n            x = yield jaxns.Prior(tfpd.Uniform(low=lo, high=hi), name='x')\n            return x\n\n        def log_likelihood(x):\n            params = _utils.dict_unpack(x, keys=self.stat_model.paramnames())\n            with numpyro.handlers.block(hide=self.stat_model.paramnames()):\n                LL = self.stat_model._log_likelihood(params, data=data)\n            return LL\n\n        # jaxns object setup\n        self._jaxnsmodel = jaxns.Model(prior_model=prior_model,\n                                       log_likelihood=log_likelihood,\n                                       )\n\n        self._jaxnstermination = jaxns.TerminationCondition(\n            dlogZ=self.evidence_uncert,\n            max_samples=self.max_samples,\n        )\n\n        # Build jaxns Nested Sampler\n        self.sampler = jaxns.NestedSampler(\n            model=self._jaxnsmodel,\n            max_samples=self.max_samples,\n            verbose=self.debug,\n            num_live_points=self.num_live_points,\n            num_parallel_workers=self.num_parallel_samplers,\n            difficult_model=True,\n        )\n\n        self.has_prefit = True\n\n    # --------------\n    def fit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None):\n        # ---------------------\n        fitting_procedure.fit(**locals())\n        if seed is None: seed = _utils.randint()\n        # ---------------------\n        if not self.has_prefit:\n            self.prefit(lc_1, lc_2, seed)\n        self.readyup()\n        # ---------------------\n\n        # -----------------\n        # Run the sampler!\n        termination_reason, state = self.sampler(jax.random.PRNGKey(seed),\n                                                 self._jaxnstermination)\n\n        # -----------------\n        # Extract &amp; save results\n        self._jaxnsresults = self.sampler.to_results(\n            termination_reason=termination_reason,\n            state=state\n        )\n\n        self.has_run = True\n\n    def get_samples(self, N: int = None, seed: int = None, importance_sampling: bool = False) -&gt; {str: [float]}:\n        if seed is None: seed = _utils.randint()\n\n        samples, logweights = self._jaxnsresults.samples['x'], self._jaxnsresults.log_dp_mean\n\n        if N is None:\n            if importance_sampling:\n                out = {key: samples.T[i] for i, key in enumerate(self.stat_model.paramnames())}\n                return out\n            else:\n                N = samples.shape[0]\n\n        if importance_sampling:\n            logweights = jnp.zeros_like(logweights)\n\n        samples = jaxns.resample(\n            key=jax.random.PRNGKey(seed),\n            samples=samples,\n            log_weights=logweights,\n            S=N\n        )\n\n        out = {key: samples.T[i] for i, key in enumerate(self.stat_model.paramnames())}\n\n        return (out)\n\n    def get_evidence(self, seed: int = None, return_type='linear') -&gt; [float, float, float]:\n        '''\n        Returns the -1, 0 and +1 sigma values for model evidence from nested sampling.\n        This represents an estimate of numerical uncertainty\n        '''\n\n        if seed is None: seed = _utils.randint()\n\n        l, l_e = self._jaxnsresults.log_Z_mean, self._jaxnsresults.log_Z_uncert\n\n        if return_type == 'linear':\n\n            out = np.exp([\n                l,\n                l - l_e,\n                l + l_e\n            ])\n\n            out -= np.array([0, out[0], out[0]])\n        elif return_type == 'log':\n            out = np.array([l, -l_e, l_e])\n\n        return out\n\n    def get_information(self, seed: int = None) -&gt; [float, float, float]:\n        '''\n        Use the Nested Sampling shells to estimate the model information relative to prior\n        '''\n        # todo - this is outmoded\n\n        if seed is None: seed = _utils.randint()\n\n        NS = self.sampler\n        samples, logweights = self._jaxnsresults.samples, self._jaxnsresults.log_dp_mean\n\n        weights = np.exp(logweights)\n        weights /= weights.sum()\n\n        log_density = self._jaxnsresults.log_posterior_density\n        prior_values = self.stat_model.log_prior(samples)\n\n        info = np.sum((log_density - prior_values) * weights)\n\n        partial_info = np.random.choice((log_density - prior_values), len(log_density), p=weights)\n        uncert = partial_info.std() / np.sqrt(len(log_density))\n\n        return (np.array(info, uncert, uncert))\n\n    def get_peaks(self, seed: int = None) -&gt; ({str: [float]}, float):\n\n        # todo - this is outmoded\n\n        # ---------------------\n        if seed is None: seed = _utils.randint()\n        # ---------------------\n\n        self.msg_err(\"get_peaks currently placeholder.\")\n        return ({key: np.array([]) for key in self.stat_model.paramnames()}, np.array([]))\n\n        # ---------------------\n\n        NS = self.sampler\n        samples = self.get_samples()\n        log_densities = NS._results.log_posterior_density\n\n        # Find clusters\n        indices = clustering.clusterfind_1D(samples['lag'])\n\n        # Break samples and log-densities up into clusters\n        sorted_samples = clustering.sort_by_cluster(samples, indices)\n        sort_logdens = clustering.sort_by_cluster(log_densities, indices)\n\n        Nclusters = len(sorted_samples)\n\n        # Make an empty dictionary to store positions in\n        peak_locations = {key: np.zeros([Nclusters]) for key in samples.keys()}\n        peaklikes = np.zeros([Nclusters])\n\n        for i, group, lds in enumerate(sorted_samples, sort_logdens):\n            j = np.argmax(lds)\n            for key in samples.keys():\n                peak_locations[key][i] = group[key][j]\n            peaklikes[i] = lds[j]\n\n        return (peak_locations, peaklikes)\n\n    def diagnostics(self):\n        jaxns.plot_diagnostics(self._jaxnsresults)\n        return\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.nested_sampling.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = 'Nested Sampling Fitting Procedure'\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.nested_sampling.sampler","title":"sampler  <code>instance-attribute</code>","text":"<pre><code>sampler = None\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.nested_sampling.logevidence","title":"logevidence  <code>instance-attribute</code>","text":"<pre><code>logevidence = zeros(3)\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.nested_sampling.priorvolume","title":"priorvolume  <code>instance-attribute</code>","text":"<pre><code>priorvolume = 0.0\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.nested_sampling.prefit","title":"prefit","text":"<pre><code>prefit(\n    lc_1: lightcurve, lc_2: lightcurve, seed: int = None\n)\n</code></pre> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def prefit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None):\n    # ---------------------\n    fitting_procedure.prefit(**locals())\n    if seed is None: seed = _utils.randint()\n    # ---------------------\n\n    # Get uniform prior bounds\n    bounds = np.array([self.stat_model.prior_ranges[key] for key in self.stat_model.paramnames()])\n    lo, hi = jnp.array(bounds[:, 0]), jnp.array(bounds[:, 1])\n\n    data = self.stat_model.lc_to_data(lc_1, lc_2)\n\n    # Construct jaxns friendly prior &amp; likelihood\n    def prior_model():\n        x = yield jaxns.Prior(tfpd.Uniform(low=lo, high=hi), name='x')\n        return x\n\n    def log_likelihood(x):\n        params = _utils.dict_unpack(x, keys=self.stat_model.paramnames())\n        with numpyro.handlers.block(hide=self.stat_model.paramnames()):\n            LL = self.stat_model._log_likelihood(params, data=data)\n        return LL\n\n    # jaxns object setup\n    self._jaxnsmodel = jaxns.Model(prior_model=prior_model,\n                                   log_likelihood=log_likelihood,\n                                   )\n\n    self._jaxnstermination = jaxns.TerminationCondition(\n        dlogZ=self.evidence_uncert,\n        max_samples=self.max_samples,\n    )\n\n    # Build jaxns Nested Sampler\n    self.sampler = jaxns.NestedSampler(\n        model=self._jaxnsmodel,\n        max_samples=self.max_samples,\n        verbose=self.debug,\n        num_live_points=self.num_live_points,\n        num_parallel_workers=self.num_parallel_samplers,\n        difficult_model=True,\n    )\n\n    self.has_prefit = True\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.nested_sampling.fit","title":"fit","text":"<pre><code>fit(lc_1: lightcurve, lc_2: lightcurve, seed: int = None)\n</code></pre> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def fit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None):\n    # ---------------------\n    fitting_procedure.fit(**locals())\n    if seed is None: seed = _utils.randint()\n    # ---------------------\n    if not self.has_prefit:\n        self.prefit(lc_1, lc_2, seed)\n    self.readyup()\n    # ---------------------\n\n    # -----------------\n    # Run the sampler!\n    termination_reason, state = self.sampler(jax.random.PRNGKey(seed),\n                                             self._jaxnstermination)\n\n    # -----------------\n    # Extract &amp; save results\n    self._jaxnsresults = self.sampler.to_results(\n        termination_reason=termination_reason,\n        state=state\n    )\n\n    self.has_run = True\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.nested_sampling.get_samples","title":"get_samples","text":"<pre><code>get_samples(\n    N: int = None,\n    seed: int = None,\n    importance_sampling: bool = False,\n) -&gt; {str: [float]}\n</code></pre> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def get_samples(self, N: int = None, seed: int = None, importance_sampling: bool = False) -&gt; {str: [float]}:\n    if seed is None: seed = _utils.randint()\n\n    samples, logweights = self._jaxnsresults.samples['x'], self._jaxnsresults.log_dp_mean\n\n    if N is None:\n        if importance_sampling:\n            out = {key: samples.T[i] for i, key in enumerate(self.stat_model.paramnames())}\n            return out\n        else:\n            N = samples.shape[0]\n\n    if importance_sampling:\n        logweights = jnp.zeros_like(logweights)\n\n    samples = jaxns.resample(\n        key=jax.random.PRNGKey(seed),\n        samples=samples,\n        log_weights=logweights,\n        S=N\n    )\n\n    out = {key: samples.T[i] for i, key in enumerate(self.stat_model.paramnames())}\n\n    return (out)\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.nested_sampling.get_evidence","title":"get_evidence","text":"<pre><code>get_evidence(\n    seed: int = None, return_type=\"linear\"\n) -&gt; [float, float, float]\n</code></pre> <p>Returns the -1, 0 and +1 sigma values for model evidence from nested sampling. This represents an estimate of numerical uncertainty</p> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def get_evidence(self, seed: int = None, return_type='linear') -&gt; [float, float, float]:\n    '''\n    Returns the -1, 0 and +1 sigma values for model evidence from nested sampling.\n    This represents an estimate of numerical uncertainty\n    '''\n\n    if seed is None: seed = _utils.randint()\n\n    l, l_e = self._jaxnsresults.log_Z_mean, self._jaxnsresults.log_Z_uncert\n\n    if return_type == 'linear':\n\n        out = np.exp([\n            l,\n            l - l_e,\n            l + l_e\n        ])\n\n        out -= np.array([0, out[0], out[0]])\n    elif return_type == 'log':\n        out = np.array([l, -l_e, l_e])\n\n    return out\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.nested_sampling.get_information","title":"get_information","text":"<pre><code>get_information(seed: int = None) -&gt; [float, float, float]\n</code></pre> <p>Use the Nested Sampling shells to estimate the model information relative to prior</p> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def get_information(self, seed: int = None) -&gt; [float, float, float]:\n    '''\n    Use the Nested Sampling shells to estimate the model information relative to prior\n    '''\n    # todo - this is outmoded\n\n    if seed is None: seed = _utils.randint()\n\n    NS = self.sampler\n    samples, logweights = self._jaxnsresults.samples, self._jaxnsresults.log_dp_mean\n\n    weights = np.exp(logweights)\n    weights /= weights.sum()\n\n    log_density = self._jaxnsresults.log_posterior_density\n    prior_values = self.stat_model.log_prior(samples)\n\n    info = np.sum((log_density - prior_values) * weights)\n\n    partial_info = np.random.choice((log_density - prior_values), len(log_density), p=weights)\n    uncert = partial_info.std() / np.sqrt(len(log_density))\n\n    return (np.array(info, uncert, uncert))\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.nested_sampling.get_peaks","title":"get_peaks","text":"<pre><code>get_peaks(seed: int = None) -&gt; ({str: [float]}, float)\n</code></pre> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def get_peaks(self, seed: int = None) -&gt; ({str: [float]}, float):\n\n    # todo - this is outmoded\n\n    # ---------------------\n    if seed is None: seed = _utils.randint()\n    # ---------------------\n\n    self.msg_err(\"get_peaks currently placeholder.\")\n    return ({key: np.array([]) for key in self.stat_model.paramnames()}, np.array([]))\n\n    # ---------------------\n\n    NS = self.sampler\n    samples = self.get_samples()\n    log_densities = NS._results.log_posterior_density\n\n    # Find clusters\n    indices = clustering.clusterfind_1D(samples['lag'])\n\n    # Break samples and log-densities up into clusters\n    sorted_samples = clustering.sort_by_cluster(samples, indices)\n    sort_logdens = clustering.sort_by_cluster(log_densities, indices)\n\n    Nclusters = len(sorted_samples)\n\n    # Make an empty dictionary to store positions in\n    peak_locations = {key: np.zeros([Nclusters]) for key in samples.keys()}\n    peaklikes = np.zeros([Nclusters])\n\n    for i, group, lds in enumerate(sorted_samples, sort_logdens):\n        j = np.argmax(lds)\n        for key in samples.keys():\n            peak_locations[key][i] = group[key][j]\n        peaklikes[i] = lds[j]\n\n    return (peak_locations, peaklikes)\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.nested_sampling.diagnostics","title":"diagnostics","text":"<pre><code>diagnostics()\n</code></pre> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def diagnostics(self):\n    jaxns.plot_diagnostics(self._jaxnsresults)\n    return\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.hessian_scan","title":"hessian_scan","text":"<p>               Bases: <code>fitting_procedure</code></p> Source code in <code>litmus/fitting_methods.py</code> <pre><code>class hessian_scan(fitting_procedure):\n    def __init__(self, stat_model: stats_model,\n                 out_stream=sys.stdout, err_stream=sys.stderr,\n                 verbose=True, debug=False, **fit_params):\n        args_in = {**locals(), **fit_params}\n        del args_in['self']\n        del args_in['__class__']\n        del args_in['fit_params']\n\n        if not hasattr(self, '_default_params'):\n            self._default_params = {}\n\n        self._default_params |= {\n            'Nlags': 64,\n            'opt_tol': 1E-2,\n            'opt_tol_init': 1E-4,\n            'step_size': 0.001,\n            'constrained_domain': False,\n            'max_opt_eval': 1_000,\n            'max_opt_eval_init': 5_000,\n            'LL_threshold': 100.0,\n            'init_samples': 5_000,\n            'grid_bunching': 0.5,\n            'grid_depth': None,\n            'grid_Nterp': None,\n            'grid_relaxation': 0.1,  # deprecated, remove\n            'grid_firstdepth': 2.0,\n            'reverse': True,\n            'split_lags': True,\n            'optimizer_args_init': {},\n            'optimizer_args': {},\n            'seed_params': {},\n            'precondition': 'diag',\n            'interp_scale': 'log',\n        }\n\n        self._allowable_interpscales = ['linear', 'log']\n\n        super().__init__(**args_in)\n\n        # -----------------------------------\n\n        self.name = \"Hessian Scan Fitting Procedure\"\n\n        self.lags: [float] = np.zeros(self.Nlags)\n        self.converged: np.ndarray[bool] = np.zeros_like(self.lags, dtype=bool)\n\n        self.scan_peaks: dict = {None}\n        self.log_evidences: list = None\n        self.log_evidences_uncert: list = None\n\n        self.diagnostic_hessians: list = None\n        self.diagnostic_densities: list = None\n        self.diagnostic_grads: list = None\n        self.diagnostic_ints: list = None\n        self.diagnostic_tgrads: list = None\n\n        self.params_toscan = self.stat_model.free_params()\n        if 'lag' in self.params_toscan: self.params_toscan.remove('lag')\n\n        self.precon_matrix: np.ndarray[np.float64] = np.eye(len(self.params_toscan), dtype=np.float64)\n        self.solver: jaxopt.BFGS = None\n\n        self.estmap_params = {}\n\n    def readyup(self):\n\n        # Get grid properties\n        if self.grid_depth is None:\n            self.grid_depth = int(1 / (1 - self.grid_relaxation) * 5)\n        if self.grid_Nterp is None:\n            self.grid_Nterp = self.Nlags * 10\n\n        # Make list of lags for scanning\n        self.lags = np.linspace(*self.stat_model.prior_ranges['lag'], self.Nlags + 1, endpoint=False)[1:]\n        self.converged = np.zeros_like(self.lags, dtype=bool)\n\n        free_dims = len(self.stat_model.free_params())\n        self.scan_peaks = {key: np.array([]) for key in self.stat_model.paramnames()}\n        self.diagnostic_hessians = []\n        self.diagnostic_grads = []\n        self.diagnostic_densities = []\n        self.log_evidences_uncert = []\n\n        self.params_toscan = [key for key in self.stat_model.paramnames() if\n                              key not in ['lag'] and key in self.stat_model.free_params()\n                              ]\n\n        self.is_ready = True\n\n    # --------------\n    # Setup Funcs\n\n    def estimate_MAP(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None):\n        '''\n        :param lc_1:\n        :param lc_2:\n        :param seed:\n        :return:\n        '''\n\n        data = self.stat_model.lc_to_data(lc_1, lc_2)\n\n        # ----------------------------------\n        # Find seed for optimization if not supplies\n        if self.stat_model.free_params() != self.seed_params.keys():\n            seed_params, ll_start = self.stat_model.find_seed(data, guesses=self.init_samples, fixed=self.seed_params)\n\n            self.msg_run(\"Beginning scan at constrained-space position:\")\n            for it in seed_params.items():\n                self.msg_run('\\t %s: \\t %.2f' % (it[0], it[1]))\n            self.msg_run(\n                \"Log-Density for this is: %.2f\" % ll_start)\n\n        else:\n            seed_params = self.seed_params\n            ll_start = self.stat_model.log_density(seed_params,\n                                                   data=data\n                                                   )\n\n        # ----------------------------------\n        # SCANNING FOR OPT\n\n        self.msg_run(\"Moving non-lag params to new location...\")\n        estmap_params = self.stat_model.scan(start_params=seed_params,\n                                             optim_params=[key for key in self.stat_model.free_params() if\n                                                           key != 'lag'],\n                                             data=data,\n                                             optim_kwargs=self.optimizer_args_init,\n                                             precondition=self.precondition\n                                             )\n        ll_firstscan = self.stat_model.log_density(estmap_params, data)\n        if 'lag' in self.stat_model.free_params():\n            self.msg_run(\"Optimizer settled at new fit:\")\n            for it in estmap_params.items():\n                self.msg_run('\\t %s: \\t %.2f' % (it[0], it[1]))\n            self.msg_run(\n                \"Log-Density for this is: %.2f\" % ll_firstscan\n            )\n\n            self.msg_run(\"Finding a good lag...\")\n            test_lags = self.stat_model.prior_sample(self.init_samples)['lag']\n            test_samples = _utils.dict_extend(estmap_params, {'lag': test_lags})\n            ll_test = self.stat_model.log_density(test_samples, data)\n            bestlag = test_lags[ll_test.argmax()]\n\n            self.msg_run(\"Grid finds good lag at %.2f:\" % bestlag)\n            self.msg_run(\n                \"Log-Density for this is: %.2f\" % ll_test.max()\n            )\n\n            if ll_test.max() &gt; ll_firstscan:\n                bestlag = bestlag\n            else:\n                bestlag = estmap_params['lag']\n\n            estmap_params = self.stat_model.scan(start_params=estmap_params | {'lag': bestlag},\n                                                 optim_params=['lag'],\n                                                 data=data,\n                                                 optim_kwargs=self.optimizer_args_init,\n                                                 precondition=self.precondition\n                                                 )\n\n            self.msg_run(\"Lag-only opt settled at new lag %.2f...\" % estmap_params['lag'])\n\n        ll_end = self.stat_model.log_density(estmap_params,\n                                             data=data\n                                             )\n\n        # ----------------------------------\n        # CHECKING OUTPUTS\n\n        self.msg_run(\"Optimizer settled at new fit:\")\n        for it in estmap_params.items():\n            self.msg_run('\\t %s: \\t %.2f' % (it[0], it[1]))\n        self.msg_run(\n            \"Log-Density for this is: %.2f\" % ll_end\n        )\n\n        # ----------------------------------\n        # CHECKING OUTPUTS\n\n        if ll_end &lt; ll_start:\n            self.msg_err(\"Warning! Optimization seems to have diverged. Defaulting to seed params. \\n\"\n                         \"Please consider running with different optim_init inputs\")\n            estmap_params = seed_params\n        return estmap_params\n\n    def make_grid(self, data, seed_params=None, interp_scale='log'):\n        \"\"\"\n        :param data:\n        :param seed_params:\n        :param interp_scale:\n        :return:\n        \"\"\"\n        # todo - reformat this and the make_grid for better consistency in drawing from estmap and seed params\n\n        assert interp_scale in ['log', 'linear'], \"Interp scale was %s, must be in 'log' or 'linear'\" % interp_scale\n\n        if not self.is_ready: self.readyup()\n\n        if 'lag' in self.stat_model.fixed_params():\n            lags = np.array([np.mean(self.stat_model.prior_ranges['lag'])])\n            self.Nlags = 1\n            self.readyup()\n            return lags\n\n        # If no seed parameters specified, use stored\n        if seed_params is None:\n            seed_params = self.estmap_params\n\n        # If these params are incomplete, use find_seed to complete them\n        if seed_params.keys() != self.stat_model.paramnames():\n            seed_params, llstart = self.stat_model.find_seed(data, guesses=self.init_samples, fixed=seed_params)\n\n        lags = np.linspace(*self.stat_model.prior_ranges['lag'], int(self.Nlags * self.grid_firstdepth) + 1,\n                           endpoint=False)[1:]\n        lag_terp = np.linspace(*self.stat_model.prior_ranges['lag'], self.grid_Nterp)\n\n        log_density_all, lags_all = np.empty(shape=(1,)), np.empty(shape=(1,))\n        for i in range(self.grid_depth):\n            params = _utils.dict_extend(seed_params, {'lag': lags})\n            log_density_all = np.concatenate([log_density_all, self.stat_model.log_density(params, data)])\n            lags_all = np.concatenate([lags_all, lags])\n            I = lags_all.argsort()\n            log_density_all, lags_all = log_density_all[I], lags_all[I]\n\n            if interp_scale == 'linear':\n\n                density = np.exp(log_density_all - log_density_all.max())\n\n                # Linearly interpolate the density profile\n                density_terp = np.interp(lag_terp, lags_all, density, left=0, right=0)\n                density_terp /= density_terp.sum()\n\n\n            elif interp_scale == 'log':\n\n                density = np.exp(log_density_all - log_density_all.max())\n\n                # Linearly interpolate the density profile\n                log_density_terp = np.interp(lag_terp, log_density_all - log_density_all.max(), density,\n                                             left=log_density_all[0], right=log_density_all[-1])\n                density_terp = np.exp(log_density_terp)\n                density_terp /= density_terp.sum()\n\n            gets = np.linspace(0, 1, self.grid_Nterp)\n            percentiles = np.cumsum(density_terp) * self.grid_bunching + gets * (1 - self.grid_bunching)\n            percentiles /= percentiles.max()\n\n            lags = np.interp(np.linspace(0, 1, self.Nlags), percentiles, lag_terp,\n                             left=lag_terp.min(),\n                             right=lag_terp.max()\n                             )\n\n        return lags\n\n    # --------------\n    # Fiting Funcs\n\n    def prefit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None):\n        # -------------------\n        fitting_procedure.prefit(**locals())\n        seed = self._tempseed\n        # -------------------\n\n        data = self.stat_model.lc_to_data(lc_1, lc_2)\n\n        # ----------------------------------\n        # Estimate the MAP\n\n        self.estmap_params = self.estimate_MAP(lc_1, lc_2, seed)\n        self.estmap_tol = self.stat_model.opt_tol(self.estmap_params, data,\n                                                  integrate_axes=self.stat_model.free_params())\n        self.msg_run(\"Estimated to be within \u00b1%.2e\u03c3 of local optimum\" % self.estmap_tol)\n        # ----------------------------------\n\n        # Make a grid\n\n        lags = self.make_grid(data, seed_params=self.estmap_params)\n        if self.split_lags:\n            split_index = abs(lags - self.estmap_params['lag']).argmin()\n            lags_left, lags_right = lags[:split_index], lags[split_index:]\n            lags = np.concatenate([lags_right, lags_left[::-1]])\n            if self.reverse: lags = lags = np.concatenate([lags_left[::-1], lags_right])\n        elif self.reverse:\n            lags = lags[::-1]\n        self.lags = lags\n\n        self.has_prefit = True\n\n    def fit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None):\n        # -------------------\n        fitting_procedure.fit(**locals())\n        seed = self._tempseed\n        # -------------------\n        # Setup + prefit if not run\n        self.msg_run(\"Starting Hessian Scan\")\n\n        data = self.stat_model.lc_to_data(lc_1, lc_2)\n\n        if not self.has_prefit:\n            self.prefit(lc_1, lc_2, seed)\n        best_params = self.estmap_params.copy()\n\n        # ----------------------------------\n        # Create scanner and perform setup\n        params_toscan = self.params_toscan\n        lags_forscan = self.lags.copy()\n\n        solver, runsolver, [converter, deconverter, optfunc, runsolver_jit] = self.stat_model._scanner(data,\n                                                                                                       optim_params=params_toscan,\n                                                                                                       optim_kwargs=self.optimizer_args,\n                                                                                                       return_aux=True\n                                                                                                       )\n        self.solver = solver\n        x0, y0 = converter(best_params)\n        state = solver.init_state(x0, y0, data)\n\n        # ----------------------------------\n        # Sweep over lags\n        scanned_optima, grads, Hs = [], [], []\n        tols, Zs, Ints, tgrads = [], [], [], []\n        for i, lag in enumerate(lags_forscan):\n            self.msg_run(\":\" * 23)\n            self.msg_run(\"Scanning at lag=%.2f ...\" % lag)\n\n            # Get current param site in packed-function friendly terms\n            opt_params, aux_data, state = runsolver_jit(solver, best_params | {'lag': lag}, state)\n\n            # --------------\n            # Check if the optimization has succeeded or broken\n\n            l_1 = self.stat_model.log_density(best_params | {'lag': lag}, data)\n            l_2 = self.stat_model.log_density(opt_params | {'lag': lag}, data)\n            bigdrop = l_2 - l_1 &lt; -self.LL_threshold\n            diverged = np.any(np.isinf(np.array([x for x in self.stat_model.to_uncon(opt_params).values()])))\n\n            self.msg_run(\"Change of %.2f against %.2f\" % (l_2 - l_1, self.LL_threshold))\n\n            if not bigdrop and not diverged:\n                self.converged[i] = True\n\n                is_good = [True, True, True]\n\n                # ======\n                # Check position &amp; Grad\n                try:\n                    uncon_params = self.stat_model.to_uncon(opt_params)\n                    log_height = self.stat_model.log_density_uncon(uncon_params, data)\n                except:\n                    self.msg_err(\"Something wrong!\")\n                    is_good[0] = False\n\n                # ======\n                # Check tolerances &amp; hessians\n                try:\n                    H = self.stat_model.log_density_uncon_hess(uncon_params, data, keys=params_toscan)\n                    assert np.linalg.det(H), \"Error in H calc\"\n                    tol = self.stat_model.opt_tol(opt_params, data, integrate_axes=params_toscan)\n                except:\n                    self.msg_err(\"Something wrong in Hessian / Tolerance!:\")\n                    is_good[1] = False\n\n                # ======\n                # Get evidence\n                try:\n                    laplace_int = self.stat_model.laplace_log_evidence(opt_params, data,\n                                                                       integrate_axes=params_toscan,\n                                                                       constrained=self.constrained_domain)\n                    tgrad = self.stat_model.uncon_grad_lag(opt_params) if not self.constrained_domain else 0\n                    Z = laplace_int + tgrad\n                    assert not np.isnan(Z), \"Error in Z calc\"\n                except:\n                    self.msg_err(\"Something wrong in Evidence!:\")\n                    is_good[2] = False\n\n                # Check and save if good\n                if np.all(is_good):\n                    self.msg_run(\n                        \"Seems to have converged at iteration %i / %i with tolerance %.2e\" % (i, self.Nlags, tol))\n\n                    if tol &lt; 1.0:\n                        best_params = opt_params\n                    else:\n                        self.msg_run(\"Possibly stuck in a furrow. Resetting start params\")\n                        best_params = self.estmap_params.copy()\n\n                    scanned_optima.append(opt_params.copy())\n                    tols.append(tol)\n\n                    grads.append(aux_data['grad'])\n                    Hs.append(H)\n                    Ints.append(laplace_int)\n                    tgrads.append(tgrad)\n                    Zs.append(Z)\n                else:\n                    self.msg_run(\"Seems to have severely diverged at iteration %i / %i\" % (i, self.Nlags))\n                    reason = [\"Eval\", \"hessian/tol\", \"evidence\"]\n                    for a, b in zip(reason, is_good):\n                        self.msg_run(\"%s:\\t%r\" % (a, b))\n\n            else:\n                self.converged[i] = False\n                self.msg_run(\"Unable to converge at iteration %i / %i\" % (i, self.Nlags),\n                             \"\\nLarge Drop?:\\t\", bigdrop,\n                             \"\\nOptimizer Diverged:\\t\", diverged)\n\n        if sum(self.converged) == 0:\n            self.msg_err(\"All slices catastrophically diverged! Try different starting conditions and/or grid spacing\")\n\n        self.msg_run(\"Scanning Complete. Calculating laplace integrals...\")\n\n        # --------\n        # Save and apply\n        self.diagnostic_grads = grads\n        self.diagnostic_hessians = Hs\n        self.diagnostic_tgrads = np.array(tgrads).squeeze().flatten()\n        self.diagnostic_ints = np.array(Ints).squeeze().flatten()\n\n        self.scan_peaks = _utils.dict_combine(scanned_optima)\n        self.diagnostic_densities = self.stat_model.log_density(self.scan_peaks, data)\n        self.log_evidences = np.array(Zs).squeeze().flatten()\n        self.log_evidences_uncert = np.square(tols).squeeze().flatten()\n\n        self.msg_run(\"Hessian Scan Fitting complete.\", \"-\" * 23, \"-\" * 23, delim='\\n')\n        self.has_run = True\n\n    def refit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None):\n        # -------------------\n        fitting_procedure.fit(**locals())\n        seed = self._tempseed\n        # -------------------\n\n        data = self.stat_model.lc_to_data(lc_1, lc_2)\n\n        peaks = _utils.dict_divide(self.scan_peaks)\n        I = np.arange(len(peaks))\n        select = np.argwhere(self.log_evidences_uncert &gt; self.opt_tol).squeeze()\n        if not (_utils.isiter(select)): select = np.array([select])\n\n        peaks, I = np.array(peaks)[select], I[select]\n\n        self.msg_run(\"Doing re-fitting of %i lags\" % len(peaks))\n        newtols = []\n        for j, i, peak in zip(range(len(I)), I, peaks):\n\n            self.msg_run(\":\" * 23, \"Refitting lag %i/%i at lag %.2f\" % (j, len(peaks), peak['lag']), delim='\\n')\n\n            ll_old = self.stat_model.log_density(peak, data)\n            old_tol = self.log_evidences_uncert[i]\n\n            new_peak = self.stat_model.scan(start_params=peak,\n                                            optim_params=self.params_toscan,\n                                            data=data,\n                                            optim_kwargs=self.optimizer_args,\n                                            precondition=self.precondition\n                                            )\n            ll_new = self.stat_model.log_density(new_peak, data)\n            if ll_old &gt; ll_new or np.isnan(ll_new):\n                self.msg_err(\"New peak bad (LL from %.2e to %.2e. Trying new start location\" % (ll_old, ll_new))\n                new_peak = self.stat_model.scan(start_params=self.estmap_params,\n                                                optim_params=self.params_toscan,\n                                                data=data,\n                                                optim_kwargs=self.optimizer_args,\n                                                precondition=self.precondition\n                                                )\n                ll_new = self.stat_model.log_density(new_peak, data)\n\n                if ll_old &gt; ll_new:\n                    self.msg_err(\"New peak bad (LL from %.2e to %.2e. Trying new start location\" % (ll_old, ll_new))\n                    continue\n\n            new_peak_uncon = self.stat_model.to_uncon(new_peak)\n            new_grad = _utils.dict_pack(new_grad, keys=self.params_toscan)\n            new_hessian = self.stat_model.log_density_uncon_hess(new_peak_uncon, data, keys=self.params_toscan)\n\n            try:\n                int = self.stat_model.laplace_log_evidence(new_peak, data, constrained=self.constrained_domain)\n                tgrad = self.stat_model.uncon_grad_lag(new_peak)\n                Z = tgrad + int\n                Hinv = np.linalg.inv(new_hessian)\n            except:\n                self.msg_run(\"Optimization failed on %i/%i\" % (j, len(peaks)))\n                continue\n\n            tol = self.stat_model.opt_tol(new_peak, data, self.params_toscan)\n\n            if tol &lt; old_tol:\n                self.diagnostic_tgrads[i] = tgrad\n                self.diagnostic_ints[i] = int\n                self.log_evidences[i] = tgrad + int\n\n                self.diagnostic_grads[i] = new_grad\n                self.diagnostic_hessians[i] = new_hessian\n                self.log_evidences_uncert[i] = tol ** 2\n                self.msg_run(\"Settled at new tol %.2e\" % tol)\n            else:\n                self.msg_run(\n                    \"Something went wrong at this refit! Consider changing the optimizer_args and trying again\")\n        self.msg_run(\"Refitting complete.\")\n\n    # --------------\n    # Checks\n\n    def diagnostics(self, plot=True):\n        '''\n        Runs some diagnostics for convergence\n        :return:\n        '''\n\n        loss = self.log_evidences_uncert\n\n        lagplot = self.scan_peaks['lag']\n        I = self.scan_peaks['lag'].argsort()\n        lagplot = lagplot[I]\n        Y = self.estmap_tol[I]\n\n        # ---------\n        fig = plt.figure()\n        plt.ylabel(\"Loss Norm, $ \\\\vert \\Delta x / \\sigma_x \\\\vert$\")\n        plt.xlabel(\"Scan Lag No.\")\n        plt.plot(lagplot, loss, 'o-', c='k', label=\"Scan Losses\")\n        plt.scatter(self.estmap_params['lag'], Y, c='r', marker='x', s=40, label=\"Initial MAP Scan Loss\")\n        plt.axhline(self.opt_tol, ls='--', c='k', label=\"Nominal Tolerance Limit\")\n        plt.legend(loc='best')\n\n        fig.text(.5, .05, \"How far each optimization slice is from its peak. Lower is good.\", ha='center')\n        plt.yscale('log')\n        plt.grid()\n        plt.show()\n\n    def diagnostic_lagplot(self, show=True):\n        f, (a1, a2) = plt.subplots(2, 1, sharex=True)\n\n        lags_forint, logZ_forint, density_forint = self._get_slices('lags', 'logZ', 'densities')\n\n        # ---------------------\n        for a in (a1, a2):\n            a.scatter(lags_forint, np.exp(logZ_forint - logZ_forint.max()), label=\"Evidence\")\n            a.scatter(lags_forint, np.exp(density_forint - density_forint.max()), label=\"Density\")\n            a.grid()\n\n        a2.set_yscale('log')\n        a1.legend()\n\n        # --------------\n        # Outputs\n        if show: plt.show()\n        return f\n\n    def _get_slices(self, *args: [str, ]) -&gt; dict:\n        \"\"\"\n        Summarizes the currently good scan peaks &amp; lag slices\n        Combined in one function for ease of access.\n        Any entries in *args (list of strings) will be returned in a keyed dict\n        Available keys and their corresponding attribute names in the class are:\n            'lags': lags_forint,\n            'logZ': logZ_forint,\n            'dlogZ': logZ_uncert_forint,\n            'peaks': peaks,\n            'covars': covars,\n            'densities': densities,\n            'grads': grads,\n        \"\"\"\n\n        good_tol = self.log_evidences_uncert &lt;= self.opt_tol\n        good_tgrad = abs(self.diagnostic_tgrads) &lt;= np.median(abs(self.diagnostic_tgrads)) * 10\n        select = np.argwhere(good_tol * good_tgrad).squeeze()\n        if not (_utils.isiter(select)): select = np.array([select])\n        if len(select) == 0:\n            self.msg_err(\"High uncertainty in slice evidences: result may be innacurate!. Try re-fitting.\")\n            select = np.where(good_tgrad)[0]\n\n        # Calculating Evidence\n        select = select[self.scan_peaks['lag'][select].argsort()]\n        lags_forint = self.scan_peaks['lag'][select]\n        logZ_forint = self.log_evidences[select]\n        logZ_uncert_forint = self.log_evidences_uncert[select]\n\n        grads = np.array([self.diagnostic_grads[i] for i in select]) if len(self.diagnostic_grads) &gt; 0 else np.zeros(\n            len(select))\n        densities = self.diagnostic_densities[select]\n\n        peaks = {key: val[select] for key, val in self.scan_peaks.items()}\n        peaks = np.array(_utils.dict_divide(peaks))\n        covars = -1 * np.array([np.linalg.inv(self.diagnostic_hessians[i]) for i in select])\n\n        out = {\n            'lags': lags_forint,\n            'logZ': logZ_forint,\n            'dlogZ': logZ_uncert_forint,\n            'peaks': peaks,\n            'covars': covars,\n            'densities': densities,\n            'grads': grads,\n        }\n\n        if args is None:\n            return out\n        else:\n            return (out[key] for key in args)\n\n    def get_evidence(self, seed: int = None, return_type='linear') -&gt; [float, float, float]:\n        # -------------------\n        fitting_procedure.get_evidence(**locals())\n        seed = self._tempseed\n        # -------------------\n\n        assert self.interp_scale in self._allowable_interpscales, \"Interp scale %s not recognised. Must be selection from %s\" % (\n            self.interp_scale, self._allowable_interpscales)\n\n        lags_forint, logZ_forint, logZ_uncert_forint = self._get_slices('lags', 'logZ', 'dlogZ')\n        minlag, maxlag = self.stat_model.prior_ranges['lag']\n\n        if maxlag - minlag == 0:\n            Z = np.exp(logZ_forint.max())\n            imax = logZ_forint.argmax()\n            uncert_plus, uncert_minus = logZ_uncert_forint[imax], logZ_uncert_forint[imax]\n\n\n        else:\n\n            if self.interp_scale == 'linear':\n                dlag = [*np.diff(lags_forint) / 2, 0]\n                dlag[1:] += np.diff(lags_forint) / 2\n                dlag[0] += lags_forint.min() - minlag\n                dlag[-1] += maxlag - lags_forint.max()\n\n                dlogZ = logZ_forint + np.log(dlag)\n                dZ = np.exp(dlogZ - dlogZ.max())\n                Z = dZ.sum() * np.exp(dlogZ.max())\n\n                # -------------------------------------\n                # Get Uncertainties\n\n                # todo Fix this to be generic and move outside of scope\n                # Estimate uncertainty from ~dt^2 error scaling\n                dlag_sub = [*np.diff(lags_forint[::2]) / 2, 0]\n                dlag_sub[1:] += np.diff(lags_forint[::2]) / 2\n                dlag_sub[0] += lags_forint.min() - minlag\n                dlag_sub[-1] += maxlag - lags_forint.max()\n\n                dlogZ_sub = logZ_forint[::2] + np.log(dlag_sub)\n                dZ_sub = np.exp(dlogZ_sub - dlogZ_sub.max())\n                Z_subsample = dZ_sub.sum() * np.exp(dlogZ_sub.max())\n                uncert_numeric = abs(Z - Z_subsample) / np.sqrt(17)\n\n                uncert_tol = np.square(dZ * logZ_uncert_forint).sum()\n                uncert_tol = np.sqrt(uncert_tol)\n                uncert_tol *= np.exp(dlogZ.max())\n\n\n            elif self.interp_scale == 'log':\n                # dZ = dXdY/dln|Y|\n                dlag = np.diff(lags_forint)\n                dY = np.diff(np.exp(logZ_forint - logZ_forint.max()))\n                dE = np.diff(logZ_forint)\n                dZ = dlag * dY / dE\n                Z = np.sum(dZ) * np.exp(logZ_forint.max())\n\n                uncert_tol = 4 * np.square(\n                    np.exp(logZ_forint - logZ_forint.max())[:-1] - dZ / dE\n                ) * logZ_uncert_forint[:-1]\n                uncert_tol += np.square(logZ_uncert_forint[-1] * np.exp(logZ_forint - logZ_forint.max())[-1])\n                uncert_tol = np.sqrt(uncert_tol.sum())\n                uncert_tol *= np.exp(logZ_forint.max())\n\n                dlag_sub = np.diff(lags_forint[::2])\n                dY_sub = np.diff(np.exp(logZ_forint[::2] - logZ_forint.max()))\n                dE_sub = np.diff(logZ_forint[::2])\n                dZ_sub = dlag_sub * dY_sub / dE_sub\n                Z_subsample = np.sum(dZ_sub) * np.exp(logZ_forint.max())\n                uncert_numeric = abs(Z - Z_subsample) / np.sqrt(17)\n\n            self.msg_debug(\"Evidence Est: \\t %.2e\" % Z)\n            self.msg_debug(\n                \"Evidence uncerts: \\n Numeric: \\t %.2e \\n Convergence: \\t %.2e\" % (uncert_numeric, uncert_tol))\n\n            uncert_plus = uncert_numeric + uncert_tol.sum()\n            uncert_minus = uncert_numeric\n\n        if return_type == 'linear':\n            return np.array([Z, -uncert_minus, uncert_plus])\n        elif return_type == 'log':\n            return np.array([np.log(Z), np.log(1 - uncert_minus / Z), np.log(1 + uncert_plus / Z)])\n\n    def get_samples(self, N: int = 1, seed: int = None, importance_sampling: bool = False) -&gt; {str: [float]}:\n        # -------------------\n        fitting_procedure.get_samples(**locals())\n        seed = self._tempseed\n        # -------------------\n\n        assert self.interp_scale in self._allowable_interpscales, \"Interp scale %s not recognised. Must be selection from %s\" % (\n            self.interp_scale, self._allowable_interpscales)\n        lags_forint, logZ_forint, peaks, covars = self._get_slices('lags', 'logZ', 'peaks', 'covars')\n\n        if len(lags_forint) == 0:\n            self.msg_err(\"Zero good slices in evidence integral!\")\n            return (0, -np.inf, np.inf)\n\n        # Get weights and peaks etc\n        Npeaks = len(lags_forint)\n        minlag, maxlag = self.stat_model.prior_ranges['lag']\n\n        Y = np.exp(logZ_forint - logZ_forint.max()).squeeze()\n\n        dlag = [*np.diff(lags_forint) / 2, 0]\n        dlag[1:] += np.diff(lags_forint) / 2\n        dlag[0] += lags_forint.min() - minlag\n        dlag[-1] += maxlag - lags_forint.max()\n\n        if sum(dlag) == 0:\n            dlag = 1.0\n\n        weights = Y * dlag\n        weights /= weights.sum()\n\n        # Get hessians and peak locations\n        if Npeaks &gt; 1:\n            I = np.random.choice(range(Npeaks), N, replace=True, p=weights)\n        else:\n            I = np.zeros(N)\n\n        to_choose = [(I == i).sum() for i in range(Npeaks)]  # number of samples to draw from peak i\n\n        # Sweep over scan peaks and add scatter\n        outs = []\n        for i in range(Npeaks):\n            if to_choose[i] &gt; 0:\n                peak_uncon = self.stat_model.to_uncon(peaks[i])\n\n                # Get normal dist properties in uncon space in vector form\n                mu = _utils.dict_pack(peak_uncon, keys=self.params_toscan)\n                cov = covars[i]\n\n                # Generate samples\n                samps = np.random.multivariate_normal(mean=mu, cov=cov, size=to_choose[i])\n                samps = _utils.dict_unpack(samps.T, keys=self.params_toscan, recursive=False)\n                samps = _utils.dict_extend(peak_uncon, samps)\n\n                # Reconvert to constrained space\n                samps = self.stat_model.to_con(samps)\n\n                # -------------\n                # Add linear interpolation 'smudging' to lags\n                if Npeaks &gt; 1 and 'lag' in self.stat_model.free_params():\n\n                    # Get nodes\n                    tnow, ynow = lags_forint[i], Y[i]\n                    if i == 0:\n                        yprev, ynext = ynow, Y[i + 1]\n                        tprev, tnext = min(self.stat_model.prior_ranges['lag']), lags_forint[i + 1]\n                    elif i == Npeaks - 1:\n                        yprev, ynext = Y[i - 1], ynow\n                        tprev, tnext = lags_forint[i - 1], max(self.stat_model.prior_ranges['lag'])\n                    else:\n                        yprev, ynext = Y[i - 1], Y[i + 1]\n                        tprev, tnext = lags_forint[i - 1], lags_forint[i + 1]\n                    # --\n\n                    # Perform CDF shift\n                    Ti, Yi = [tprev, tnow, tnext], [yprev, ynow, ynext]\n                    if self.interp_scale == 'linear':\n                        tshift = linscatter(Ti, Yi, N=to_choose[i])\n                    elif self.interp_scale == 'log':\n                        tshift = expscatter(Ti, Yi, N=to_choose[i])\n                    if np.isnan(tshift).any():\n                        self.msg_err(\"Something wrong with the lag shift at node %i in sample generation\" % i)\n                    else:\n                        samps['lag'] += tshift\n                # -------------\n\n                if np.isnan(samps['lag']).any():\n                    self.msg_err(\"Something wrong with the lags at node %i in sample generation\" % i)\n                else:\n                    outs.append(samps)\n\n        outs = {key: np.concatenate([out[key] for out in outs]) for key in self.stat_model.paramnames()}\n        return (outs)\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.hessian_scan.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = 'Hessian Scan Fitting Procedure'\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.hessian_scan.lags","title":"lags  <code>instance-attribute</code>","text":"<pre><code>lags: [float] = zeros(Nlags)\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.hessian_scan.converged","title":"converged  <code>instance-attribute</code>","text":"<pre><code>converged: ndarray[bool] = zeros_like(lags, dtype=bool)\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.hessian_scan.scan_peaks","title":"scan_peaks  <code>instance-attribute</code>","text":"<pre><code>scan_peaks: dict = {None}\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.hessian_scan.log_evidences","title":"log_evidences  <code>instance-attribute</code>","text":"<pre><code>log_evidences: list = None\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.hessian_scan.log_evidences_uncert","title":"log_evidences_uncert  <code>instance-attribute</code>","text":"<pre><code>log_evidences_uncert: list = None\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.hessian_scan.diagnostic_hessians","title":"diagnostic_hessians  <code>instance-attribute</code>","text":"<pre><code>diagnostic_hessians: list = None\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.hessian_scan.diagnostic_densities","title":"diagnostic_densities  <code>instance-attribute</code>","text":"<pre><code>diagnostic_densities: list = None\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.hessian_scan.diagnostic_grads","title":"diagnostic_grads  <code>instance-attribute</code>","text":"<pre><code>diagnostic_grads: list = None\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.hessian_scan.diagnostic_ints","title":"diagnostic_ints  <code>instance-attribute</code>","text":"<pre><code>diagnostic_ints: list = None\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.hessian_scan.diagnostic_tgrads","title":"diagnostic_tgrads  <code>instance-attribute</code>","text":"<pre><code>diagnostic_tgrads: list = None\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.hessian_scan.params_toscan","title":"params_toscan  <code>instance-attribute</code>","text":"<pre><code>params_toscan = free_params()\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.hessian_scan.precon_matrix","title":"precon_matrix  <code>instance-attribute</code>","text":"<pre><code>precon_matrix: ndarray[float64] = eye(\n    len(params_toscan), dtype=float64\n)\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.hessian_scan.solver","title":"solver  <code>instance-attribute</code>","text":"<pre><code>solver: BFGS = None\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.hessian_scan.estmap_params","title":"estmap_params  <code>instance-attribute</code>","text":"<pre><code>estmap_params = {}\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.hessian_scan.readyup","title":"readyup","text":"<pre><code>readyup()\n</code></pre> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def readyup(self):\n\n    # Get grid properties\n    if self.grid_depth is None:\n        self.grid_depth = int(1 / (1 - self.grid_relaxation) * 5)\n    if self.grid_Nterp is None:\n        self.grid_Nterp = self.Nlags * 10\n\n    # Make list of lags for scanning\n    self.lags = np.linspace(*self.stat_model.prior_ranges['lag'], self.Nlags + 1, endpoint=False)[1:]\n    self.converged = np.zeros_like(self.lags, dtype=bool)\n\n    free_dims = len(self.stat_model.free_params())\n    self.scan_peaks = {key: np.array([]) for key in self.stat_model.paramnames()}\n    self.diagnostic_hessians = []\n    self.diagnostic_grads = []\n    self.diagnostic_densities = []\n    self.log_evidences_uncert = []\n\n    self.params_toscan = [key for key in self.stat_model.paramnames() if\n                          key not in ['lag'] and key in self.stat_model.free_params()\n                          ]\n\n    self.is_ready = True\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.hessian_scan.estimate_MAP","title":"estimate_MAP","text":"<pre><code>estimate_MAP(\n    lc_1: lightcurve, lc_2: lightcurve, seed: int = None\n)\n</code></pre> <p>:param lc_1: :param lc_2: :param seed: :return:</p> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def estimate_MAP(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None):\n    '''\n    :param lc_1:\n    :param lc_2:\n    :param seed:\n    :return:\n    '''\n\n    data = self.stat_model.lc_to_data(lc_1, lc_2)\n\n    # ----------------------------------\n    # Find seed for optimization if not supplies\n    if self.stat_model.free_params() != self.seed_params.keys():\n        seed_params, ll_start = self.stat_model.find_seed(data, guesses=self.init_samples, fixed=self.seed_params)\n\n        self.msg_run(\"Beginning scan at constrained-space position:\")\n        for it in seed_params.items():\n            self.msg_run('\\t %s: \\t %.2f' % (it[0], it[1]))\n        self.msg_run(\n            \"Log-Density for this is: %.2f\" % ll_start)\n\n    else:\n        seed_params = self.seed_params\n        ll_start = self.stat_model.log_density(seed_params,\n                                               data=data\n                                               )\n\n    # ----------------------------------\n    # SCANNING FOR OPT\n\n    self.msg_run(\"Moving non-lag params to new location...\")\n    estmap_params = self.stat_model.scan(start_params=seed_params,\n                                         optim_params=[key for key in self.stat_model.free_params() if\n                                                       key != 'lag'],\n                                         data=data,\n                                         optim_kwargs=self.optimizer_args_init,\n                                         precondition=self.precondition\n                                         )\n    ll_firstscan = self.stat_model.log_density(estmap_params, data)\n    if 'lag' in self.stat_model.free_params():\n        self.msg_run(\"Optimizer settled at new fit:\")\n        for it in estmap_params.items():\n            self.msg_run('\\t %s: \\t %.2f' % (it[0], it[1]))\n        self.msg_run(\n            \"Log-Density for this is: %.2f\" % ll_firstscan\n        )\n\n        self.msg_run(\"Finding a good lag...\")\n        test_lags = self.stat_model.prior_sample(self.init_samples)['lag']\n        test_samples = _utils.dict_extend(estmap_params, {'lag': test_lags})\n        ll_test = self.stat_model.log_density(test_samples, data)\n        bestlag = test_lags[ll_test.argmax()]\n\n        self.msg_run(\"Grid finds good lag at %.2f:\" % bestlag)\n        self.msg_run(\n            \"Log-Density for this is: %.2f\" % ll_test.max()\n        )\n\n        if ll_test.max() &gt; ll_firstscan:\n            bestlag = bestlag\n        else:\n            bestlag = estmap_params['lag']\n\n        estmap_params = self.stat_model.scan(start_params=estmap_params | {'lag': bestlag},\n                                             optim_params=['lag'],\n                                             data=data,\n                                             optim_kwargs=self.optimizer_args_init,\n                                             precondition=self.precondition\n                                             )\n\n        self.msg_run(\"Lag-only opt settled at new lag %.2f...\" % estmap_params['lag'])\n\n    ll_end = self.stat_model.log_density(estmap_params,\n                                         data=data\n                                         )\n\n    # ----------------------------------\n    # CHECKING OUTPUTS\n\n    self.msg_run(\"Optimizer settled at new fit:\")\n    for it in estmap_params.items():\n        self.msg_run('\\t %s: \\t %.2f' % (it[0], it[1]))\n    self.msg_run(\n        \"Log-Density for this is: %.2f\" % ll_end\n    )\n\n    # ----------------------------------\n    # CHECKING OUTPUTS\n\n    if ll_end &lt; ll_start:\n        self.msg_err(\"Warning! Optimization seems to have diverged. Defaulting to seed params. \\n\"\n                     \"Please consider running with different optim_init inputs\")\n        estmap_params = seed_params\n    return estmap_params\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.hessian_scan.make_grid","title":"make_grid","text":"<pre><code>make_grid(data, seed_params=None, interp_scale='log')\n</code></pre> <p>:param data: :param seed_params: :param interp_scale: :return:</p> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def make_grid(self, data, seed_params=None, interp_scale='log'):\n    \"\"\"\n    :param data:\n    :param seed_params:\n    :param interp_scale:\n    :return:\n    \"\"\"\n    # todo - reformat this and the make_grid for better consistency in drawing from estmap and seed params\n\n    assert interp_scale in ['log', 'linear'], \"Interp scale was %s, must be in 'log' or 'linear'\" % interp_scale\n\n    if not self.is_ready: self.readyup()\n\n    if 'lag' in self.stat_model.fixed_params():\n        lags = np.array([np.mean(self.stat_model.prior_ranges['lag'])])\n        self.Nlags = 1\n        self.readyup()\n        return lags\n\n    # If no seed parameters specified, use stored\n    if seed_params is None:\n        seed_params = self.estmap_params\n\n    # If these params are incomplete, use find_seed to complete them\n    if seed_params.keys() != self.stat_model.paramnames():\n        seed_params, llstart = self.stat_model.find_seed(data, guesses=self.init_samples, fixed=seed_params)\n\n    lags = np.linspace(*self.stat_model.prior_ranges['lag'], int(self.Nlags * self.grid_firstdepth) + 1,\n                       endpoint=False)[1:]\n    lag_terp = np.linspace(*self.stat_model.prior_ranges['lag'], self.grid_Nterp)\n\n    log_density_all, lags_all = np.empty(shape=(1,)), np.empty(shape=(1,))\n    for i in range(self.grid_depth):\n        params = _utils.dict_extend(seed_params, {'lag': lags})\n        log_density_all = np.concatenate([log_density_all, self.stat_model.log_density(params, data)])\n        lags_all = np.concatenate([lags_all, lags])\n        I = lags_all.argsort()\n        log_density_all, lags_all = log_density_all[I], lags_all[I]\n\n        if interp_scale == 'linear':\n\n            density = np.exp(log_density_all - log_density_all.max())\n\n            # Linearly interpolate the density profile\n            density_terp = np.interp(lag_terp, lags_all, density, left=0, right=0)\n            density_terp /= density_terp.sum()\n\n\n        elif interp_scale == 'log':\n\n            density = np.exp(log_density_all - log_density_all.max())\n\n            # Linearly interpolate the density profile\n            log_density_terp = np.interp(lag_terp, log_density_all - log_density_all.max(), density,\n                                         left=log_density_all[0], right=log_density_all[-1])\n            density_terp = np.exp(log_density_terp)\n            density_terp /= density_terp.sum()\n\n        gets = np.linspace(0, 1, self.grid_Nterp)\n        percentiles = np.cumsum(density_terp) * self.grid_bunching + gets * (1 - self.grid_bunching)\n        percentiles /= percentiles.max()\n\n        lags = np.interp(np.linspace(0, 1, self.Nlags), percentiles, lag_terp,\n                         left=lag_terp.min(),\n                         right=lag_terp.max()\n                         )\n\n    return lags\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.hessian_scan.prefit","title":"prefit","text":"<pre><code>prefit(\n    lc_1: lightcurve, lc_2: lightcurve, seed: int = None\n)\n</code></pre> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def prefit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None):\n    # -------------------\n    fitting_procedure.prefit(**locals())\n    seed = self._tempseed\n    # -------------------\n\n    data = self.stat_model.lc_to_data(lc_1, lc_2)\n\n    # ----------------------------------\n    # Estimate the MAP\n\n    self.estmap_params = self.estimate_MAP(lc_1, lc_2, seed)\n    self.estmap_tol = self.stat_model.opt_tol(self.estmap_params, data,\n                                              integrate_axes=self.stat_model.free_params())\n    self.msg_run(\"Estimated to be within \u00b1%.2e\u03c3 of local optimum\" % self.estmap_tol)\n    # ----------------------------------\n\n    # Make a grid\n\n    lags = self.make_grid(data, seed_params=self.estmap_params)\n    if self.split_lags:\n        split_index = abs(lags - self.estmap_params['lag']).argmin()\n        lags_left, lags_right = lags[:split_index], lags[split_index:]\n        lags = np.concatenate([lags_right, lags_left[::-1]])\n        if self.reverse: lags = lags = np.concatenate([lags_left[::-1], lags_right])\n    elif self.reverse:\n        lags = lags[::-1]\n    self.lags = lags\n\n    self.has_prefit = True\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.hessian_scan.fit","title":"fit","text":"<pre><code>fit(lc_1: lightcurve, lc_2: lightcurve, seed: int = None)\n</code></pre> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def fit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None):\n    # -------------------\n    fitting_procedure.fit(**locals())\n    seed = self._tempseed\n    # -------------------\n    # Setup + prefit if not run\n    self.msg_run(\"Starting Hessian Scan\")\n\n    data = self.stat_model.lc_to_data(lc_1, lc_2)\n\n    if not self.has_prefit:\n        self.prefit(lc_1, lc_2, seed)\n    best_params = self.estmap_params.copy()\n\n    # ----------------------------------\n    # Create scanner and perform setup\n    params_toscan = self.params_toscan\n    lags_forscan = self.lags.copy()\n\n    solver, runsolver, [converter, deconverter, optfunc, runsolver_jit] = self.stat_model._scanner(data,\n                                                                                                   optim_params=params_toscan,\n                                                                                                   optim_kwargs=self.optimizer_args,\n                                                                                                   return_aux=True\n                                                                                                   )\n    self.solver = solver\n    x0, y0 = converter(best_params)\n    state = solver.init_state(x0, y0, data)\n\n    # ----------------------------------\n    # Sweep over lags\n    scanned_optima, grads, Hs = [], [], []\n    tols, Zs, Ints, tgrads = [], [], [], []\n    for i, lag in enumerate(lags_forscan):\n        self.msg_run(\":\" * 23)\n        self.msg_run(\"Scanning at lag=%.2f ...\" % lag)\n\n        # Get current param site in packed-function friendly terms\n        opt_params, aux_data, state = runsolver_jit(solver, best_params | {'lag': lag}, state)\n\n        # --------------\n        # Check if the optimization has succeeded or broken\n\n        l_1 = self.stat_model.log_density(best_params | {'lag': lag}, data)\n        l_2 = self.stat_model.log_density(opt_params | {'lag': lag}, data)\n        bigdrop = l_2 - l_1 &lt; -self.LL_threshold\n        diverged = np.any(np.isinf(np.array([x for x in self.stat_model.to_uncon(opt_params).values()])))\n\n        self.msg_run(\"Change of %.2f against %.2f\" % (l_2 - l_1, self.LL_threshold))\n\n        if not bigdrop and not diverged:\n            self.converged[i] = True\n\n            is_good = [True, True, True]\n\n            # ======\n            # Check position &amp; Grad\n            try:\n                uncon_params = self.stat_model.to_uncon(opt_params)\n                log_height = self.stat_model.log_density_uncon(uncon_params, data)\n            except:\n                self.msg_err(\"Something wrong!\")\n                is_good[0] = False\n\n            # ======\n            # Check tolerances &amp; hessians\n            try:\n                H = self.stat_model.log_density_uncon_hess(uncon_params, data, keys=params_toscan)\n                assert np.linalg.det(H), \"Error in H calc\"\n                tol = self.stat_model.opt_tol(opt_params, data, integrate_axes=params_toscan)\n            except:\n                self.msg_err(\"Something wrong in Hessian / Tolerance!:\")\n                is_good[1] = False\n\n            # ======\n            # Get evidence\n            try:\n                laplace_int = self.stat_model.laplace_log_evidence(opt_params, data,\n                                                                   integrate_axes=params_toscan,\n                                                                   constrained=self.constrained_domain)\n                tgrad = self.stat_model.uncon_grad_lag(opt_params) if not self.constrained_domain else 0\n                Z = laplace_int + tgrad\n                assert not np.isnan(Z), \"Error in Z calc\"\n            except:\n                self.msg_err(\"Something wrong in Evidence!:\")\n                is_good[2] = False\n\n            # Check and save if good\n            if np.all(is_good):\n                self.msg_run(\n                    \"Seems to have converged at iteration %i / %i with tolerance %.2e\" % (i, self.Nlags, tol))\n\n                if tol &lt; 1.0:\n                    best_params = opt_params\n                else:\n                    self.msg_run(\"Possibly stuck in a furrow. Resetting start params\")\n                    best_params = self.estmap_params.copy()\n\n                scanned_optima.append(opt_params.copy())\n                tols.append(tol)\n\n                grads.append(aux_data['grad'])\n                Hs.append(H)\n                Ints.append(laplace_int)\n                tgrads.append(tgrad)\n                Zs.append(Z)\n            else:\n                self.msg_run(\"Seems to have severely diverged at iteration %i / %i\" % (i, self.Nlags))\n                reason = [\"Eval\", \"hessian/tol\", \"evidence\"]\n                for a, b in zip(reason, is_good):\n                    self.msg_run(\"%s:\\t%r\" % (a, b))\n\n        else:\n            self.converged[i] = False\n            self.msg_run(\"Unable to converge at iteration %i / %i\" % (i, self.Nlags),\n                         \"\\nLarge Drop?:\\t\", bigdrop,\n                         \"\\nOptimizer Diverged:\\t\", diverged)\n\n    if sum(self.converged) == 0:\n        self.msg_err(\"All slices catastrophically diverged! Try different starting conditions and/or grid spacing\")\n\n    self.msg_run(\"Scanning Complete. Calculating laplace integrals...\")\n\n    # --------\n    # Save and apply\n    self.diagnostic_grads = grads\n    self.diagnostic_hessians = Hs\n    self.diagnostic_tgrads = np.array(tgrads).squeeze().flatten()\n    self.diagnostic_ints = np.array(Ints).squeeze().flatten()\n\n    self.scan_peaks = _utils.dict_combine(scanned_optima)\n    self.diagnostic_densities = self.stat_model.log_density(self.scan_peaks, data)\n    self.log_evidences = np.array(Zs).squeeze().flatten()\n    self.log_evidences_uncert = np.square(tols).squeeze().flatten()\n\n    self.msg_run(\"Hessian Scan Fitting complete.\", \"-\" * 23, \"-\" * 23, delim='\\n')\n    self.has_run = True\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.hessian_scan.refit","title":"refit","text":"<pre><code>refit(lc_1: lightcurve, lc_2: lightcurve, seed: int = None)\n</code></pre> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def refit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None):\n    # -------------------\n    fitting_procedure.fit(**locals())\n    seed = self._tempseed\n    # -------------------\n\n    data = self.stat_model.lc_to_data(lc_1, lc_2)\n\n    peaks = _utils.dict_divide(self.scan_peaks)\n    I = np.arange(len(peaks))\n    select = np.argwhere(self.log_evidences_uncert &gt; self.opt_tol).squeeze()\n    if not (_utils.isiter(select)): select = np.array([select])\n\n    peaks, I = np.array(peaks)[select], I[select]\n\n    self.msg_run(\"Doing re-fitting of %i lags\" % len(peaks))\n    newtols = []\n    for j, i, peak in zip(range(len(I)), I, peaks):\n\n        self.msg_run(\":\" * 23, \"Refitting lag %i/%i at lag %.2f\" % (j, len(peaks), peak['lag']), delim='\\n')\n\n        ll_old = self.stat_model.log_density(peak, data)\n        old_tol = self.log_evidences_uncert[i]\n\n        new_peak = self.stat_model.scan(start_params=peak,\n                                        optim_params=self.params_toscan,\n                                        data=data,\n                                        optim_kwargs=self.optimizer_args,\n                                        precondition=self.precondition\n                                        )\n        ll_new = self.stat_model.log_density(new_peak, data)\n        if ll_old &gt; ll_new or np.isnan(ll_new):\n            self.msg_err(\"New peak bad (LL from %.2e to %.2e. Trying new start location\" % (ll_old, ll_new))\n            new_peak = self.stat_model.scan(start_params=self.estmap_params,\n                                            optim_params=self.params_toscan,\n                                            data=data,\n                                            optim_kwargs=self.optimizer_args,\n                                            precondition=self.precondition\n                                            )\n            ll_new = self.stat_model.log_density(new_peak, data)\n\n            if ll_old &gt; ll_new:\n                self.msg_err(\"New peak bad (LL from %.2e to %.2e. Trying new start location\" % (ll_old, ll_new))\n                continue\n\n        new_peak_uncon = self.stat_model.to_uncon(new_peak)\n        new_grad = _utils.dict_pack(new_grad, keys=self.params_toscan)\n        new_hessian = self.stat_model.log_density_uncon_hess(new_peak_uncon, data, keys=self.params_toscan)\n\n        try:\n            int = self.stat_model.laplace_log_evidence(new_peak, data, constrained=self.constrained_domain)\n            tgrad = self.stat_model.uncon_grad_lag(new_peak)\n            Z = tgrad + int\n            Hinv = np.linalg.inv(new_hessian)\n        except:\n            self.msg_run(\"Optimization failed on %i/%i\" % (j, len(peaks)))\n            continue\n\n        tol = self.stat_model.opt_tol(new_peak, data, self.params_toscan)\n\n        if tol &lt; old_tol:\n            self.diagnostic_tgrads[i] = tgrad\n            self.diagnostic_ints[i] = int\n            self.log_evidences[i] = tgrad + int\n\n            self.diagnostic_grads[i] = new_grad\n            self.diagnostic_hessians[i] = new_hessian\n            self.log_evidences_uncert[i] = tol ** 2\n            self.msg_run(\"Settled at new tol %.2e\" % tol)\n        else:\n            self.msg_run(\n                \"Something went wrong at this refit! Consider changing the optimizer_args and trying again\")\n    self.msg_run(\"Refitting complete.\")\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.hessian_scan.diagnostics","title":"diagnostics","text":"<pre><code>diagnostics(plot=True)\n</code></pre> <p>Runs some diagnostics for convergence :return:</p> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def diagnostics(self, plot=True):\n    '''\n    Runs some diagnostics for convergence\n    :return:\n    '''\n\n    loss = self.log_evidences_uncert\n\n    lagplot = self.scan_peaks['lag']\n    I = self.scan_peaks['lag'].argsort()\n    lagplot = lagplot[I]\n    Y = self.estmap_tol[I]\n\n    # ---------\n    fig = plt.figure()\n    plt.ylabel(\"Loss Norm, $ \\\\vert \\Delta x / \\sigma_x \\\\vert$\")\n    plt.xlabel(\"Scan Lag No.\")\n    plt.plot(lagplot, loss, 'o-', c='k', label=\"Scan Losses\")\n    plt.scatter(self.estmap_params['lag'], Y, c='r', marker='x', s=40, label=\"Initial MAP Scan Loss\")\n    plt.axhline(self.opt_tol, ls='--', c='k', label=\"Nominal Tolerance Limit\")\n    plt.legend(loc='best')\n\n    fig.text(.5, .05, \"How far each optimization slice is from its peak. Lower is good.\", ha='center')\n    plt.yscale('log')\n    plt.grid()\n    plt.show()\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.hessian_scan.diagnostic_lagplot","title":"diagnostic_lagplot","text":"<pre><code>diagnostic_lagplot(show=True)\n</code></pre> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def diagnostic_lagplot(self, show=True):\n    f, (a1, a2) = plt.subplots(2, 1, sharex=True)\n\n    lags_forint, logZ_forint, density_forint = self._get_slices('lags', 'logZ', 'densities')\n\n    # ---------------------\n    for a in (a1, a2):\n        a.scatter(lags_forint, np.exp(logZ_forint - logZ_forint.max()), label=\"Evidence\")\n        a.scatter(lags_forint, np.exp(density_forint - density_forint.max()), label=\"Density\")\n        a.grid()\n\n    a2.set_yscale('log')\n    a1.legend()\n\n    # --------------\n    # Outputs\n    if show: plt.show()\n    return f\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.hessian_scan.get_evidence","title":"get_evidence","text":"<pre><code>get_evidence(\n    seed: int = None, return_type=\"linear\"\n) -&gt; [float, float, float]\n</code></pre> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def get_evidence(self, seed: int = None, return_type='linear') -&gt; [float, float, float]:\n    # -------------------\n    fitting_procedure.get_evidence(**locals())\n    seed = self._tempseed\n    # -------------------\n\n    assert self.interp_scale in self._allowable_interpscales, \"Interp scale %s not recognised. Must be selection from %s\" % (\n        self.interp_scale, self._allowable_interpscales)\n\n    lags_forint, logZ_forint, logZ_uncert_forint = self._get_slices('lags', 'logZ', 'dlogZ')\n    minlag, maxlag = self.stat_model.prior_ranges['lag']\n\n    if maxlag - minlag == 0:\n        Z = np.exp(logZ_forint.max())\n        imax = logZ_forint.argmax()\n        uncert_plus, uncert_minus = logZ_uncert_forint[imax], logZ_uncert_forint[imax]\n\n\n    else:\n\n        if self.interp_scale == 'linear':\n            dlag = [*np.diff(lags_forint) / 2, 0]\n            dlag[1:] += np.diff(lags_forint) / 2\n            dlag[0] += lags_forint.min() - minlag\n            dlag[-1] += maxlag - lags_forint.max()\n\n            dlogZ = logZ_forint + np.log(dlag)\n            dZ = np.exp(dlogZ - dlogZ.max())\n            Z = dZ.sum() * np.exp(dlogZ.max())\n\n            # -------------------------------------\n            # Get Uncertainties\n\n            # todo Fix this to be generic and move outside of scope\n            # Estimate uncertainty from ~dt^2 error scaling\n            dlag_sub = [*np.diff(lags_forint[::2]) / 2, 0]\n            dlag_sub[1:] += np.diff(lags_forint[::2]) / 2\n            dlag_sub[0] += lags_forint.min() - minlag\n            dlag_sub[-1] += maxlag - lags_forint.max()\n\n            dlogZ_sub = logZ_forint[::2] + np.log(dlag_sub)\n            dZ_sub = np.exp(dlogZ_sub - dlogZ_sub.max())\n            Z_subsample = dZ_sub.sum() * np.exp(dlogZ_sub.max())\n            uncert_numeric = abs(Z - Z_subsample) / np.sqrt(17)\n\n            uncert_tol = np.square(dZ * logZ_uncert_forint).sum()\n            uncert_tol = np.sqrt(uncert_tol)\n            uncert_tol *= np.exp(dlogZ.max())\n\n\n        elif self.interp_scale == 'log':\n            # dZ = dXdY/dln|Y|\n            dlag = np.diff(lags_forint)\n            dY = np.diff(np.exp(logZ_forint - logZ_forint.max()))\n            dE = np.diff(logZ_forint)\n            dZ = dlag * dY / dE\n            Z = np.sum(dZ) * np.exp(logZ_forint.max())\n\n            uncert_tol = 4 * np.square(\n                np.exp(logZ_forint - logZ_forint.max())[:-1] - dZ / dE\n            ) * logZ_uncert_forint[:-1]\n            uncert_tol += np.square(logZ_uncert_forint[-1] * np.exp(logZ_forint - logZ_forint.max())[-1])\n            uncert_tol = np.sqrt(uncert_tol.sum())\n            uncert_tol *= np.exp(logZ_forint.max())\n\n            dlag_sub = np.diff(lags_forint[::2])\n            dY_sub = np.diff(np.exp(logZ_forint[::2] - logZ_forint.max()))\n            dE_sub = np.diff(logZ_forint[::2])\n            dZ_sub = dlag_sub * dY_sub / dE_sub\n            Z_subsample = np.sum(dZ_sub) * np.exp(logZ_forint.max())\n            uncert_numeric = abs(Z - Z_subsample) / np.sqrt(17)\n\n        self.msg_debug(\"Evidence Est: \\t %.2e\" % Z)\n        self.msg_debug(\n            \"Evidence uncerts: \\n Numeric: \\t %.2e \\n Convergence: \\t %.2e\" % (uncert_numeric, uncert_tol))\n\n        uncert_plus = uncert_numeric + uncert_tol.sum()\n        uncert_minus = uncert_numeric\n\n    if return_type == 'linear':\n        return np.array([Z, -uncert_minus, uncert_plus])\n    elif return_type == 'log':\n        return np.array([np.log(Z), np.log(1 - uncert_minus / Z), np.log(1 + uncert_plus / Z)])\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.hessian_scan.get_samples","title":"get_samples","text":"<pre><code>get_samples(\n    N: int = 1,\n    seed: int = None,\n    importance_sampling: bool = False,\n) -&gt; {str: [float]}\n</code></pre> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def get_samples(self, N: int = 1, seed: int = None, importance_sampling: bool = False) -&gt; {str: [float]}:\n    # -------------------\n    fitting_procedure.get_samples(**locals())\n    seed = self._tempseed\n    # -------------------\n\n    assert self.interp_scale in self._allowable_interpscales, \"Interp scale %s not recognised. Must be selection from %s\" % (\n        self.interp_scale, self._allowable_interpscales)\n    lags_forint, logZ_forint, peaks, covars = self._get_slices('lags', 'logZ', 'peaks', 'covars')\n\n    if len(lags_forint) == 0:\n        self.msg_err(\"Zero good slices in evidence integral!\")\n        return (0, -np.inf, np.inf)\n\n    # Get weights and peaks etc\n    Npeaks = len(lags_forint)\n    minlag, maxlag = self.stat_model.prior_ranges['lag']\n\n    Y = np.exp(logZ_forint - logZ_forint.max()).squeeze()\n\n    dlag = [*np.diff(lags_forint) / 2, 0]\n    dlag[1:] += np.diff(lags_forint) / 2\n    dlag[0] += lags_forint.min() - minlag\n    dlag[-1] += maxlag - lags_forint.max()\n\n    if sum(dlag) == 0:\n        dlag = 1.0\n\n    weights = Y * dlag\n    weights /= weights.sum()\n\n    # Get hessians and peak locations\n    if Npeaks &gt; 1:\n        I = np.random.choice(range(Npeaks), N, replace=True, p=weights)\n    else:\n        I = np.zeros(N)\n\n    to_choose = [(I == i).sum() for i in range(Npeaks)]  # number of samples to draw from peak i\n\n    # Sweep over scan peaks and add scatter\n    outs = []\n    for i in range(Npeaks):\n        if to_choose[i] &gt; 0:\n            peak_uncon = self.stat_model.to_uncon(peaks[i])\n\n            # Get normal dist properties in uncon space in vector form\n            mu = _utils.dict_pack(peak_uncon, keys=self.params_toscan)\n            cov = covars[i]\n\n            # Generate samples\n            samps = np.random.multivariate_normal(mean=mu, cov=cov, size=to_choose[i])\n            samps = _utils.dict_unpack(samps.T, keys=self.params_toscan, recursive=False)\n            samps = _utils.dict_extend(peak_uncon, samps)\n\n            # Reconvert to constrained space\n            samps = self.stat_model.to_con(samps)\n\n            # -------------\n            # Add linear interpolation 'smudging' to lags\n            if Npeaks &gt; 1 and 'lag' in self.stat_model.free_params():\n\n                # Get nodes\n                tnow, ynow = lags_forint[i], Y[i]\n                if i == 0:\n                    yprev, ynext = ynow, Y[i + 1]\n                    tprev, tnext = min(self.stat_model.prior_ranges['lag']), lags_forint[i + 1]\n                elif i == Npeaks - 1:\n                    yprev, ynext = Y[i - 1], ynow\n                    tprev, tnext = lags_forint[i - 1], max(self.stat_model.prior_ranges['lag'])\n                else:\n                    yprev, ynext = Y[i - 1], Y[i + 1]\n                    tprev, tnext = lags_forint[i - 1], lags_forint[i + 1]\n                # --\n\n                # Perform CDF shift\n                Ti, Yi = [tprev, tnow, tnext], [yprev, ynow, ynext]\n                if self.interp_scale == 'linear':\n                    tshift = linscatter(Ti, Yi, N=to_choose[i])\n                elif self.interp_scale == 'log':\n                    tshift = expscatter(Ti, Yi, N=to_choose[i])\n                if np.isnan(tshift).any():\n                    self.msg_err(\"Something wrong with the lag shift at node %i in sample generation\" % i)\n                else:\n                    samps['lag'] += tshift\n            # -------------\n\n            if np.isnan(samps['lag']).any():\n                self.msg_err(\"Something wrong with the lags at node %i in sample generation\" % i)\n            else:\n                outs.append(samps)\n\n    outs = {key: np.concatenate([out[key] for out in outs]) for key in self.stat_model.paramnames()}\n    return (outs)\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.SVI_scan","title":"SVI_scan","text":"<p>               Bases: <code>hessian_scan</code></p> <p>An alternative to hessian_scan that fits each slice with stochastic variational inference instead of the laplace approximation. Typically slower, but more robust against numerical failure in low SNR signals and gives more accurate evidence estimates.</p> Source code in <code>litmus/fitting_methods.py</code> <pre><code>class SVI_scan(hessian_scan):\n    \"\"\"\n    An alternative to hessian_scan that fits each slice with stochastic variational\n    inference instead of the laplace approximation. Typically slower, but more robust against numerical failure in\n    low SNR signals and gives more accurate evidence estimates.\n    \"\"\"\n\n    def __init__(self, stat_model: stats_model,\n                 out_stream=sys.stdout, err_stream=sys.stderr,\n                 verbose=True, debug=False, **fit_params):\n        args_in = {**locals(), **fit_params}\n        del args_in['self']\n        del args_in['__class__']\n        del args_in['fit_params']\n\n        if not hasattr(self, '_default_params'):\n            self._default_params = {}\n\n        self._default_params |= {\n            'ELBO_threshold': 100.0,\n            'ELBO_optimstep': 5E-3,\n            'ELBO_particles': 128,\n            'ELBO_Nsteps': 128,\n            'ELBO_Nsteps_init': 1_000,\n            'ELBO_fraction': 0.25,\n        }\n\n        super().__init__(**args_in)\n\n        # -----------------------------\n\n        self.name = \"SVI Scan Fitting Procedure\"\n\n        self.diagnostic_losses = []\n        self.diagnostic_loss_init = []\n        self.diagnostic_ints = []\n\n    def fit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None):\n        # -------------------\n        fitting_procedure.fit(**locals())\n        seed = self._tempseed\n        # -------------------\n\n        self.msg_run(\"Starting SVI Scan\")\n\n        if not self.has_prefit:\n            self.prefit(lc_1, lc_2, seed)\n\n        data = self.stat_model.lc_to_data(lc_1, lc_2)\n\n        # ----------------------------------\n        # Estimate the MAP and its hessian for starting conditions\n\n        estmap_uncon = self.stat_model.to_uncon(self.estmap_params)\n\n        fix_param_dict_con = {key: self.estmap_params[key] for key in self.stat_model.fixed_params()}\n        fix_param_dict_uncon = {key: estmap_uncon[key] for key in self.stat_model.fixed_params()}\n\n        init_hess = -1 * self.stat_model.log_density_uncon_hess(estmap_uncon, data=data, keys=self.params_toscan)\n\n        # Convert these into SVI friendly objects and fit an SVI at the map\n        self.msg_run(\"Performing SVI slice at the MAP estimate\")\n        init_loc = _utils.dict_pack(estmap_uncon, keys=self.params_toscan)\n        init_tril = jnp.linalg.cholesky(jnp.linalg.inv(init_hess))\n\n        bad_starts = False\n        if np.isnan(init_loc).any() or np.isnan(init_tril).any():\n            self.msg_err(\"Issue with finding initial solver state for SVI. Proceeding /w numpyro defaults\")\n            bad_starts = True\n        # ----------------------------------\n        self.msg_debug(\"\\t Constructing slice model\")\n\n        def slice_function(data, lag):\n            '''\n            This is the conditional model that SVI will map\n            '''\n\n            params = {}\n            for key in self.stat_model.free_params():\n                if key != 'lag':\n                    val = quickprior(self.stat_model, key)\n                    params |= {key: val}\n            params |= {'lag': lag}\n            params |= fix_param_dict_con\n\n            with numpyro.handlers.block(hide=self.stat_model.paramnames()):\n                LL = self.stat_model._log_likelihood(params, data)\n\n            dilute = -np.log(self.stat_model.prior_ranges['lag'][1] - self.stat_model.prior_ranges['lag'][\n                0]) if 'lag' in self.stat_model.free_params() else 0.0\n            numpyro.factor('lag_prior', dilute)\n\n        # SVI settup\n        self.msg_debug(\"\\t Constructing and running optimizer and SVI guides\")\n        optimizer = numpyro.optim.Adam(step_size=self.ELBO_optimstep)\n        autoguide = numpyro.infer.autoguide.AutoMultivariateNormal(slice_function)\n        autosvi = numpyro.infer.SVI(slice_function, autoguide, optim=optimizer,\n                                    loss=numpyro.infer.Trace_ELBO(self.ELBO_particles),\n                                    )\n\n        self.msg_debug(\"\\t Running SVI\")\n        MAP_SVI_results = autosvi.run(jax.random.PRNGKey(seed), self.ELBO_Nsteps_init,\n                                      data=data, lag=self.estmap_params['lag'],\n                                      init_params={'auto_loc': init_loc,\n                                                   'auto_scale_tril': init_tril\n                                                   } if not bad_starts else None\n                                      )\n\n        self.msg_debug(\"\\t Success. Extracting solution\")\n        BEST_loc, BEST_tril = MAP_SVI_results.params['auto_loc'], MAP_SVI_results.params['auto_scale_tril']\n\n        self.diagnostic_loss_init = MAP_SVI_results.losses\n\n        # ----------------------------------\n        # Main Scan\n\n        lags_forscan = self.lags\n        l_old = -np.inf\n\n        scanned_optima = []\n        ELBOS_tosave = []\n        ElBOS_uncert = []\n        diagnostic_hessians = []\n        diagnostic_losses = []\n\n        for i, lag in enumerate(lags_forscan):\n            print(\":\" * 23)\n            self.msg_run(\"Doing SVI fit at lag=%.2f ...\" % lag)\n\n            svi_loop_result = autosvi.run(jax.random.PRNGKey(seed),\n                                          self.ELBO_Nsteps,\n                                          data=data, lag=lag,\n                                          init_params={'auto_loc': BEST_loc,\n                                                       'auto_scale_tril': BEST_tril\n                                                       },\n                                          progress_bar=False\n                                          )\n\n            NEW_loc, NEW_tril = svi_loop_result.params['auto_loc'], svi_loop_result.params['auto_scale_tril']\n\n            # --------------\n            # Check if the optimization has suceeded or broken\n\n            l_old = l_old\n            l_new = self._getELBO(svi_loop_result.losses)[0]\n            diverged = bool(np.isinf(NEW_loc).any() + np.isinf(NEW_tril).any())\n            big_drop = l_new - l_old &lt; - self.ELBO_threshold\n\n            self.msg_run(\n                \"From %.2f to %.2f, change of %.2f against %.2f\" % (l_old, l_new, l_new - l_old, self.ELBO_threshold))\n\n            if not big_drop and not diverged:\n                self.msg_run(\"Seems to have converged at iteration %i / %i\" % (i, self.Nlags))\n\n                self.converged[i] = True\n                l_old = l_new\n                BEST_loc, BEST_tril = NEW_loc, NEW_tril\n\n                uncon_params = self.stat_model.to_uncon(self.estmap_params | {'lag': lag}) | _utils.dict_unpack(NEW_loc,\n                                                                                                                self.params_toscan)\n                con_params = self.stat_model.to_con(uncon_params)\n                scanned_optima.append(con_params)\n\n                H = np.dot(NEW_tril, NEW_tril.T)\n                H = (H + H.T) / 2\n                H = jnp.linalg.inv(-H)\n                diagnostic_hessians.append(H)\n\n                diagnostic_losses.append(svi_loop_result.losses)\n\n                ELBO, uncert = self._getELBO(svi_loop_result.losses)\n                ELBOS_tosave.append(ELBO)\n                ElBOS_uncert.append(uncert)\n\n\n            else:\n                self.msg_run(\"Unable to converge at iteration %i / %i\" % (i, self.Nlags))\n                self.msg_debug(\"Reason for failure: \\n large ELBO drop: \\t %r \\n diverged: \\t %r\" % (\n                    big_drop, diverged))\n\n        self.msg_run(\"Scanning Complete. Calculating ELBO integrals...\")\n        if sum(self.converged) == 0:\n            self.msg_err(\"All slices catastrophically diverged! Try different starting conditions and/or grid spacing\")\n\n        self.diagnostic_ints = np.array(ELBOS_tosave)\n\n        self.log_evidences_uncert = np.array(ElBOS_uncert)\n        self.diagnostic_losses = np.array(diagnostic_losses)\n        self.diagnostic_hessians = np.array(diagnostic_hessians)\n\n        self.scan_peaks = _utils.dict_combine(scanned_optima)\n        self.diagnostic_densities = self.stat_model.log_density(self.scan_peaks, data)\n\n        # ---------------------------------------------------------------------------------\n        # For each of these peaks, estimate the evidence\n        # todo - vmap and parallelize\n\n        Zs, tgrads = [], []\n        for j, params in enumerate(scanned_optima):\n            Z = self.diagnostic_ints[j]\n            tgrad = self.stat_model.uncon_grad_lag(params) if not self.constrained_domain else 0\n            tgrad = 0\n            tgrads.append(tgrad)\n            Zs.append(Z + tgrad)\n\n        self.log_evidences = np.array(Zs)\n        self.diagnostic_tgrads = np.array(tgrads)\n        self.has_run = True\n\n        self.msg_run(\"SVI Fitting complete.\", \"-\" * 23, \"-\" * 23, delim='\\n')\n\n    def refit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None):\n        # TODO - fill this out\n\n        return\n\n    def _getELBO(self, losses):\n        \"\"\"\n        A utility for taking the chains of losses output by SVI and returning\n        estimates of log|Z| and dlog|Z|\n        \"\"\"\n\n        N = int(self.ELBO_Nsteps * self.ELBO_fraction)\n        ns = np.arange(2, N // 2) * 2\n        MEANS = np.zeros(len(ns))\n        UNCERTS = np.zeros(len(ns))\n\n        for i, n in enumerate(ns):\n            ELBO_samps = -1 * losses[-n:]\n            mean = ELBO_samps.mean()\n\n            samps_left, samps_right = np.split(ELBO_samps, 2)\n            uncert = ELBO_samps.var() / n\n            gap = max(0, samps_left.mean() - samps_right.mean())\n            skew = gap ** 2\n            uncert += skew\n\n            MEANS[i], UNCERTS[i] = mean, uncert ** 0.5\n\n        mean, uncert = MEANS[UNCERTS.argmin()], UNCERTS.min()\n\n        return (mean, uncert)\n\n    def diagnostics(self, plot=True):\n\n        f, (a2, a1) = plt.subplots(2, 1)\n\n        for i, x in enumerate(self.diagnostic_losses):\n            a1.plot(x - (self.diagnostic_ints[i]), c='k', alpha=0.25)\n        a2.plot(self.diagnostic_loss_init, c='k')\n\n        a1.axvline(int((1 - self.ELBO_fraction) * self.ELBO_Nsteps), c='k', ls='--')\n\n        a1.set_yscale('symlog')\n        a2.set_yscale('symlog')\n        a1.grid(), a2.grid()\n\n        a1.set_xlim(0, self.ELBO_Nsteps)\n        a2.set_xlim(0, self.ELBO_Nsteps_init)\n\n        a1.set_title(\"Scan SVIs\")\n        a2.set_title(\"Initial MAP SVI\")\n\n        f.supylabel(\"Loss - loss_final (log scale)\")\n        a1.set_xlabel(\"iteration Number\"), a2.set_xlabel(\"iteration Number\")\n\n        txt = \"Trace plots of ELBO convergence. All lines should be flat by the right hand side.\\n\" \\\n              \"Top panel is for initial guess and need only be flat. Bottom panel should be flat within\" \\\n              \"averaging range, i.e. to the right of dotted line.\"\n        f.supxlabel('$\\begin{center}X-axis\\\\*\\textit{\\small{%s}}\\end{center}$' % txt)\n\n        f.tight_layout()\n\n        if plot: plt.show()\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.SVI_scan.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = 'SVI Scan Fitting Procedure'\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.SVI_scan.diagnostic_losses","title":"diagnostic_losses  <code>instance-attribute</code>","text":"<pre><code>diagnostic_losses = []\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.SVI_scan.diagnostic_loss_init","title":"diagnostic_loss_init  <code>instance-attribute</code>","text":"<pre><code>diagnostic_loss_init = []\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.SVI_scan.diagnostic_ints","title":"diagnostic_ints  <code>instance-attribute</code>","text":"<pre><code>diagnostic_ints = []\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.SVI_scan.fit","title":"fit","text":"<pre><code>fit(lc_1: lightcurve, lc_2: lightcurve, seed: int = None)\n</code></pre> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def fit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None):\n    # -------------------\n    fitting_procedure.fit(**locals())\n    seed = self._tempseed\n    # -------------------\n\n    self.msg_run(\"Starting SVI Scan\")\n\n    if not self.has_prefit:\n        self.prefit(lc_1, lc_2, seed)\n\n    data = self.stat_model.lc_to_data(lc_1, lc_2)\n\n    # ----------------------------------\n    # Estimate the MAP and its hessian for starting conditions\n\n    estmap_uncon = self.stat_model.to_uncon(self.estmap_params)\n\n    fix_param_dict_con = {key: self.estmap_params[key] for key in self.stat_model.fixed_params()}\n    fix_param_dict_uncon = {key: estmap_uncon[key] for key in self.stat_model.fixed_params()}\n\n    init_hess = -1 * self.stat_model.log_density_uncon_hess(estmap_uncon, data=data, keys=self.params_toscan)\n\n    # Convert these into SVI friendly objects and fit an SVI at the map\n    self.msg_run(\"Performing SVI slice at the MAP estimate\")\n    init_loc = _utils.dict_pack(estmap_uncon, keys=self.params_toscan)\n    init_tril = jnp.linalg.cholesky(jnp.linalg.inv(init_hess))\n\n    bad_starts = False\n    if np.isnan(init_loc).any() or np.isnan(init_tril).any():\n        self.msg_err(\"Issue with finding initial solver state for SVI. Proceeding /w numpyro defaults\")\n        bad_starts = True\n    # ----------------------------------\n    self.msg_debug(\"\\t Constructing slice model\")\n\n    def slice_function(data, lag):\n        '''\n        This is the conditional model that SVI will map\n        '''\n\n        params = {}\n        for key in self.stat_model.free_params():\n            if key != 'lag':\n                val = quickprior(self.stat_model, key)\n                params |= {key: val}\n        params |= {'lag': lag}\n        params |= fix_param_dict_con\n\n        with numpyro.handlers.block(hide=self.stat_model.paramnames()):\n            LL = self.stat_model._log_likelihood(params, data)\n\n        dilute = -np.log(self.stat_model.prior_ranges['lag'][1] - self.stat_model.prior_ranges['lag'][\n            0]) if 'lag' in self.stat_model.free_params() else 0.0\n        numpyro.factor('lag_prior', dilute)\n\n    # SVI settup\n    self.msg_debug(\"\\t Constructing and running optimizer and SVI guides\")\n    optimizer = numpyro.optim.Adam(step_size=self.ELBO_optimstep)\n    autoguide = numpyro.infer.autoguide.AutoMultivariateNormal(slice_function)\n    autosvi = numpyro.infer.SVI(slice_function, autoguide, optim=optimizer,\n                                loss=numpyro.infer.Trace_ELBO(self.ELBO_particles),\n                                )\n\n    self.msg_debug(\"\\t Running SVI\")\n    MAP_SVI_results = autosvi.run(jax.random.PRNGKey(seed), self.ELBO_Nsteps_init,\n                                  data=data, lag=self.estmap_params['lag'],\n                                  init_params={'auto_loc': init_loc,\n                                               'auto_scale_tril': init_tril\n                                               } if not bad_starts else None\n                                  )\n\n    self.msg_debug(\"\\t Success. Extracting solution\")\n    BEST_loc, BEST_tril = MAP_SVI_results.params['auto_loc'], MAP_SVI_results.params['auto_scale_tril']\n\n    self.diagnostic_loss_init = MAP_SVI_results.losses\n\n    # ----------------------------------\n    # Main Scan\n\n    lags_forscan = self.lags\n    l_old = -np.inf\n\n    scanned_optima = []\n    ELBOS_tosave = []\n    ElBOS_uncert = []\n    diagnostic_hessians = []\n    diagnostic_losses = []\n\n    for i, lag in enumerate(lags_forscan):\n        print(\":\" * 23)\n        self.msg_run(\"Doing SVI fit at lag=%.2f ...\" % lag)\n\n        svi_loop_result = autosvi.run(jax.random.PRNGKey(seed),\n                                      self.ELBO_Nsteps,\n                                      data=data, lag=lag,\n                                      init_params={'auto_loc': BEST_loc,\n                                                   'auto_scale_tril': BEST_tril\n                                                   },\n                                      progress_bar=False\n                                      )\n\n        NEW_loc, NEW_tril = svi_loop_result.params['auto_loc'], svi_loop_result.params['auto_scale_tril']\n\n        # --------------\n        # Check if the optimization has suceeded or broken\n\n        l_old = l_old\n        l_new = self._getELBO(svi_loop_result.losses)[0]\n        diverged = bool(np.isinf(NEW_loc).any() + np.isinf(NEW_tril).any())\n        big_drop = l_new - l_old &lt; - self.ELBO_threshold\n\n        self.msg_run(\n            \"From %.2f to %.2f, change of %.2f against %.2f\" % (l_old, l_new, l_new - l_old, self.ELBO_threshold))\n\n        if not big_drop and not diverged:\n            self.msg_run(\"Seems to have converged at iteration %i / %i\" % (i, self.Nlags))\n\n            self.converged[i] = True\n            l_old = l_new\n            BEST_loc, BEST_tril = NEW_loc, NEW_tril\n\n            uncon_params = self.stat_model.to_uncon(self.estmap_params | {'lag': lag}) | _utils.dict_unpack(NEW_loc,\n                                                                                                            self.params_toscan)\n            con_params = self.stat_model.to_con(uncon_params)\n            scanned_optima.append(con_params)\n\n            H = np.dot(NEW_tril, NEW_tril.T)\n            H = (H + H.T) / 2\n            H = jnp.linalg.inv(-H)\n            diagnostic_hessians.append(H)\n\n            diagnostic_losses.append(svi_loop_result.losses)\n\n            ELBO, uncert = self._getELBO(svi_loop_result.losses)\n            ELBOS_tosave.append(ELBO)\n            ElBOS_uncert.append(uncert)\n\n\n        else:\n            self.msg_run(\"Unable to converge at iteration %i / %i\" % (i, self.Nlags))\n            self.msg_debug(\"Reason for failure: \\n large ELBO drop: \\t %r \\n diverged: \\t %r\" % (\n                big_drop, diverged))\n\n    self.msg_run(\"Scanning Complete. Calculating ELBO integrals...\")\n    if sum(self.converged) == 0:\n        self.msg_err(\"All slices catastrophically diverged! Try different starting conditions and/or grid spacing\")\n\n    self.diagnostic_ints = np.array(ELBOS_tosave)\n\n    self.log_evidences_uncert = np.array(ElBOS_uncert)\n    self.diagnostic_losses = np.array(diagnostic_losses)\n    self.diagnostic_hessians = np.array(diagnostic_hessians)\n\n    self.scan_peaks = _utils.dict_combine(scanned_optima)\n    self.diagnostic_densities = self.stat_model.log_density(self.scan_peaks, data)\n\n    # ---------------------------------------------------------------------------------\n    # For each of these peaks, estimate the evidence\n    # todo - vmap and parallelize\n\n    Zs, tgrads = [], []\n    for j, params in enumerate(scanned_optima):\n        Z = self.diagnostic_ints[j]\n        tgrad = self.stat_model.uncon_grad_lag(params) if not self.constrained_domain else 0\n        tgrad = 0\n        tgrads.append(tgrad)\n        Zs.append(Z + tgrad)\n\n    self.log_evidences = np.array(Zs)\n    self.diagnostic_tgrads = np.array(tgrads)\n    self.has_run = True\n\n    self.msg_run(\"SVI Fitting complete.\", \"-\" * 23, \"-\" * 23, delim='\\n')\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.SVI_scan.refit","title":"refit","text":"<pre><code>refit(lc_1: lightcurve, lc_2: lightcurve, seed: int = None)\n</code></pre> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def refit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None):\n    # TODO - fill this out\n\n    return\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.SVI_scan.diagnostics","title":"diagnostics","text":"<pre><code>diagnostics(plot=True)\n</code></pre> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def diagnostics(self, plot=True):\n\n    f, (a2, a1) = plt.subplots(2, 1)\n\n    for i, x in enumerate(self.diagnostic_losses):\n        a1.plot(x - (self.diagnostic_ints[i]), c='k', alpha=0.25)\n    a2.plot(self.diagnostic_loss_init, c='k')\n\n    a1.axvline(int((1 - self.ELBO_fraction) * self.ELBO_Nsteps), c='k', ls='--')\n\n    a1.set_yscale('symlog')\n    a2.set_yscale('symlog')\n    a1.grid(), a2.grid()\n\n    a1.set_xlim(0, self.ELBO_Nsteps)\n    a2.set_xlim(0, self.ELBO_Nsteps_init)\n\n    a1.set_title(\"Scan SVIs\")\n    a2.set_title(\"Initial MAP SVI\")\n\n    f.supylabel(\"Loss - loss_final (log scale)\")\n    a1.set_xlabel(\"iteration Number\"), a2.set_xlabel(\"iteration Number\")\n\n    txt = \"Trace plots of ELBO convergence. All lines should be flat by the right hand side.\\n\" \\\n          \"Top panel is for initial guess and need only be flat. Bottom panel should be flat within\" \\\n          \"averaging range, i.e. to the right of dotted line.\"\n    f.supxlabel('$\\begin{center}X-axis\\\\*\\textit{\\small{%s}}\\end{center}$' % txt)\n\n    f.tight_layout()\n\n    if plot: plt.show()\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.JAVELIKE","title":"JAVELIKE","text":"<p>               Bases: <code>fitting_procedure</code></p> <p>A direct MCMC implementation using the AEIS in the style of JAVELIN Note that, because NumPyro fits in the unconstrained domain while JAVELIN fits in the constrained domain, the behaviour of the two will be slightly different near the prior boundaries.</p> <p>Note that this is for example / comparison only, and should not be used for actual fitting as it cannot handle the multimodal distributions of seasonal lightcurves</p> Source code in <code>litmus/fitting_methods.py</code> <pre><code>class JAVELIKE(fitting_procedure):\n    \"\"\"\n    A direct MCMC implementation using the AEIS in the style of JAVELIN\n    Note that, because NumPyro fits in the unconstrained domain while JAVELIN fits in the constrained domain,\n    the behaviour of the two will be slightly different near the prior boundaries.\n\n    Note that this is for example / comparison only, and _should not be used for actual fitting_ as it cannot handle\n    the multimodal distributions of seasonal lightcurves\n    \"\"\"\n\n    def __init__(self, stat_model: stats_model,\n                 out_stream=sys.stdout, err_stream=sys.stderr,\n                 verbose=True, debug=False, **fit_params):\n        args_in = {**locals(), **fit_params}\n        del args_in['self']\n        del args_in['__class__']\n        del args_in['fit_params']\n\n        if not hasattr(self, '_default_params'):\n            self._default_params = {}\n        self._default_params |= {\n            'alpha': 2.0,\n            'num_chains': 256,\n            'num_samples': 200_000 // 256,\n            'num_warmup': 5_000,\n        }\n\n        self.sampler: numpyro.infer.MCMC = None\n        self.kernel: numpyro.infer.AEIS = None\n        self.limited_model: Callable = None\n\n        super().__init__(**args_in)\n\n        # -----------------------------\n\n        self.name = \"AEIS JAVELIN Emulator fitting Procedure\"\n\n    def readyup(self):\n\n        fixed_vals = {key: self.stat_model.prior_ranges[key][0] for key in self.stat_model.fixed_params()}\n\n        def limited_model(data):\n            with numpyro.handlers.block(hide=self.stat_model.fixed_params()):\n                params = {key: val for key, val in zip(self.stat_model.paramnames(), self.stat_model.prior())}\n\n            params |= fixed_vals\n            with numpyro.handlers.block(hide=self.stat_model.paramnames()):\n                LL = self.stat_model._log_density(params, data)\n\n            numpyro.factor('ll', LL)\n\n        self.limited_model = limited_model\n\n        self.kernel = numpyro.infer.AIES(self.limited_model,\n                                         moves={numpyro.infer.AIES.StretchMove(a=self.alpha): 1.0}\n                                         )\n\n        self.sampler = numpyro.infer.MCMC(self.kernel,\n                                          num_warmup=self.num_warmup,\n                                          num_samples=self.num_samples,\n                                          num_chains=self.num_chains,\n                                          chain_method='vectorized',\n                                          progress_bar=self.verbose)\n\n        self.is_ready = True\n\n    def prefit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None):\n        if seed is None: seed = self.seed\n        if not self.is_ready: self.readyup()\n\n        self.msg_run(\"Running warmup with %i chains and %i samples\" % (self.num_chains, self.num_warmup))\n        # self.sampler.warmup(jax.random.PRNGKey(seed), self.stat_model.lc_to_data(lc_1, lc_2))\n\n        self.has_prefit = True\n\n    def fit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None):\n        if seed is None: seed = self.seed\n        if not self.is_ready: self.readyup()\n        if not self.has_prefit: self.prefit(lc_1, lc_2, seed=seed)\n\n        self.msg_run(\"Running sampler with %i chains and %i samples\" % (self.num_chains, self.num_samples))\n\n        self.sampler.run(jax.random.PRNGKey(seed), self.stat_model.lc_to_data(lc_1, lc_2))\n        self.has_run = True\n\n    def get_samples(self, N: int = None, seed: int = None, importance_sampling: bool = False) -&gt; {str: [float]}:\n        if seed is None: seed = self.seed\n        if not self.has_run:\n            self.msg_err(\"Can't get samples before running!\")\n        if importance_sampling:\n            self.msg_err(\"JAVELIKE Already distributed according to posterior (ideally)\")\n        samps = self.sampler.get_samples()\n\n        if not (N is None):\n            M = _utils.dict_dim(samps)[1]\n            if M &gt; N: self.msg_err(\"Tried to get %i sub-samples from chain of %i total samples.\" % (M, N))\n\n            I = np.random.choice(np.arange(M), N, replace=True)\n            samps = {key: samps[key][I] for key in samps.keys()}\n        return (samps)\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.JAVELIKE.sampler","title":"sampler  <code>instance-attribute</code>","text":"<pre><code>sampler: MCMC = None\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.JAVELIKE.kernel","title":"kernel  <code>instance-attribute</code>","text":"<pre><code>kernel: AEIS = None\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.JAVELIKE.limited_model","title":"limited_model  <code>instance-attribute</code>","text":"<pre><code>limited_model: Callable = None\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.JAVELIKE.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = 'AEIS JAVELIN Emulator fitting Procedure'\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.JAVELIKE.readyup","title":"readyup","text":"<pre><code>readyup()\n</code></pre> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def readyup(self):\n\n    fixed_vals = {key: self.stat_model.prior_ranges[key][0] for key in self.stat_model.fixed_params()}\n\n    def limited_model(data):\n        with numpyro.handlers.block(hide=self.stat_model.fixed_params()):\n            params = {key: val for key, val in zip(self.stat_model.paramnames(), self.stat_model.prior())}\n\n        params |= fixed_vals\n        with numpyro.handlers.block(hide=self.stat_model.paramnames()):\n            LL = self.stat_model._log_density(params, data)\n\n        numpyro.factor('ll', LL)\n\n    self.limited_model = limited_model\n\n    self.kernel = numpyro.infer.AIES(self.limited_model,\n                                     moves={numpyro.infer.AIES.StretchMove(a=self.alpha): 1.0}\n                                     )\n\n    self.sampler = numpyro.infer.MCMC(self.kernel,\n                                      num_warmup=self.num_warmup,\n                                      num_samples=self.num_samples,\n                                      num_chains=self.num_chains,\n                                      chain_method='vectorized',\n                                      progress_bar=self.verbose)\n\n    self.is_ready = True\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.JAVELIKE.prefit","title":"prefit","text":"<pre><code>prefit(\n    lc_1: lightcurve, lc_2: lightcurve, seed: int = None\n)\n</code></pre> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def prefit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None):\n    if seed is None: seed = self.seed\n    if not self.is_ready: self.readyup()\n\n    self.msg_run(\"Running warmup with %i chains and %i samples\" % (self.num_chains, self.num_warmup))\n    # self.sampler.warmup(jax.random.PRNGKey(seed), self.stat_model.lc_to_data(lc_1, lc_2))\n\n    self.has_prefit = True\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.JAVELIKE.fit","title":"fit","text":"<pre><code>fit(lc_1: lightcurve, lc_2: lightcurve, seed: int = None)\n</code></pre> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def fit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None):\n    if seed is None: seed = self.seed\n    if not self.is_ready: self.readyup()\n    if not self.has_prefit: self.prefit(lc_1, lc_2, seed=seed)\n\n    self.msg_run(\"Running sampler with %i chains and %i samples\" % (self.num_chains, self.num_samples))\n\n    self.sampler.run(jax.random.PRNGKey(seed), self.stat_model.lc_to_data(lc_1, lc_2))\n    self.has_run = True\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.JAVELIKE.get_samples","title":"get_samples","text":"<pre><code>get_samples(\n    N: int = None,\n    seed: int = None,\n    importance_sampling: bool = False,\n) -&gt; {str: [float]}\n</code></pre> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def get_samples(self, N: int = None, seed: int = None, importance_sampling: bool = False) -&gt; {str: [float]}:\n    if seed is None: seed = self.seed\n    if not self.has_run:\n        self.msg_err(\"Can't get samples before running!\")\n    if importance_sampling:\n        self.msg_err(\"JAVELIKE Already distributed according to posterior (ideally)\")\n    samps = self.sampler.get_samples()\n\n    if not (N is None):\n        M = _utils.dict_dim(samps)[1]\n        if M &gt; N: self.msg_err(\"Tried to get %i sub-samples from chain of %i total samples.\" % (M, N))\n\n        I = np.random.choice(np.arange(M), N, replace=True)\n        samps = {key: samps[key][I] for key in samps.keys()}\n    return (samps)\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.correl_jax","title":"correl_jax","text":"<pre><code>correl_jax(X1, Y1, X2, Y2, Nterp=1024)\n</code></pre> <p>Calculates a linearly interpolated correlation value for two data series {X1,Y1} and {X2,Y2}, with Nterp interpolation points</p> Source code in <code>litmus/ICCF_working.py</code> <pre><code>def correl_jax(X1, Y1, X2, Y2, Nterp=1024):\n    \"\"\"\n    Calculates a linearly interpolated correlation value for two data series {X1,Y1} and\n    {X2,Y2}, with Nterp interpolation points\n    \"\"\"\n\n    # Find X values that are common to both arrays\n    Xmin = jnp.max(jnp.array([jnp.min(X1), jnp.min(X2)]))\n    Xmax = jnp.min(jnp.array([jnp.max(X1), jnp.max(X2)]))\n\n    extrap = 0.0\n\n    def f():\n        X_interp = jnp.linspace(Xmin, Xmax, Nterp)\n        Y1_interp = jnp.interp(X_interp, X1, fp=Y1, left=extrap, right=extrap)\n        Y2_interp = jnp.interp(X_interp, X2, fp=Y2, left=extrap, right=extrap)\n        out = jnp.corrcoef(x=Y1_interp, y=Y2_interp)[0][1]\n        return (out)\n\n    out = jax.lax.cond(Xmax &gt; Xmin, f, lambda: 0.0)\n\n    return (out)\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.correlfunc_jax","title":"correlfunc_jax","text":"<pre><code>correlfunc_jax(\n    lag: float,\n    X1: [float],\n    Y1: [float],\n    X2: [float],\n    Y2: [float],\n    Nterp=1024,\n) -&gt; [float]\n</code></pre> <p>Like correl_jax, but with signal 2 delayed by some lag</p> Source code in <code>litmus/ICCF_working.py</code> <pre><code>def correlfunc_jax(lag: float, X1: [float], Y1: [float], X2: [float], Y2: [float], Nterp=1024) -&gt; [float]:\n    \"\"\"\n    Like correl_jax, but with signal 2 delayed by some lag\n    \"\"\"\n    return (\n        correl_jax(X1, Y1, X2 - lag, Y2, Nterp)\n    )\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.correl_func_boot_jax_wrapper","title":"correl_func_boot_jax_wrapper","text":"<pre><code>correl_func_boot_jax_wrapper(\n    lags,\n    X1,\n    Y1,\n    X2,\n    Y2,\n    E1,\n    E2,\n    Nterp=1024,\n    Nboot=512,\n    r=jnp.exp(-1),\n)\n</code></pre> <p>DEPRECATED</p> Source code in <code>litmus/ICCF_working.py</code> <pre><code>def correl_func_boot_jax_wrapper(lags, X1, Y1, X2, Y2, E1, E2, Nterp=1024, Nboot=512, r=jnp.exp(-1)):\n    '''\n    DEPRECATED\n    '''\n    seeds = jnp.arange(Nboot)\n    N1, N2 = int(len(X1) * r), int(len(X2) * r)\n\n    out = correl_func_boot_jax(seeds, lags, X1, Y1, X2, Y2, E1, E2, Nterp, N1, N2)\n\n    return (out)\n</code></pre>"},{"location":"doctest/#litmus.fitting_methods.correl_func_boot_jax_wrapper_nomap","title":"correl_func_boot_jax_wrapper_nomap","text":"<pre><code>correl_func_boot_jax_wrapper_nomap(\n    lags,\n    X1,\n    Y1,\n    X2,\n    Y2,\n    E1,\n    E2,\n    Nterp=1024,\n    Nboot=512,\n    r=jnp.exp(-1),\n)\n</code></pre> Source code in <code>litmus/ICCF_working.py</code> <pre><code>def correl_func_boot_jax_wrapper_nomap(lags, X1, Y1, X2, Y2, E1, E2, Nterp=1024, Nboot=512, r=jnp.exp(-1)):\n    seeds = jnp.arange(Nboot)\n    N1, N2 = int(len(X1) * r), int(len(X2) * r)\n\n    out = [correl_func_boot_jax_nomap(seed, lags, X1, Y1, X2, Y2, E1, E2, Nterp, N1, N2) for seed in range(Nboot)]\n\n    return (jnp.array(out))\n</code></pre>"}]}