{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>HUGH MCDOUGALL 2024</p>"},{"location":"#litmus-documentation","title":"LITMUS Documentation","text":"<p>Lag Inference Through the Mixed Use of Samplers</p> <p>LITMUS is an in-progress program that uses modern statistical techniques, like nested sampling and stochastic variational inference, in combination with cutting edge programming tools like the just-in-time compilation framework <code>jax</code> and its bayesian modelling package <code>NumPyro</code>, to perform the messy task of lag recovery in AGN reverberation mapping.</p> <p></p> <p>This project is still very much in the early stages. If you have any questions, contact the author directly at hughmcdougallemail@gmail.com.</p>"},{"location":"GP_working/","title":"<code>litmus.GP_working</code>","text":"<p>gp_working.py</p> <p>Contains all interfacing with the tinyGP gaussian process modelling package</p> <p>Multi-band kernel adapated from:     \"Gaussian Process regression for astronomical time-series\"     Aigrain &amp; Foreman-Mackey, 2022:     https://arxiv.org/abs/2209.08940</p> <p>HM 2024</p>"},{"location":"GP_working/#litmus.gp_working.Multiband","title":"<code>Multiband</code>","text":"<p>               Bases: <code>Wrapper</code></p> <p>Multi-band GP kernel that knows how to scale GP to output amplitudes</p>"},{"location":"GP_working/#litmus.gp_working.Multiband.amplitudes","title":"<code>amplitudes: jnp.ndarray</code>  <code>instance-attribute</code>","text":""},{"location":"GP_working/#litmus.gp_working.Multiband.coord_to_sortable","title":"<code>coord_to_sortable(Y) -&gt; float</code>","text":"<p>Extracts the time value from the (time,band) coordinate so the GP can interpret the ordering of points in multiple bands</p> Source code in <code>litmus/gp_working.py</code> <pre><code>def coord_to_sortable(self, Y) -&gt; float:\n    \"\"\"\n    Extracts the time value from the (time,band) coordinate so the GP can interpret the ordering of points\n    in multiple bands\n    \"\"\"\n    t, band = Y\n    return t\n</code></pre>"},{"location":"GP_working/#litmus.gp_working.Multiband.observation_model","title":"<code>observation_model(Y) -&gt; float</code>","text":"<p>Scales the prediction for each band by their respective band amplitude in the predicted model</p> Source code in <code>litmus/gp_working.py</code> <pre><code>def observation_model(self, Y) -&gt; float:\n    \"\"\"\n    Scales the prediction for each band by their respective band amplitude in the predicted model\n    \"\"\"\n    t, band = Y\n    return self.amplitudes[band] * self.kernel.observation_model(t)\n</code></pre>"},{"location":"GP_working/#litmus.gp_working.mean_func","title":"<code>mean_func(means, Y) -&gt; _types.ArrayN</code>","text":"<p>DEPRECATED - means are subtracted in the model now Utitlity function to take array of constants and return as gp-friendly functions</p> Source code in <code>litmus/gp_working.py</code> <pre><code>def mean_func(means, Y) -&gt; _types.ArrayN:\n    \"\"\"\n    DEPRECATED - means are subtracted in the model now\n    Utitlity function to take array of constants and return as gp-friendly functions\n    \"\"\"\n    t, band = Y\n    return (means[band])\n</code></pre>"},{"location":"GP_working/#litmus.gp_working.build_gp","title":"<code>build_gp(T: _types.ArrayN, Y: _types.ArrayN, diag: _types.ArrayNxN, bands: _types.ArrayN, tau: float, amps: tuple[float, float], means: tuple[float, float], basekernel: tinygp.kernels.quasisep.Quasisep = tinygp.kernels.quasisep.Exp) -&gt; GaussianProcess</code>","text":"<p>Builds a tinygp two-band kernel for predictions</p> <p>Parameters:</p> Name Type Description Default <code>T</code> <code>ArrayN</code> <p>Time values for the GP</p> required <code>Y</code> <code>ArrayN</code> <p>Y values for the GP (No effect)</p> required <code>diag</code> <code>ArrayNxN</code> <p>Variance matrix (square uncertainties) of the GP</p> required <code>bands</code> <code>ArrayN</code> <p>The bands that the different entries in the time series correspond to</p> required <code>tau</code> <code>float</code> <p>Timescale of the GP</p> required <code>amps</code> <code>tuple[float, float]</code> <p>Amplitudes of the GP</p> required <code>means</code> <code>tuple[float, float]</code> required <code>basekernel</code> <code>Quasisep</code> <code>Exp</code> <p>Returns:</p> Type Description <code>GaussianProcess</code> <p>The tinyGP gaussian process object</p> Source code in <code>litmus/gp_working.py</code> <pre><code>def build_gp(T: _types.ArrayN, Y: _types.ArrayN, diag: _types.ArrayNxN, bands: _types.ArrayN, tau: float,\n             amps: tuple[float, float], means: tuple[float, float],\n             basekernel: tinygp.kernels.quasisep.Quasisep = tinygp.kernels.quasisep.Exp) -&gt; GaussianProcess:\n    \"\"\"\n    Builds a tinygp two-band kernel for predictions\n\n    :parameter T: Time values for the GP\n    :parameter Y: Y values for the GP (No effect)\n    :parameter diag: Variance matrix (square uncertainties) of the GP\n    :parameter bands: The bands that the different entries in the time series correspond to\n    :parameter tau: Timescale of the GP\n    :parameter amps: Amplitudes of the GP\n    :parameter means:\n    :parameter basekernel:\n\n    :return: The tinyGP gaussian process object\n    \"\"\"\n\n    # Create GP kernel with Multiband\n    multi_kernel = Multiband(\n        kernel=basekernel(scale=tau),\n        amplitudes=amps,\n    )\n\n    # Mean functions for offsetting signals\n    meanf = lambda X: mean_func(means, X)\n\n    # Construct GP object and return\n    gp = GaussianProcess(\n        multi_kernel,\n        (T, bands),\n        diag=diag,\n        mean=meanf\n    )\n    return (gp)\n</code></pre>"},{"location":"_utils/","title":"Utilities &amp; Typing","text":"<p>These are internal Utilities and variable types for type hinting. The end user is unlikely to need to worry about these, but these may of interest to developers</p>"},{"location":"_utils/#litmus_utils","title":"<code>litmus._utils</code>","text":"<p>These are internal utilities and convenience functions for use in <code>LITMUS</code>.</p> <p>utils.py Handy internal utilities for brevity and convenience. Nothing in here is accesible in the public _init file</p>"},{"location":"_utils/#litmus._utils.suppress_stdout","title":"<code>suppress_stdout()</code>","text":"Source code in <code>litmus/_utils.py</code> <pre><code>@contextmanager\ndef suppress_stdout():\n    ipython = get_ipython()\n\n    if ipython and hasattr(ipython, 'kernel'):  # Likely in a Jupyter notebook\n        with io.capture_output() as captured:\n            yield\n    else:\n        # Standard Python environment\n        original_stdout_fd = os.dup(sys.stdout.fileno())\n        with open(os.devnull, 'w') as devnull:\n            os.dup2(devnull.fileno(), sys.stdout.fileno())\n            try:\n                yield\n            finally:\n                os.dup2(original_stdout_fd, sys.stdout.fileno())\n                os.close(original_stdout_fd)\n</code></pre>"},{"location":"_utils/#litmus._utils.isiter","title":"<code>isiter(x: any) -&gt; bool</code>","text":"<p>Checks to see if an object is itterable</p> Source code in <code>litmus/_utils.py</code> <pre><code>def isiter(x: any) -&gt; bool:\n    \"\"\"\n    Checks to see if an object is itterable\n    \"\"\"\n    if type(x) == dict:\n        return len(x[list(x.keys())[0]]) &gt; 1\n    try:\n        iter(x)\n    except:\n        return (False)\n    else:\n        return (True)\n</code></pre>"},{"location":"_utils/#litmus._utils.isiter_dict","title":"<code>isiter_dict(DICT: dict) -&gt; bool</code>","text":"<p>like isiter but for a dictionary. Checks only the first element in DICT.keys</p> Source code in <code>litmus/_utils.py</code> <pre><code>def isiter_dict(DICT: dict) -&gt; bool:\n    \"\"\"\n    like isiter but for a dictionary. Checks only the first element in DICT.keys\n    \"\"\"\n\n    key = list(DICT.keys())[0]\n    if isiter(DICT[key]):\n        return True\n    else:\n        return False\n</code></pre>"},{"location":"_utils/#litmus._utils.dict_dim","title":"<code>dict_dim(DICT: dict) -&gt; (int, int)</code>","text":"<p>Checks the first element of a dictionary and returns its length</p> Source code in <code>litmus/_utils.py</code> <pre><code>def dict_dim(DICT: dict) -&gt; (int, int):\n    \"\"\"\n    Checks the first element of a dictionary and returns its length\n    \"\"\"\n\n    if isiter_dict(DICT):\n        firstkey = list(DICT.keys())[0]\n        return (len(list(DICT.keys())), len(DICT[firstkey]))\n    else:\n        return (len(list(DICT.keys())), 1)\n</code></pre>"},{"location":"_utils/#litmus._utils.dict_pack","title":"<code>dict_pack(DICT: dict, keys=None, recursive=True, H=None, d0={}) -&gt; np.array</code>","text":"<p>Packs a dictionary into an array format</p> <p>Parameters:</p> Name Type Description Default <code>DICT</code> <code>dict</code> <p>the dict to unpack</p> required <code>keys</code> <p>the order in which to index the keyed elements. If none, will use DICT.keys(). Can be partial</p> <code>None</code> <code>recursive</code> <p>whether to recurse into arrays</p> <code>True</code> <code>H</code> <p>Matrix to scale parameters by</p> <code>None</code> <code>d0</code> <p>Value to offset by before packing</p> <code>{}</code> <p>Returns:</p> Type Description <code>array</code> <p>(nkeys x len_array) np.arrayobject  X = H (d-d0)</p> Source code in <code>litmus/_utils.py</code> <pre><code>def dict_pack(DICT: dict, keys=None, recursive=True, H=None, d0={}) -&gt; np.array:\n    \"\"\"\n    Packs a dictionary into an array format\n    :param DICT: the dict to unpack\n    :param keys: the order in which to index the keyed elements. If none, will use DICT.keys(). Can be partial\n    :param recursive: whether to recurse into arrays\n    :param H: Matrix to scale parameters by\n    :param d0: Value to offset by before packing\n    :return: (nkeys x len_array) np.arrayobject\n\n    X = H (d-d0)\n    \"\"\"\n\n    nokeys = True if keys is None else 0\n    keys = keys if keys is not None else DICT.keys()\n\n    if d0 is {}: d0 = {key:0 for key in keys}\n\n    for key in keys:\n        if key in DICT.keys() and key not in d0.keys(): d0 |= {key: 0.0}\n\n    if recursive and type(list(DICT.values())[0]) == dict:\n        out = np.array(\n            [dict_pack(DICT[key] - d0[key], keys=keys if not nokeys else None, recursive=recursive) for key in keys])\n    else:\n        if isiter(DICT[list(keys)[0]]):\n            out = np.array([[DICT[key][i] - d0[key] for i in range(dict_dim(DICT)[1])] for key in keys])\n        else:\n            out = np.array([DICT[key] - d0[key] for key in keys])\n\n    return (out)\n</code></pre>"},{"location":"_utils/#litmus._utils.dict_unpack","title":"<code>dict_unpack(X: np.array, keys: [str], recursive=True, Hinv=None, x0=None) -&gt; np.array</code>","text":"<p>Unpacks an array into a dict</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>Array to unpack</p> required <code>keys</code> <code>[str]</code> <p>keys to unpack with</p> required <p>Returns:</p> Type Description <code>array</code> <p>Hinv(X) + x0</p> Source code in <code>litmus/_utils.py</code> <pre><code>def dict_unpack(X: np.array, keys: [str], recursive=True, Hinv=None, x0=None) -&gt; np.array:\n    \"\"\"\n    Unpacks an array into a dict\n    :param X: Array to unpack\n    :param keys: keys to unpack with\n    :return:\n\n    Hinv(X) + x0\n    \"\"\"\n    if Hinv is not None: assert Hinv.shape[0] == len(keys), \"Size of H must be equal to number of keys in dict_unpack\"\n\n    if recursive and isiter(X[0]):\n        out = {key: dict_unpack(X[i], keys, recursive) for i, key in enumerate(list(keys))}\n    else:\n        X = X.copy()\n        if Hinv is not None:\n            X = np.dot(Hinv, X)\n        if x0 is not None:\n            X += x0\n        out = {key: X[i] for i, key in enumerate(list(keys))}\n\n    return (out)\n</code></pre>"},{"location":"_utils/#litmus._utils.dict_sortby","title":"<code>dict_sortby(A: dict, B: dict, match_only=True) -&gt; dict</code>","text":"<p>Sorts dict A to match keys of dict B.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>dict</code> <p>Dict to be sorted</p> required <code>B</code> <code>dict</code> <p>Dict whose keys are will provide the ordering</p> required <code>match_only</code> <p>If true, returns only for keys common to both A and B. Else, append un-sorted entries to end</p> <code>True</code> <p>Returns:</p> Type Description <code>dict</code> <p>{key: A[key] for key in B if key in A}</p> Source code in <code>litmus/_utils.py</code> <pre><code>def dict_sortby(A: dict, B: dict, match_only=True) -&gt; dict:\n    \"\"\"\n    Sorts dict A to match keys of dict B.\n\n    :param A: Dict to be sorted\n    :param B: Dict whose keys are will provide the ordering\n    :param match_only: If true, returns only for keys common to both A and B. Else, append un-sorted entries to end\n    :return: {key: A[key] for key in B if key in A}\n    \"\"\"\n    out = {key: A[key] for key in B if key in A}\n    if not match_only:\n        out |= {key: A[key] for key in A if key not in B}\n    return (out)\n</code></pre>"},{"location":"_utils/#litmus._utils.dict_extend","title":"<code>dict_extend(A: dict, B: dict = None) -&gt; dict</code>","text":"<p>Extends all single-length entries of a dict to match the length of a non-singular element</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>dict</code> <p>Dictionary whose elements are to be extended</p> required <code>B</code> <code>dict</code> <p>(optional) the array to extend by, equivalent to dict_extend(A|B)</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dict A with any singleton elements extended to the longest entry in A or B</p> Source code in <code>litmus/_utils.py</code> <pre><code>def dict_extend(A: dict, B: dict = None) -&gt; dict:\n    \"\"\"\n    Extends all single-length entries of a dict to match the length of a non-singular element\n    :param A: Dictionary whose elements are to be extended\n    :param B: (optional) the array to extend by, equivalent to dict_extend(A|B)\n    :return: Dict A with any singleton elements extended to the longest entry in A or B\n    \"\"\"\n\n    out = A.copy()\n    if B is not None: out |= B\n\n    to_extend = [key for key in out if not isiter(out[key])]\n    to_leave = [key for key in out if isiter(out[key])]\n\n    if len(to_extend) == 0: return out\n    if len(to_leave) == 0: return out\n\n    N = len(out[to_leave[0]])\n    for key in to_leave[1:]:\n        assert len(out[key]) == N, \"Tried to dict_extend() a dictionary with inhomogeneous lengths\"\n\n    for key in to_extend:\n        out[key] = np.array([A[key]] * N)\n\n    return (out)\n</code></pre>"},{"location":"_utils/#litmus._utils.dict_combine","title":"<code>dict_combine(X: [dict]) -&gt; {str: [float]}</code>","text":"<p>Combines an array, list etc. of dictionaries into a dictionary of arrays</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>[dict]</code> <p>1D Iterable of dicts</p> required <p>Returns:</p> Type Description <code>{str: [float]}</code> <p>Dict of 1D iterables</p> Source code in <code>litmus/_utils.py</code> <pre><code>def dict_combine(X: [dict]) -&gt; {str: [float]}:\n    \"\"\"\n    Combines an array, list etc. of dictionaries into a dictionary of arrays\n\n    :param X: 1D Iterable of dicts\n    :return: Dict of 1D iterables\n    \"\"\"\n\n    N = len(X)\n    keys = X[0].keys()\n\n    out = {key: np.zeros(N) for key in keys}\n    for n in range(N):\n        for key in keys:\n            out[key][n] = X[n][key]\n    return (out)\n</code></pre>"},{"location":"_utils/#litmus._utils.dict_divide","title":"<code>dict_divide(X: dict) -&gt; [dict]</code>","text":"<p>Splits dict of arrays into array of dicts. Opposite of dict_combine</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>dict</code> <p>Dict of 1D iterables</p> required <p>Returns:</p> Type Description <code>[dict]</code> <p>1D Iterable of dicts</p> Source code in <code>litmus/_utils.py</code> <pre><code>def dict_divide(X: dict) -&gt; [dict]:\n    \"\"\"\n    Splits dict of arrays into array of dicts. Opposite of dict_combine\n\n    :param X: Dict of 1D iterables\n    :return: 1D Iterable of dicts\n    \"\"\"\n\n    keys = list(X.keys())\n    N = len(X[keys[0]])\n\n    out = [{key: X[key][i] for key in X} for i in range(N)]\n\n    return (out)\n</code></pre>"},{"location":"_utils/#litmus._utils.dict_split","title":"<code>dict_split(X: dict, keys: [str]) -&gt; (dict, dict)</code>","text":"<p>Splits a dict in two based on keys</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>dict</code> <p>Dict to be split into A,B</p> required <code>keys</code> <code>[str]</code> <p>Keys to be present in A, but not in B</p> required <p>Returns:</p> Type Description <code>(dict, dict)</code> <p>tuple of dicts (A,B)</p> Source code in <code>litmus/_utils.py</code> <pre><code>def dict_split(X: dict, keys: [str]) -&gt; (dict, dict):\n    \"\"\"\n    Splits a dict in two based on keys\n\n    :param X: Dict to be split into A,B\n    :param keys: Keys to be present in A, but not in B\n    :return: tuple of dicts (A,B)\n    \"\"\"\n    assert type(X) is dict, \"input to dict_split() must be of type dict\"\n    assert isiter(keys) and type(keys[0])==str, \"in dict_split() keys must be list of strings\"\n    A = {key: X[key] for key in keys}\n    B = {key: X[key] for key in X.keys() if key not in keys}\n    return (A, B)\n</code></pre>"},{"location":"_utils/#litmus._utils.pack_function","title":"<code>pack_function(func, packed_keys: [str], fixed_values: dict = {}, invert: bool = False, jit: bool = False, H: np.array = None, d0: dict = {}) -&gt; _types.FunctionType</code>","text":"<p>Re-arranges a function that takes dict arguments to tak array-like arguments instead, so as to be autograd friendly Takes a function f(D:dict, arg, kwargs) and returns f(X, D2, args, **kwargs), D2 is all elements of D not listed in 'packed_keys' or fixed_values.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <p>Function to be unpacked</p> required <code>packed_keys</code> <code>[str]</code> <p>Keys in 'D' to be packed in an array</p> required <code>fixed_values</code> <code>dict</code> <p>Elements of 'D' to be fixed</p> <code>{}</code> <code>invert</code> <code>bool</code> <p>If true, will 'flip' the function upside down</p> <code>False</code> <code>jit</code> <code>bool</code> <p>If true, will 'jit' the function</p> <code>False</code> <code>H</code> <code>array</code> <p>(optional) scaling matrix to reparameterize H with</p> <code>None</code> <code>d0</code> <code>dict</code> <p>(optional) If given, will center the reparameterized  function at x0</p> <code>{}</code> Source code in <code>litmus/_utils.py</code> <pre><code>def pack_function(func, packed_keys: ['str'], fixed_values: dict = {}, invert: bool = False, jit: bool = False,\n                  H: np.array = None, d0: dict = {}) -&gt; _types.FunctionType:\n    \"\"\"\n    Re-arranges a function that takes dict arguments to tak array-like arguments instead, so as to be autograd friendly\n    Takes a function f(D:dict, *arg, **kwargs) and returns f(X, D2, *args, **kwargs), D2 is all elements of D not\n    listed in 'packed_keys' or fixed_values.\n\n    :param func: Function to be unpacked\n    :param packed_keys: Keys in 'D' to be packed in an array\n    :param fixed_values: Elements of 'D' to be fixed\n    :param invert:  If true, will 'flip' the function upside down\n    :param jit: If true, will 'jit' the function\n    :param H: (optional) scaling matrix to reparameterize H with\n    :param d0: (optional) If given, will center the reparameterized  function at x0\n    \"\"\"\n\n    if H is not None:\n        assert H.shape[0] == len(packed_keys), \"Scaling matrix H must be same length as packed_keys\"\n    else:\n        H = jnp.eye(len(packed_keys))\n    d0 = {key: 0.0 for key in packed_keys} | d0\n    x0 = dict_pack(d0, packed_keys)\n\n    # --------\n\n    sign = -1 if invert else 1\n\n    # --------\n    def new_func(X, unpacked_params={}, *args, **kwargs):\n        X = jnp.dot(H, X - x0)\n        packed_dict = {key: x for key, x in zip(packed_keys, X)}\n        packed_dict |= unpacked_params\n        packed_dict |= fixed_values\n\n        out = func(packed_dict, *args, **kwargs)\n        return (sign * out)\n\n    # --------\n    if jit: new_func = jax.jit(new_func)\n\n    return (new_func)\n</code></pre>"},{"location":"_utils/#litmus._utils.randint","title":"<code>randint()</code>","text":"<p>Quick utility to generate a random integer</p> Source code in <code>litmus/_utils.py</code> <pre><code>def randint():\n    \"\"\"\n    Quick utility to generate a random integer\n    \"\"\"\n    return (np.random.randint(0, sys.maxsize // 1024))\n</code></pre>"},{"location":"_utils/#litmus_types","title":"<code>litmus._types</code>","text":"<p>Data types for type hinting. By convention in <code>LITMUS</code>, <code>N</code> indicates an over sample sites or observations while <code>M</code> indicates an index over number of parameters.</p> <p>A single ref package to create and pull in types for type hinting</p>"},{"location":"_utils/#litmus._types.DType","title":"<code>DType = TypeVar('DType', bound=generic)</code>  <code>module-attribute</code>","text":"<p>Any Dtype</p>"},{"location":"_utils/#litmus._types.ArrayN","title":"<code>ArrayN = Annotated[NDArray[DType], Literal['N']]</code>  <code>module-attribute</code>","text":"<p>1D array corresponding to N sample sites, e.g. an array of log densities</p>"},{"location":"_utils/#litmus._types.ArrayNxN","title":"<code>ArrayNxN = Annotated[NDArray[DType], Literal['N', 'N']]</code>  <code>module-attribute</code>","text":"<p>2D array corresponding to NxN sample sites, e.g. a GP covariance matrix</p>"},{"location":"_utils/#litmus._types.ArrayM","title":"<code>ArrayM = Annotated[NDArray[DType], Literal['M']]</code>  <code>module-attribute</code>","text":"<p>1D array corresponding to M parameters, e.g. a grad</p>"},{"location":"_utils/#litmus._types.ArrayMxM","title":"<code>ArrayMxM = Annotated[NDArray[DType], Literal['M', 'M']]</code>  <code>module-attribute</code>","text":"<p>2D array corresponding to MxM parameters, e.g. a hessian</p>"},{"location":"_utils/#litmus._types.ArrayNxMxM","title":"<code>ArrayNxMxM = Annotated[NDArray[DType], Literal['M', 'N', 'N']]</code>  <code>module-attribute</code>","text":"<p>3D array corresponding to N sheets of MxM arrays for N data points and M parameters, e.g. a plate of hessians</p>"},{"location":"fitting_methods/","title":"<code>litmus.fitting_methods</code>","text":"<p>Contains fitting procedures to be executed by the litmus class object</p> <p>HM 24</p>"},{"location":"fitting_methods/#litmus.fitting_methods.fitting_procedure","title":"<code>fitting_procedure(stat_model: stats_model, out_stream=sys.stdout, err_stream=sys.stderr, verbose=True, debug=False, **fit_params)</code>","text":"<p>               Bases: <code>logger</code></p> <p>Generic class for lag fitting procedures. Contains parent methods for setting properties</p> <p>fitting_procedures extend the logger class and inherit all init args. See the logger documentation for details</p> <p>The only required argument is the stat_model to perform fitting for. All other fitting parameters (listed below) can be passed as keyword arguments at init, or via .set_config(), or reset to default values with .reset().</p> <p>Parameters:</p> Name Type Description Default <code>stat_model</code> <code>stats_model</code> <p>Statistics model to fit for</p> required Source code in <code>litmus/fitting_methods.py</code> <pre><code>def __init__(self, stat_model: stats_model,\n             out_stream=sys.stdout,\n             err_stream=sys.stderr,\n             verbose=True,\n             debug=False,\n             **fit_params):\n    \"\"\"\n    fitting_procedures extend the logger class and inherit all __init__ args. See the logger documentation for details\n\n    The only required argument is the stat_model to perform fitting for. All other fitting parameters (listed below) can be passed as keyword arguments at init, or via .set_config(), or reset to default values with .reset().\n\n    :param stats_model stat_model: Statistics model to fit for\n    \"\"\"\n\n    logger.__init__(self,\n                    out_stream=out_stream,\n                    err_stream=err_stream,\n                    verbose=verbose,\n                    debug=debug,\n                    )\n\n    if not hasattr(self, \"_default_params\"):\n        self._default_params = {}\n\n    # --------------------\n    # Attributes\n    self.stat_model: litmus.models.stats_model = stat_model\n    \"\"\"Stats model to do fitting for\"\"\"\n    self.name = \"Base Fitting Procedure\"\n    \"\"\"Name for string printing\"\"\"\n    self.is_ready = False\n    \"\"\"Flag to see if pre-pre-fitting has been done\"\"\"\n    self.has_prefit = False\n    \"\"\"Flag to see if pre-fitting has been done\"\"\"\n    self.has_run = False\n    \"\"\"Flag to see if fitting procedure has run to completion\"\"\"\n    self.fitting_params = {} | self._default_params\n    \"\"\"A keyed dict of tuning parameters for the fitting method\"\"\"\n    self.seed = _utils.randint() if \"seed\" not in fit_params.keys() else fit_params['seed']\n    \"\"\"Int seed for any randomized elements in the fitting method\"\"\"\n    # --------------------\n\n    self.set_config(**(self._default_params | fit_params))\n    self._tempseed = self.seed\n    self._data = None\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.fitting_procedure.stat_model","title":"<code>stat_model: litmus.models.stats_model = stat_model</code>  <code>instance-attribute</code>","text":"<p>Stats model to do fitting for</p>"},{"location":"fitting_methods/#litmus.fitting_methods.fitting_procedure.name","title":"<code>name = 'Base Fitting Procedure'</code>  <code>instance-attribute</code>","text":"<p>Name for string printing</p>"},{"location":"fitting_methods/#litmus.fitting_methods.fitting_procedure.is_ready","title":"<code>is_ready = False</code>  <code>instance-attribute</code>","text":"<p>Flag to see if pre-pre-fitting has been done</p>"},{"location":"fitting_methods/#litmus.fitting_methods.fitting_procedure.has_prefit","title":"<code>has_prefit = False</code>  <code>instance-attribute</code>","text":"<p>Flag to see if pre-fitting has been done</p>"},{"location":"fitting_methods/#litmus.fitting_methods.fitting_procedure.has_run","title":"<code>has_run = False</code>  <code>instance-attribute</code>","text":"<p>Flag to see if fitting procedure has run to completion</p>"},{"location":"fitting_methods/#litmus.fitting_methods.fitting_procedure.fitting_params","title":"<code>fitting_params = {} | self._default_params</code>  <code>instance-attribute</code>","text":"<p>A keyed dict of tuning parameters for the fitting method</p>"},{"location":"fitting_methods/#litmus.fitting_methods.fitting_procedure.seed","title":"<code>seed = _utils.randint() if 'seed' not in fit_params.keys() else fit_params['seed']</code>  <code>instance-attribute</code>","text":"<p>Int seed for any randomized elements in the fitting method</p>"},{"location":"fitting_methods/#litmus.fitting_methods.fitting_procedure.reset","title":"<code>reset() -&gt; None</code>","text":"<p>Clears all memory and resets params to defaults</p> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"\n    Clears all memory and resets params to defaults\n    \"\"\"\n    self.set_config(**self._default_params)\n\n    self.has_run, self.is_ready = False, False\n\n    return\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.fitting_procedure.set_config","title":"<code>set_config(**fit_params) -&gt; None</code>","text":"<p>Configure fitting parameters for fitting_method() object Accepts any parameters present with a name in fitting_method.fitting_params Unlisted parameters will be ignored.</p> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def set_config(self, **fit_params) -&gt; None:\n    \"\"\"\n    Configure fitting parameters for fitting_method() object\n    Accepts any parameters present with a name in fitting_method.fitting_params\n    Unlisted parameters will be ignored.\n    \"\"\"\n\n    self.msg_debug(\"Doing config with keys\", fit_params.keys())\n\n    badkeys = [key for key in fit_params.keys() if key not in self._default_params.keys()]\n\n    for key, val in zip(fit_params.keys(), fit_params.values()):\n        if key in badkeys: continue\n\n        # If something's changed, flag as having not run\n        currval = self.__getattribute__(key)\n        if self.has_run and val != currval: self.has_run = False\n\n        self.__setattr__(key, val)\n        # self.fitting_params |= {key: val}\n        self.msg_debug(\"\\t set attr\", key)\n\n    if len(badkeys) &gt; 0:\n        self.msg_err(\"Tried to configure bad keys:\", *badkeys, delim='\\t')\n    return\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.fitting_procedure.readyup","title":"<code>readyup() -&gt; None</code>","text":"<p>Performs pre-fit preparation calcs. Should only be called if not self.is_ready()</p> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def readyup(self) -&gt; None:\n    \"\"\"\n    Performs pre-fit preparation calcs. Should only be called if not self.is_ready()\n    \"\"\"\n    self.is_ready = True\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.fitting_procedure.prefit","title":"<code>prefit(lc_1: lightcurve, lc_2: lightcurve, seed: int = None) -&gt; None</code>","text":"<p>Performs any tasks required after setup but prior to actual fitting</p> <p>Parameters:</p> Name Type Description Default <code>lc_1</code> <code>lightcurve</code> <p>Lightcurve 1 (Main)</p> required <code>lc_2</code> <code>lightcurve</code> <p>Lightcurve 2 (Response)</p> required <code>seed</code> <code>int</code> <p>A random seed for feeding to the fitting process. If none, will select randomly</p> <code>None</code> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def prefit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None) -&gt; None:\n    \"\"\"\n    Performs any tasks required after setup but prior to actual fitting\n    :param lc_1: Lightcurve 1 (Main)\n    :param lc_2: Lightcurve 2 (Response)\n    :param seed: A random seed for feeding to the fitting process. If none, will select randomly\n    \"\"\"\n\n    self.has_prefit = True\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.fitting_procedure.fit","title":"<code>fit(lc_1: lightcurve, lc_2: lightcurve, seed: int = None) -&gt; None</code>","text":"<p>Performs the lag recovery method for this fitting procedure. If not prefit, will run prefit()</p> <p>Parameters:</p> Name Type Description Default <code>lc_1</code> <code>lightcurve</code> <p>Lightcurve 1 (Main)</p> required <code>lc_2</code> <code>lightcurve</code> <p>Lightcurve 2 (Response)</p> required <code>seed</code> <code>int</code> <p>A random seed for feeding to the fitting process. If none, will select randomly</p> <code>None</code> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def fit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None) -&gt; None:\n    \"\"\"\n    Performs the lag recovery method for this fitting procedure. If not prefit, will run prefit()\n    :param lc_1: Lightcurve 1 (Main)\n    :param lc_2: Lightcurve 2 (Response)\n    :param seed: A random seed for feeding to the fitting process. If none, will select randomly\n    \"\"\"\n\n    # Sanity checks inherited by all subclasses\n    if not self.is_ready: self.readyup()\n    if isinstance(seed, int):\n        self._tempseed = seed\n        self._tempseed = _utils.randint()\n        self._tempseed = _utils.randint()\n    seed = self._tempseed\n\n    data = self.stat_model.lc_to_data(lc_1, lc_2)\n    self._data = data\n\n    # An error message raised if this fitting procedure doesn't have .fit()\n    if self.__class__.fit == fitting_procedure.fit:\n        self.msg_err(\"Fitting \\\"%s\\\" method does not have method .fit() implemented\" % self.name)\n\n    return\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.fitting_procedure.get_samples","title":"<code>get_samples(N: int = None, seed: int = None, importance_sampling: bool = False) -&gt; {str: [float]}</code>","text":"<p>Returns MCMC-like posterior samples if fit() has been run</p> <p>Parameters:</p> Name Type Description Default <code>N</code> <code>int</code> <p>Number of samples to return. If None, return all</p> <code>None</code> <code>seed</code> <code>int</code> <p>Random seed for any stochastic elements</p> <code>None</code> <code>importance_sampling</code> <code>bool</code> <p>If true, will weight the results by</p> <code>False</code> <p>Returns:</p> Type Description <code>{str: [float]}</code> <p>keyed dict of samples in the constrained domain</p> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def get_samples(self, N: int = None, seed: int = None, importance_sampling: bool = False) -&gt; {str: [float]}:\n    \"\"\"\n    Returns MCMC-like posterior samples if fit() has been run\n    :param N: Number of samples to return. If None, return all\n    :param seed: Random seed for any stochastic elements\n    :param importance_sampling: If true, will weight the results by\n    :return: keyed dict of samples in the constrained domain\n    \"\"\"\n\n    if not self.is_ready: self.readyup()\n    if isinstance(seed, int):\n        self._tempseed = seed\n        self._tempseed = _utils.randint()\n    seed = self._tempseed\n\n    if self.__class__.fit == fitting_procedure.fit:\n        self.msg_err(\"Fitting \\\"%s\\\" method does not have method .get_samples() implemented\" % self.name)\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.fitting_procedure.get_evidence","title":"<code>get_evidence(seed: int = None, return_type='linear') -&gt; [float, float, float]</code>","text":"<p>Returns the estimated evidence for the fit model.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>int seed for random number generation</p> <code>None</code> <code>return_type</code> <p>if 'linear', returns as array-like [Z, -dZ-, dZ+]. If 'log', returns as array-like [logZ, -dlogZ-, dlogZ+]</p> <code>'linear'</code> <p>Returns:</p> Type Description <code>[float, float, float]</code> <p>len 3 array of type [mu, -E-, E+]</p> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def get_evidence(self, seed: int = None, return_type='linear') -&gt; [float, float, float]:\n    \"\"\"\n    Returns the estimated evidence for the fit model.\n    :param seed: int seed for random number generation\n    :param return_type: if 'linear', returns as array-like [Z, -dZ-, dZ+]. If 'log', returns as array-like [logZ, -dlogZ-, dlogZ+]\n    :return: len 3 array of type [mu, -E-, E+]\n    \"\"\"\n\n    assert return_type in ['linear', 'log'], \"Return type must be 'linear' or 'log'\"\n\n    if not self.is_ready: self.readyup()\n    if not self.has_run: self.msg_err(\"Warning! Tried to call get_evidence without running first!\")\n\n    if isinstance(seed, int):\n        self._tempseed = seed\n        self._tempseed = _utils.randint()\n    seed = self._tempseed\n\n    if self.__class__.get_evidence == fitting_procedure.get_evidence:\n        self.msg_err(\"Fitting \\\"%s\\\" method does not have method .get_evidence() implemented\" % self.name)\n        return np.array([0.0, 0.0, 0.0])\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.fitting_procedure.get_information","title":"<code>get_information(seed: int = None) -&gt; [float, float, float]</code>","text":"<p>Returns an estimate of the information (KL divergence relative to prior). Returns as array-like [I,dI-,dI+]</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>int seed for random number generation</p> <code>None</code> <p>Returns:</p> Type Description <code>[float, float, float]</code> <p>len 3 array of type [mu, -E-, E+]</p> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def get_information(self, seed: int = None) -&gt; [float, float, float]:\n    \"\"\"\n    Returns an estimate of the information (KL divergence relative to prior). Returns as array-like [I,dI-,dI+]\n    :param seed: int seed for random number generation\n    :return: len 3 array of type [mu, -E-, E+]\n    \"\"\"\n\n    if not self.is_ready: self.readyup()\n    if isinstance(seed, int):\n        self._tempseed = seed\n        self._tempseed = _utils.randint()\n    seed = self._tempseed\n\n    if self.__class__.get_information == fitting_procedure.get_information:\n        self.msg_err(\"Fitting \\\"%s\\\" method does not have method .get_information() implemented\" % self.name)\n\n        return np.array([0.0, 0.0, 0.0])\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.fitting_procedure.get_peaks","title":"<code>get_peaks(seed=None)</code>","text":"<p>Returns the maximum posterior position in parameter space</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <p>int seed for random number generation</p> <code>None</code> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def get_peaks(self, seed=None):\n    \"\"\"\n    Returns the maximum posterior position in parameter space\n    :param seed: int seed for random number generation\n    \"\"\"\n\n    if not self.is_ready: self.readyup()\n    if isinstance(seed, int):\n        self._tempseed = seed\n        self._tempseed = _utils.randint()\n    seed = self._tempseed\n\n    if self.__class__.get_peaks == fitting_procedure.get_peaks:\n        self.msg_err(\"Fitting \\\"%s\\\" method does not have method .get_peaks() implemented\" % self.name)\n\n        return {}, np.array([])\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.fitting_procedure.diagnostics","title":"<code>diagnostics(plot=True) -&gt; _types.Figure</code>","text":"<p>Generates some plots to check if the solver has converged</p> <p>Parameters:</p> Name Type Description Default <code>plot</code> <p>If True, run plt.show()</p> <code>True</code> <p>Returns:</p> Type Description <code>Figure</code> <p>generated matplotlib figure</p> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def diagnostics(self, plot=True) -&gt; _types.Figure:\n    \"\"\"\n    Generates some plots to check if the solver has converged\n    :param plot: If True, run plt.show()\n    :return: generated matplotlib figure\n    \"\"\"\n\n    if self.__class__.diagnostics == fitting_procedure.diagnostics:\n        self.msg_err(\"Fitting \\\"%s\\\" method does not have method .diagnostics() implemented\" % self.name)\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.fitting_procedure.diagnostic_lagplot","title":"<code>diagnostic_lagplot(plot=True) -&gt; _types.Figure</code>","text":"<p>Generates diagnostic plots specifically for lags</p> <p>Parameters:</p> Name Type Description Default <code>plot</code> <p>If True, run plt.show()</p> <code>True</code> <p>Returns:</p> Type Description <code>Figure</code> <p>generated matplotlib figure</p> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def diagnostic_lagplot(self, plot=True) -&gt; _types.Figure:\n    \"\"\"\n    Generates diagnostic plots specifically for lags\n    :param plot: If True, run plt.show()\n    :return: generated matplotlib figure\n    \"\"\"\n\n    if self.__class__.diagnostic_lagplot == fitting_procedure.diagnostic_lagplot:\n        self.msg_err(\"Fitting \\\"%s\\\" method does not have method .diagnostic_lagplot() implemented\" % self.name)\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.ICCF","title":"<code>ICCF(stat_model: stats_model, out_stream=sys.stdout, err_stream=sys.stderr, verbose=True, debug=False, **fit_params)</code>","text":"<p>               Bases: <code>fitting_procedure</code></p> <p>Fit lags using interpolated cross correlation function in the style of pyCCF. Note that this is not a Bayesian fitter and gives only approximate measures of the lag</p> <p>todo     - Add p value, false positive and evidence estimates (?)</p> <p>Parameters:</p> Name Type Description Default <code>stat_model</code> <code>stats_model</code> <p>Statistics model to fit for</p> required <code>Nboot</code> <code>int</code> <p>Number of bootstraps in ICCF</p> required <code>Nterp</code> <code>int</code> <p>Number of points to interpolate with in ICCF correlation evals</p> required <code>Nlags</code> <code>int</code> <p>Number of lags to test with.</p> required Source code in <code>litmus/fitting_methods.py</code> <pre><code>def __init__(self, stat_model: stats_model,\n             out_stream=sys.stdout, err_stream=sys.stderr,\n             verbose=True, debug=False, **fit_params):\n\n    \"\"\"\n    :param stats_model stat_model: Statistics model to fit for\n    :param int Nboot: Number of bootstraps in ICCF\n    :param int Nterp: Number of points to interpolate with in ICCF correlation evals\n    :param int Nlags: Number of lags to test with.\n    \"\"\"\n\n    args_in = {**locals(), **fit_params}\n    del args_in['self']\n    del args_in['__class__']\n    del args_in['fit_params']\n\n    if not hasattr(self, '_default_params'):\n        self._default_params = {}\n\n    self._default_params |= {\n        'Nboot': 512,\n        'Nterp': 2014,\n        'Nlags': 512,\n    }\n\n    super().__init__(**args_in)\n\n    # --------------------\n    # Attributes\n    self.name = \"ICCF Fitting Procedure\"\n\n    self.lags = np.zeros(self.Nterp)\n    \"\"\"Array of lags to test at\"\"\"\n    self.samples = np.zeros(self.Nboot)\n    \"\"\"The bootstrapped samples of the best fit lag\"\"\"\n    self.correls = np.zeros(self.Nterp)\n    \"\"\"Un-bootstrapped correlation function\"\"\"\n    self.lag_mean = 0.0\n    \"\"\"Mean of bootstrapped best fit lags\"\"\"\n    self.lag_err = 0.0\n    \"\"\"Std err of bootstrapped bestfit lags\"\"\"\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.ICCF.name","title":"<code>name = 'ICCF Fitting Procedure'</code>  <code>instance-attribute</code>","text":""},{"location":"fitting_methods/#litmus.fitting_methods.ICCF.lags","title":"<code>lags = np.zeros(self.Nterp)</code>  <code>instance-attribute</code>","text":"<p>Array of lags to test at</p>"},{"location":"fitting_methods/#litmus.fitting_methods.ICCF.samples","title":"<code>samples = np.zeros(self.Nboot)</code>  <code>instance-attribute</code>","text":"<p>The bootstrapped samples of the best fit lag</p>"},{"location":"fitting_methods/#litmus.fitting_methods.ICCF.correls","title":"<code>correls = np.zeros(self.Nterp)</code>  <code>instance-attribute</code>","text":"<p>Un-bootstrapped correlation function</p>"},{"location":"fitting_methods/#litmus.fitting_methods.ICCF.lag_mean","title":"<code>lag_mean = 0.0</code>  <code>instance-attribute</code>","text":"<p>Mean of bootstrapped best fit lags</p>"},{"location":"fitting_methods/#litmus.fitting_methods.ICCF.lag_err","title":"<code>lag_err = 0.0</code>  <code>instance-attribute</code>","text":"<p>Std err of bootstrapped bestfit lags</p>"},{"location":"fitting_methods/#litmus.fitting_methods.ICCF.set_config","title":"<code>set_config(**fit_params)</code>","text":"Source code in <code>litmus/fitting_methods.py</code> <pre><code>def set_config(self, **fit_params):\n    super().set_config(**fit_params)\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.ICCF.readyup","title":"<code>readyup()</code>","text":"Source code in <code>litmus/fitting_methods.py</code> <pre><code>def readyup(self):\n    super().readyup()\n    \"\"\"\n    # self.lags = jnp.linspace(*self.stat_model.prior_ranges['lag'], self.Nlags)\n    self.lags = np.random.randn(self.Nlags) * self.stat_model.prior_ranges['lag'].ptp() + \\\n                self.stat_model.prior_ranges['lag'][0]\n    \"\"\"\n    self.is_ready = True\n    self.has_prefit = False\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.ICCF.fit","title":"<code>fit(lc_1: lightcurve, lc_2: lightcurve, seed: int = None)</code>","text":"Source code in <code>litmus/fitting_methods.py</code> <pre><code>def fit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None):\n    # -------------------\n    fitting_procedure.fit(**locals())\n    seed = self._tempseed\n    # -------------------\n\n    # Unpack lightcurve\n    X1, Y1, E1 = lc_1.T, lc_1.Y, lc_1.E\n    X2, Y2, E2 = lc_2.T, lc_2.Y, lc_2.E\n\n    # Get interpolated correlation for all un-bootstrapped data\n    self.correls = iccf.correlfunc_jax_vmapped(self.lags, X1, Y1, X2, Y2, self.Nterp)\n\n    # Do bootstrap fitting\n    lagrange = jnp.linspace(*self.stat_model.prior_ranges['lag'], self.Nlags)\n    jax_samples = iccf.correl_func_boot_jax_wrapper_nomap(lagrange, X1, Y1, X2, Y2, E1, E2,\n                                                          Nterp=self.Nterp,\n                                                          Nboot=self.Nboot)\n\n    # Store Results\n    self.samples = jax_samples\n    self.lag_mean, self.lag_err = jax_samples.mean(), jax_samples.std()\n\n    self.has_run = True\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.ICCF.get_samples","title":"<code>get_samples(N: int = None, seed: int = None, importance_sampling: bool = False) -&gt; {str: [float]}</code>","text":"Source code in <code>litmus/fitting_methods.py</code> <pre><code>def get_samples(self, N: int = None, seed: int = None, importance_sampling: bool = False) -&gt; {str: [float]}:\n    # -------------------\n    fitting_procedure.get_samples(**locals())\n    seed = self._tempseed\n\n    if importance_sampling:\n        self.msg_err(\"Warning! Cannot use important sampling with ICCF. Try implementing manually\")\n        return\n    # -------------------\n\n    # Return entire sample chain or sub-set of samples\n    if N is None:\n        return ({'lag': self.samples})\n    else:\n        if N &gt; self.Nboot:\n            self.msg_err(\n                \"Warning, tried to get %i sub-samples from %i boot-strap iterations in ICCF\" % (N, self.Nboot),\n            )\n        return ({'lag': np.random.choice(a=self.samples, size=N, replace=True)})\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.ICCF.get_peaks","title":"<code>get_peaks(seed: int = None) -&gt; ({float: [float]}, [float])</code>","text":"Source code in <code>litmus/fitting_methods.py</code> <pre><code>def get_peaks(self, seed: int = None) -&gt; ({float: [float]}, [float]):\n    # -------------------\n    fitting_procedure.get_peaks(**locals())\n    seed = self._tempseed\n    # --------------\n    out = self.lags[np.argmax(self.correls)]\n    return {'lag': np.array([out])}\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.prior_sampling","title":"<code>prior_sampling(stat_model: stats_model, out_stream=sys.stdout, err_stream=sys.stderr, verbose=True, debug=False, **fit_params)</code>","text":"<p>               Bases: <code>fitting_procedure</code></p> <p>Randomly samples from the prior and weights with importance sampling. The crudest available sampler. For test purposes only, not suggested for actual use.</p> <p>Parameters:</p> Name Type Description Default <code>stat_model</code> <code>stats_model</code> <p>Statistics model to fit for</p> required <code>Nsamples</code> <code>int</code> <p>Number of samples to draw from the prior. Defaults to 4096.</p> required Source code in <code>litmus/fitting_methods.py</code> <pre><code>def __init__(self, stat_model: stats_model,\n             out_stream=sys.stdout, err_stream=sys.stderr,\n             verbose=True, debug=False, **fit_params):\n\n    \"\"\"\n    :param stats_model stat_model: Statistics model to fit for\n    :param int Nsamples: Number of samples to draw from the prior. Defaults to 4096.\n    \"\"\"\n\n    # ------------------------------------\n    args_in = {**locals(), **fit_params}\n    del args_in['self']\n    del args_in['__class__']\n    del args_in['fit_params']\n\n    if not hasattr(self, '_default_params'):\n        self._default_params = {}\n    self._default_params |= {\n        'Nsamples': 4096\n    }\n\n    super().__init__(**args_in)\n    # --------------------\n    # Attributes\n\n    self.name = \"Prior Sampling Fitting Procedure\"\n\n    self.samples = {key: np.zeros(self.Nsamples) for key in self.stat_model.paramnames()}\n    \"\"\"Samples drawn from the prior\"\"\"\n    self.log_prior = np.zeros(self.Nsamples)\n    \"\"\"Log prior densities of the samples\"\"\"\n    self.log_likes = np.zeros(self.Nsamples)\n    \"\"\"Log likelihood of the samples\"\"\"\n    self.log_density = np.zeros(self.Nsamples)\n    \"\"\"Log joint density of the samples\"\"\"\n    self.weights = np.zeros(self.Nsamples)\n    \"\"\"Normalized weights for drawing the samples\"\"\"\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.prior_sampling.name","title":"<code>name = 'Prior Sampling Fitting Procedure'</code>  <code>instance-attribute</code>","text":""},{"location":"fitting_methods/#litmus.fitting_methods.prior_sampling.samples","title":"<code>samples = {key: np.zeros(self.Nsamples)for key in self.stat_model.paramnames()}</code>  <code>instance-attribute</code>","text":"<p>Samples drawn from the prior</p>"},{"location":"fitting_methods/#litmus.fitting_methods.prior_sampling.log_prior","title":"<code>log_prior = np.zeros(self.Nsamples)</code>  <code>instance-attribute</code>","text":"<p>Log prior densities of the samples</p>"},{"location":"fitting_methods/#litmus.fitting_methods.prior_sampling.log_likes","title":"<code>log_likes = np.zeros(self.Nsamples)</code>  <code>instance-attribute</code>","text":"<p>Log likelihood of the samples</p>"},{"location":"fitting_methods/#litmus.fitting_methods.prior_sampling.log_density","title":"<code>log_density = np.zeros(self.Nsamples)</code>  <code>instance-attribute</code>","text":"<p>Log joint density of the samples</p>"},{"location":"fitting_methods/#litmus.fitting_methods.prior_sampling.weights","title":"<code>weights = np.zeros(self.Nsamples)</code>  <code>instance-attribute</code>","text":"<p>Normalized weights for drawing the samples</p>"},{"location":"fitting_methods/#litmus.fitting_methods.prior_sampling.fit","title":"<code>fit(lc_1: lightcurve, lc_2: lightcurve, seed: int = None)</code>","text":"Source code in <code>litmus/fitting_methods.py</code> <pre><code>def fit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None):\n    # -------------------\n    fitting_procedure.fit(**locals())\n    seed = self._tempseed\n    # -------------------\n\n    # Generate samples &amp; calculate likelihoods todo - These currently give posterior densities and not log likes\n    data = self.stat_model.lc_to_data(lc_1, lc_2)\n    samples = self.stat_model.prior_sample(num_samples=self.Nsamples, seed=seed)\n    log_density = self.stat_model.log_density(data=data, params=samples)\n    log_prior = self.stat_model.log_prior(params=samples)\n    log_likes = log_density - log_prior\n    likes = np.exp(log_likes)\n\n    # Store results\n    self.log_prior = log_prior\n    self.log_likes = log_likes\n    self.log_density = log_density\n    self.samples = samples\n    self.weights = likes / likes.sum()\n\n    # Mark as having completed a run\n    self.has_run = True\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.prior_sampling.get_samples","title":"<code>get_samples(N: int = None, seed: int = None, importance_sampling: bool = True) -&gt; {str: [float]}</code>","text":"Source code in <code>litmus/fitting_methods.py</code> <pre><code>def get_samples(self, N: int = None, seed: int = None, importance_sampling: bool = True) -&gt; {str: [float]}:\n    # -------------------\n    fitting_procedure.get_samples(**locals())\n    seed = self._tempseed\n    # -------------------\n\n    if N is None:\n        N = self.Nsamples\n    else:\n        if N &gt; self.Nsamples:\n            self.msg_err(\"Warning, tried to get %i sub-samples from %i samples\" % (N, self.Nsamples))\n\n    if importance_sampling:\n        weights = self.weights / self.weights.sum()\n    else:\n        weights = None\n\n    I = np.random.choice(a=np.arange(self.Nsamples), size=N, replace=True,\n                         p=weights)\n    return ({\n        key: val[I] for key, val in zip(self.samples.keys(), self.samples.values())\n    })\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.prior_sampling.get_evidence","title":"<code>get_evidence(seed=None, return_type='linear') -&gt; [float, float, float]</code>","text":"Source code in <code>litmus/fitting_methods.py</code> <pre><code>def get_evidence(self, seed=None, return_type='linear') -&gt; [float, float, float]:\n    # -------------------\n    fitting_procedure.get_evidence(**locals())\n    seed = self._tempseed\n    # -------------------\n    density = np.exp(self.log_density - self.log_density.max())\n\n    Z = density.mean() * self.stat_model.prior_volume * np.exp(self.log_density.max())\n    uncert = density.std() / np.sqrt(self.Nsamples) * self.stat_model.prior_volume\n\n    if return_type == 'linear':\n        np.array([Z, -uncert, uncert])\n    elif return_type == 'log':\n        np.array([np.log(Z), np.log(1 - uncert / Z), np.log(1 + uncert / Z)])\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.prior_sampling.get_information","title":"<code>get_information(seed: int = None) -&gt; [float, float, float]</code>","text":"Source code in <code>litmus/fitting_methods.py</code> <pre><code>def get_information(self, seed: int = None) -&gt; [float, float, float]:\n    # -------------------\n    fitting_procedure.get_information(**locals())\n    seed = self._tempseed\n    # -------------------\n    info_partial = np.random.choice(self.log_density - self.log_prior, self.Nsamples,\n                                    p=self.weights)\n    info = info_partial.mean() * self.stat_model.prior_volume\n    uncert = info_partial.std() / np.sqrt(self.Nsamples) * self.stat_model.prior_volume\n\n    return np.array([info, -uncert, uncert])\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.nested_sampling","title":"<code>nested_sampling(stat_model: stats_model, out_stream=sys.stdout, err_stream=sys.stderr, verbose=True, debug=False, **fit_params)</code>","text":"<p>               Bases: <code>fitting_procedure</code></p> <p>Fits the Bayesian model with Nested Sampling by using JAXNS. Highly accurate evidence / posterior distributions, but can be slow for models with more than a few parameters. Use only if hessian_scan and SVI_scan fail.</p> <p>Parameters:</p> Name Type Description Default <code>stat_model</code> <code>stats_model</code> <p>Statistics model to fit for</p> required <code>num_live_points</code> <code>int</code> <p>Number of live points to use in nested sampling fitting. Defaults to 500.</p> required <code>max_samples</code> <code>int</code> <p>Maximum samples before terminating the run. Defaults to 10_000.</p> required <code>num_parallel_samplers</code> <code>int</code> <p>Number of parallel samplers to fit with. Defaults to 1.</p> required <code>evidence_uncert</code> <code>float</code> <p>Termination condition for evidence uncertainty. Default to 1E-3.</p> required <code>live_evidence_frac</code> <code>float</code> <p>Termination condition for live fraction of evidence remaining. Defaults to log(1 + 1e-3).</p> required Source code in <code>litmus/fitting_methods.py</code> <pre><code>def __init__(self, stat_model: stats_model,\n             out_stream=sys.stdout, err_stream=sys.stderr,\n             verbose=True, debug=False, **fit_params):\n    \"\"\"\n    :param stats_model stat_model: Statistics model to fit for\n    :param int num_live_points: Number of live points to use in nested sampling fitting. Defaults to 500.\n    :param int max_samples: Maximum samples before terminating the run. Defaults to 10_000.\n    :param int num_parallel_samplers: Number of parallel samplers to fit with. Defaults to 1.\n    :param float evidence_uncert: Termination condition for evidence uncertainty. Default to 1E-3.\n    :param float live_evidence_frac: Termination condition for live fraction of evidence remaining. Defaults to log(1 + 1e-3).\n    \"\"\"\n\n    args_in = {**locals(), **fit_params}\n    del args_in['self']\n    del args_in['__class__']\n    del args_in['fit_params']\n\n    if not hasattr(self, '_default_params'):\n        self._default_params = {}\n    self._default_params |= {\n        'num_live_points': 500,\n        'max_samples': 10_000,\n        'num_parallel_samplers': 1,\n        'evidence_uncert': 1E-3,\n        'live_evidence_frac': np.log(1 + 1e-3),\n    }\n\n    super().__init__(**args_in)\n\n    self._jaxnsmodel = None\n    self._jaxnsresults = None\n    self._jaxnstermination = None\n    self._jaxnsresults = None\n\n    # --------------------\n    # Attributes\n    self.name = \"Nested Sampling Fitting Procedure\"\n    self.sampler = None\n    \"\"\"The JAXNS nested sampler object\"\"\"\n    self.logevidence = jnp.zeros(3)\n    \"\"\"JAXNS log evidence\"\"\"\n    self.priorvolume = 0.0\n    \"\"\"Prior volume for correcting from unit cube to actual density\"\"\"\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.nested_sampling.name","title":"<code>name = 'Nested Sampling Fitting Procedure'</code>  <code>instance-attribute</code>","text":""},{"location":"fitting_methods/#litmus.fitting_methods.nested_sampling.sampler","title":"<code>sampler = None</code>  <code>instance-attribute</code>","text":"<p>The JAXNS nested sampler object</p>"},{"location":"fitting_methods/#litmus.fitting_methods.nested_sampling.logevidence","title":"<code>logevidence = jnp.zeros(3)</code>  <code>instance-attribute</code>","text":"<p>JAXNS log evidence</p>"},{"location":"fitting_methods/#litmus.fitting_methods.nested_sampling.priorvolume","title":"<code>priorvolume = 0.0</code>  <code>instance-attribute</code>","text":"<p>Prior volume for correcting from unit cube to actual density</p>"},{"location":"fitting_methods/#litmus.fitting_methods.nested_sampling.prefit","title":"<code>prefit(lc_1: lightcurve, lc_2: lightcurve, seed: int = None)</code>","text":"Source code in <code>litmus/fitting_methods.py</code> <pre><code>def prefit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None):\n    # ---------------------\n    fitting_procedure.prefit(**locals())\n    if seed is None: seed = _utils.randint()\n    # ---------------------\n\n    # Get uniform prior bounds\n    bounds = np.array([self.stat_model.prior_ranges[key] for key in self.stat_model.paramnames()])\n    lo, hi = jnp.array(bounds[:, 0]), jnp.array(bounds[:, 1])\n\n    data = self.stat_model.lc_to_data(lc_1, lc_2)\n\n    # Construct jaxns friendly prior &amp; likelihood\n    def prior_model():\n        x = yield jaxns.Prior(tfpd.Uniform(low=lo, high=hi), name='x')\n        return x\n\n    def log_likelihood(x):\n        params = _utils.dict_unpack(x, keys=self.stat_model.paramnames())\n        with numpyro.handlers.block(hide=self.stat_model.paramnames()):\n            LL = self.stat_model._log_likelihood(params, data=data)\n        return LL\n\n    # jaxns object setup\n    self._jaxnsmodel = jaxns.Model(prior_model=prior_model,\n                                   log_likelihood=log_likelihood,\n                                   )\n\n    self._jaxnstermination = jaxns.TerminationCondition(\n        dlogZ=self.evidence_uncert,\n        max_samples=self.max_samples,\n    )\n\n    # Build jaxns Nested Sampler\n    self.sampler = jaxns.NestedSampler(\n        model=self._jaxnsmodel,\n        max_samples=self.max_samples,\n        verbose=self.debug,\n        num_live_points=self.num_live_points,\n        num_parallel_workers=self.num_parallel_samplers,\n        difficult_model=True,\n    )\n\n    self.has_prefit = True\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.nested_sampling.fit","title":"<code>fit(lc_1: lightcurve, lc_2: lightcurve, seed: int = None)</code>","text":"Source code in <code>litmus/fitting_methods.py</code> <pre><code>def fit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None):\n    # ---------------------\n    fitting_procedure.fit(**locals())\n    if seed is None: seed = _utils.randint()\n    # ---------------------\n    if not self.has_prefit:\n        self.prefit(lc_1, lc_2, seed)\n    self.readyup()\n    # ---------------------\n\n    # -----------------\n    # Run the sampler!\n    termination_reason, state = self.sampler(jax.random.PRNGKey(seed),\n                                             self._jaxnstermination)\n\n    # -----------------\n    # Extract &amp; save results\n    self._jaxnsresults = self.sampler.to_results(\n        termination_reason=termination_reason,\n        state=state\n    )\n\n    self.has_run = True\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.nested_sampling.get_samples","title":"<code>get_samples(N: int = None, seed: int = None, importance_sampling: bool = False) -&gt; {str: [float]}</code>","text":"Source code in <code>litmus/fitting_methods.py</code> <pre><code>def get_samples(self, N: int = None, seed: int = None, importance_sampling: bool = False) -&gt; {str: [float]}:\n    if seed is None: seed = _utils.randint()\n\n    samples, logweights = self._jaxnsresults.samples['x'], self._jaxnsresults.log_dp_mean\n\n    if N is None:\n        if importance_sampling:\n            out = {key: samples.T[i] for i, key in enumerate(self.stat_model.paramnames())}\n            return out\n        else:\n            N = samples.shape[0]\n\n    if importance_sampling:\n        logweights = jnp.zeros_like(logweights)\n\n    samples = jaxns.resample(\n        key=jax.random.PRNGKey(seed),\n        samples=samples,\n        log_weights=logweights,\n        S=N\n    )\n\n    out = {key: samples.T[i] for i, key in enumerate(self.stat_model.paramnames())}\n\n    return (out)\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.nested_sampling.get_evidence","title":"<code>get_evidence(seed: int = None, return_type='linear') -&gt; [float, float, float]</code>","text":"<p>Returns the -1, 0 and +1 sigma values for model evidence from nested sampling. This represents an estimate of numerical uncertainty</p> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def get_evidence(self, seed: int = None, return_type='linear') -&gt; [float, float, float]:\n    \"\"\"\n    Returns the -1, 0 and +1 sigma values for model evidence from nested sampling.\n    This represents an estimate of numerical uncertainty\n    \"\"\"\n\n    if seed is None: seed = _utils.randint()\n\n    l, l_e = self._jaxnsresults.log_Z_mean, self._jaxnsresults.log_Z_uncert\n\n    if return_type == 'linear':\n\n        out = np.exp([\n            l,\n            l - l_e,\n            l + l_e\n        ])\n\n        out -= np.array([0, out[0], out[0]])\n    elif return_type == 'log':\n        out = np.array([l, -l_e, l_e])\n    else:\n        self.msg_err(\n            \"Warning! Tried to call get_evidence in %s with bad return_type. Should be 'log' or 'linear'\" % self.name)\n        out = [0.0, 0.0, 0.0]\n\n    return out\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.nested_sampling.get_information","title":"<code>get_information(seed: int = None) -&gt; [float, float, float]</code>","text":"<p>Use the Nested Sampling shells to estimate the model information relative to prior</p> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def get_information(self, seed: int = None) -&gt; [float, float, float]:\n    \"\"\"\n    Use the Nested Sampling shells to estimate the model information relative to prior\n    \"\"\"\n    # todo - this is outmoded\n\n    if seed is None: seed = _utils.randint()\n\n    samples, logweights = self._jaxnsresults.samples, self._jaxnsresults.log_dp_mean\n\n    weights = np.exp(logweights)\n    weights /= weights.sum()\n\n    log_density = self._jaxnsresults.log_posterior_density\n    prior_values = self.stat_model.log_prior(samples)\n\n    info = np.sum((log_density - prior_values) * weights)\n\n    partial_info = np.random.choice((log_density - prior_values), len(log_density), p=weights)\n    uncert = partial_info.std() / np.sqrt(len(log_density))\n\n    return np.array(info, uncert, uncert)\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.nested_sampling.get_peaks","title":"<code>get_peaks(seed: int = None) -&gt; ({str: [float]}, float)</code>","text":"Source code in <code>litmus/fitting_methods.py</code> <pre><code>def get_peaks(self, seed: int = None) -&gt; ({str: [float]}, float):\n\n    # todo - this is outmoded\n\n    # ---------------------\n    if seed is None: seed = _utils.randint()\n    # ---------------------\n\n    self.msg_err(\"get_peaks currently placeholder.\")\n    return ({key: np.array([]) for key in self.stat_model.paramnames()}, np.array([]))\n\n    # ---------------------\n\n    NS = self.sampler\n    samples = self.get_samples()\n    log_densities = NS._results.log_posterior_density\n\n    # Find clusters\n    indices = clustering.clusterfind_1D(samples['lag'])\n\n    # Break samples and log-densities up into clusters\n    sorted_samples = clustering.sort_by_cluster(samples, indices)\n    sort_logdens = clustering.sort_by_cluster(log_densities, indices)\n\n    Nclusters = len(sorted_samples)\n\n    # Make an empty dictionary to store positions in\n    peak_locations = {key: np.zeros([Nclusters]) for key in samples.keys()}\n    peaklikes = np.zeros([Nclusters])\n\n    for i, group, lds in enumerate(sorted_samples, sort_logdens):\n        j = np.argmax(lds)\n        for key in samples.keys():\n            peak_locations[key][i] = group[key][j]\n        peaklikes[i] = lds[j]\n\n    return (peak_locations, peaklikes)\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.nested_sampling.diagnostics","title":"<code>diagnostics(show=True) -&gt; _types.Figure</code>","text":"Source code in <code>litmus/fitting_methods.py</code> <pre><code>def diagnostics(self, show=True) -&gt; _types.Figure:\n\n    # todo fix this to make the show work properly\n    jaxns.plot_diagnostics(self._jaxnsresults)\n    return plt.gcf()\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.hessian_scan","title":"<code>hessian_scan(stat_model: stats_model, out_stream=sys.stdout, err_stream=sys.stderr, verbose=True, debug=False, **fit_params)</code>","text":"<p>               Bases: <code>fitting_procedure</code></p> <p>Litmus's main hessian scan fitting procedure.</p> <p>Parameters:</p> Name Type Description Default <code>stat_model</code> <code>stats_model</code> <p>Statistics model to fit for.</p> required <code>Nlags'</code> <code>int</code> <p>Number of lag slices to test. Defaults to 64.</p> required <code>opt_tol'</code> <code>float</code> <p>Relative evidence uncertainty tolerance for each slice. Defaults to 1E-2.</p> required <code>opt_tol_init'</code> <code>float</code> <p>Relative uncertainty in MAP optimisation tolerance. Defaults to 1E-4.</p> required <code>step_size'</code> <code>float</code> <p>Defaults to 0.001.</p> required <code>constrained_domain'</code> <code>bool</code> <p>Whether to perform fitting / laplace approx in constrained domain. Defaults to False.</p> required <code>max_opt_eval'</code> <code>int</code> <p>Termination args eval limit for slice optimisation. Defaults to 1_000.</p> required <code>max_opt_eval_init'</code> <code>int</code> <p>Termination args eval limit for MAP optimisation.Defaults to 5_00.</p> required <code>LL_threshold'</code> <code>float</code> <p>Amount log-likelihood should decrement by before a slice is considered diverged. Defaults to 100.0.</p> required <code>init_samples'</code> <code>int</code> <p>Samples to use in finding seed. Defaults to 5_000.</p> required <code>grid_bunching'</code> <code>float</code> <p>Amount to bunch up points about peaks in grid smoothing, with 0.0 being even spacing and 1.0 being MCMC_like sample spacing. Defaults to 0.5.</p> required <code>grid_depth'</code> <p>Number of iterations to use in the grid_smoothing. Defaults to 5.</p> required <code>grid_Nterp'</code> <p>Number of evals in grid_smoothing itterations. Defaults to Nlags.</p> required <code>grid_firstdepth'</code> <code>float</code> <p>Factor to increase grid_Nterp by for the first pass in grid smoothing. Defaults to 10.0.</p> required <code>reverse'</code> <code>bool</code> <p>Whether to fit the slices in reverse order. Defaults to True.</p> required <code>split_lags'</code> <code>bool</code> <p>Whether to attack the lags in order of MAP outwards rather than end to end. Defaults to True.</p> required <code>optimizer_args_init'</code> <code>dict</code> <p>Args to over-write the jaxopt.BFGS defaults with in the initial MAP optimisation</p> required <code>optimizer_args'</code> <code>dict</code> <p>Args to over-write the jaxopt.BFGS defaults with in the slice optimisation</p> required <code>seed_params'</code> <code>dict</code> <p>Initial guess parameters to begin MAP estimation. If incomplete will use supplement with statmodel's .find_seed</p> required <code>precondition'</code> <code>str</code> <p>Type of preconditioning to use in scan. Should be \"cholesky\", \"eig\", \"half-eig\", \"diag\" or \"none\". Defaults to 'diag'.</p> required <code>interp_scale'</code> <code>str</code> <p>Scale to peform grid smoothing interpolation on . Defaults to 'log'.</p> required Source code in <code>litmus/fitting_methods.py</code> <pre><code>def __init__(self, stat_model: stats_model,\n             out_stream=sys.stdout, err_stream=sys.stderr,\n             verbose=True, debug=False, **fit_params):\n\n    \"\"\"\n    :param stats_model stat_model: Statistics model to fit for.\n    :param int Nlags': Number of lag slices to test. Defaults to 64.\n    :param float opt_tol': Relative evidence uncertainty tolerance for each slice. Defaults to 1E-2.\n    :param float opt_tol_init': Relative uncertainty in MAP optimisation tolerance. Defaults to 1E-4.\n    :param float step_size': Defaults to 0.001.\n    :param bool constrained_domain': Whether to perform fitting / laplace approx in constrained domain. Defaults to False.\n    :param int max_opt_eval': Termination args eval limit for slice optimisation. Defaults to 1_000.\n    :param int max_opt_eval_init': Termination args eval limit for MAP optimisation.Defaults to 5_00.\n    :param float LL_threshold': Amount log-likelihood should decrement by before a slice is considered diverged. Defaults to 100.0.\n    :param int init_samples': Samples to use in finding seed. Defaults to 5_000.\n    :param float grid_bunching': Amount to bunch up points about peaks in grid smoothing, with 0.0 being even spacing and 1.0 being MCMC_like sample spacing. Defaults to 0.5.\n    :param grid_depth': Number of iterations to use in the grid_smoothing. Defaults to 5.\n    :param grid_Nterp': Number of evals in grid_smoothing itterations. Defaults to Nlags.\n    :param float grid_firstdepth': Factor to increase grid_Nterp by for the first pass in grid smoothing. Defaults to 10.0.\n    :param bool reverse': Whether to fit the slices in reverse order. Defaults to True.\n    :param bool split_lags': Whether to attack the lags in order of MAP outwards rather than end to end. Defaults to True.\n    :param dict optimizer_args_init': Args to over-write the jaxopt.BFGS defaults with in the initial MAP optimisation\n    :param dict optimizer_args': Args to over-write the jaxopt.BFGS defaults with in the slice optimisation\n    :param dict seed_params': Initial guess parameters to begin MAP estimation. If incomplete will use supplement with statmodel's .find_seed\n    :param str precondition': Type of preconditioning to use in scan. Should be \"cholesky\", \"eig\", \"half-eig\", \"diag\" or \"none\". Defaults to 'diag'.\n    :param str interp_scale': Scale to peform grid smoothing interpolation on . Defaults to 'log'.\n    \"\"\"\n    args_in = {**locals(), **fit_params}\n    del args_in['self']\n    del args_in['__class__']\n    del args_in['fit_params']\n\n    if not hasattr(self, '_default_params'):\n        self._default_params = {}\n\n    self._default_params |= {\n        'Nlags': 64,\n        'opt_tol': 1E-2,\n        'opt_tol_init': 1E-4,\n        'step_size': 0.001,\n        'constrained_domain': False,\n        'max_opt_eval': 1_000,\n        'max_opt_eval_init': 5_000,\n        'LL_threshold': 100.0,\n        'init_samples': 5_000,\n        'grid_bunching': 0.5,\n        'grid_depth': 5,\n        'grid_Nterp': None,\n        'grid_firstdepth': 10.0,\n        'reverse': True,\n        'split_lags': True,\n        'optimizer_args_init': {},\n        'optimizer_args': {},\n        'seed_params': {},\n        'precondition': 'diag',\n        'interp_scale': 'log',\n    }\n\n    self._allowable_interpscales = ['linear', 'log']\n\n    super().__init__(**args_in)\n\n    # -----------------------------------\n\n    self.name = \"Hessian Scan Fitting Procedure\"\n\n    self.lags: [float] = np.zeros(self.Nlags)\n    self.converged: np.ndarray[bool] = np.zeros_like(self.lags, dtype=bool)\n\n    self.scan_peaks: dict = {}\n    self.log_evidences: list = []\n    self.log_evidences_uncert: list = []\n\n    self.diagnostic_hessians: list = []\n    self.diagnostic_densities: list = []\n    self.diagnostic_grads: list = []\n    self.diagnostic_ints: list = []\n    self.diagnostic_tgrads: list = []\n\n    self.params_toscan = self.stat_model.free_params()\n    if 'lag' in self.params_toscan: self.params_toscan.remove('lag')\n\n    self.precon_matrix: np.ndarray[np.float64] = np.eye(len(self.params_toscan), dtype=np.float64)\n    self.solver: jaxopt.BFGS = None\n\n    self.estmap_params = {}\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.hessian_scan.name","title":"<code>name = 'Hessian Scan Fitting Procedure'</code>  <code>instance-attribute</code>","text":""},{"location":"fitting_methods/#litmus.fitting_methods.hessian_scan.lags","title":"<code>lags: [float] = np.zeros(self.Nlags)</code>  <code>instance-attribute</code>","text":""},{"location":"fitting_methods/#litmus.fitting_methods.hessian_scan.converged","title":"<code>converged: np.ndarray[bool] = np.zeros_like(self.lags, dtype=bool)</code>  <code>instance-attribute</code>","text":""},{"location":"fitting_methods/#litmus.fitting_methods.hessian_scan.scan_peaks","title":"<code>scan_peaks: dict = {}</code>  <code>instance-attribute</code>","text":""},{"location":"fitting_methods/#litmus.fitting_methods.hessian_scan.log_evidences","title":"<code>log_evidences: list = []</code>  <code>instance-attribute</code>","text":""},{"location":"fitting_methods/#litmus.fitting_methods.hessian_scan.log_evidences_uncert","title":"<code>log_evidences_uncert: list = []</code>  <code>instance-attribute</code>","text":""},{"location":"fitting_methods/#litmus.fitting_methods.hessian_scan.diagnostic_hessians","title":"<code>diagnostic_hessians: list = []</code>  <code>instance-attribute</code>","text":""},{"location":"fitting_methods/#litmus.fitting_methods.hessian_scan.diagnostic_densities","title":"<code>diagnostic_densities: list = []</code>  <code>instance-attribute</code>","text":""},{"location":"fitting_methods/#litmus.fitting_methods.hessian_scan.diagnostic_grads","title":"<code>diagnostic_grads: list = []</code>  <code>instance-attribute</code>","text":""},{"location":"fitting_methods/#litmus.fitting_methods.hessian_scan.diagnostic_ints","title":"<code>diagnostic_ints: list = []</code>  <code>instance-attribute</code>","text":""},{"location":"fitting_methods/#litmus.fitting_methods.hessian_scan.diagnostic_tgrads","title":"<code>diagnostic_tgrads: list = []</code>  <code>instance-attribute</code>","text":""},{"location":"fitting_methods/#litmus.fitting_methods.hessian_scan.params_toscan","title":"<code>params_toscan = self.stat_model.free_params()</code>  <code>instance-attribute</code>","text":""},{"location":"fitting_methods/#litmus.fitting_methods.hessian_scan.precon_matrix","title":"<code>precon_matrix: np.ndarray[np.float64] = np.eye(len(self.params_toscan), dtype=np.float64)</code>  <code>instance-attribute</code>","text":""},{"location":"fitting_methods/#litmus.fitting_methods.hessian_scan.solver","title":"<code>solver: jaxopt.BFGS = None</code>  <code>instance-attribute</code>","text":""},{"location":"fitting_methods/#litmus.fitting_methods.hessian_scan.estmap_params","title":"<code>estmap_params = {}</code>  <code>instance-attribute</code>","text":""},{"location":"fitting_methods/#litmus.fitting_methods.hessian_scan.readyup","title":"<code>readyup()</code>","text":"Source code in <code>litmus/fitting_methods.py</code> <pre><code>def readyup(self):\n\n    # Get grid properties\n    if self.grid_Nterp is None:\n        self.grid_Nterp = self.Nlags\n\n    # Make list of lags for scanning\n    self.lags = np.linspace(*self.stat_model.prior_ranges['lag'], self.Nlags + 1, endpoint=False)[1:]\n    self.converged = np.zeros_like(self.lags, dtype=bool)\n\n    free_dims = len(self.stat_model.free_params())\n    self.scan_peaks = {key: np.array([]) for key in self.stat_model.paramnames()}\n    self.diagnostic_hessians = []\n    self.diagnostic_grads = []\n    self.diagnostic_densities = []\n    self.log_evidences_uncert = []\n\n    self.params_toscan = [key for key in self.stat_model.paramnames() if\n                          key not in ['lag'] and key in self.stat_model.free_params()\n                          ]\n\n    self.is_ready = True\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.hessian_scan.estimate_MAP","title":"<code>estimate_MAP(lc_1: lightcurve, lc_2: lightcurve, seed: int = None)</code>","text":"Source code in <code>litmus/fitting_methods.py</code> <pre><code>def estimate_MAP(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None):\n    data = self.stat_model.lc_to_data(lc_1, lc_2)\n\n    # ----------------------------------\n    # Find seed for optimization if not supplied\n    if self.stat_model.free_params() != self.seed_params.keys():\n        seed_params, ll_start = self.stat_model.find_seed(data, guesses=self.init_samples, fixed=self.seed_params)\n\n        self.msg_run(\"Beginning scan at constrained-space position:\")\n        for it in seed_params.items():\n            self.msg_run('\\t %s: \\t %.2f' % (it[0], it[1]))\n        self.msg_run(\n            \"Log-Density for this is: %.2f\" % ll_start)\n\n    else:\n        seed_params = self.seed_params\n        ll_start = self.stat_model.log_density(seed_params,\n                                               data=data\n                                               )\n\n    # ----------------------------------\n    # SCANNING FOR OPT\n\n    self.msg_run(\"Moving non-lag params to new location...\")\n    estmap_params = self.stat_model.scan(start_params=seed_params,\n                                         optim_params=[key for key in self.stat_model.free_params() if\n                                                       key != 'lag'],\n                                         data=data,\n                                         optim_kwargs=self.optimizer_args_init,\n                                         precondition=self.precondition\n                                         )\n    ll_firstscan = self.stat_model.log_density(estmap_params, data)\n    if 'lag' in self.stat_model.free_params():\n        self.msg_run(\"Optimizer settled at new fit:\")\n        for it in estmap_params.items():\n            self.msg_run('\\t %s: \\t %.2f' % (it[0], it[1]))\n        self.msg_run(\n            \"Log-Density for this is: %.2f\" % ll_firstscan\n        )\n\n        self.msg_run(\"Finding a good lag...\")\n        test_lags = self.stat_model.prior_sample(self.init_samples)['lag']\n        test_samples = _utils.dict_extend(estmap_params, {'lag': test_lags})\n        ll_test = self.stat_model.log_density(test_samples, data)\n        bestlag = test_lags[ll_test.argmax()]\n\n        self.msg_run(\"Grid finds good lag at %.2f:\" % bestlag)\n        self.msg_run(\n            \"Log-Density for this is: %.2f\" % ll_test.max()\n        )\n\n        if ll_test.max() &gt; ll_firstscan:\n            bestlag = bestlag\n        else:\n            bestlag = estmap_params['lag']\n\n        self.msg_run(\"Running final optimization...\")\n        estmap_params = self.stat_model.scan(start_params=estmap_params | {'lag': bestlag},\n                                             # optim_params=['lag'],\n                                             data=data,\n                                             optim_kwargs=self.optimizer_args_init,\n                                             precondition=self.precondition\n                                             )\n\n        self.msg_run(\"Lag-only opt settled at new lag %.2f...\" % estmap_params['lag'])\n\n    ll_end = self.stat_model.log_density(estmap_params,\n                                         data=data\n                                         )\n\n    # ----------------------------------\n    # CHECKING OUTPUTS\n\n    self.msg_run(\"Optimizer settled at new fit:\")\n    for it in estmap_params.items():\n        self.msg_run('\\t %s: \\t %.2f' % (it[0], it[1]))\n    self.msg_run(\n        \"Log-Density for this is: %.2f\" % ll_end\n    )\n\n    # ----------------------------------\n    # CHECKING OUTPUTS\n\n    if ll_end &lt; ll_start:\n        self.msg_err(\"Warning! Optimization seems to have diverged. Defaulting to seed params. \\n\"\n                     \"Please consider running with different optim_init inputs\")\n        estmap_params = seed_params\n    return estmap_params\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.hessian_scan.make_grid","title":"<code>make_grid(data, seed_params=None, interp_scale='log') -&gt; _types.ArrayN</code>","text":"<p>Generates a grid of test lags for use in the hessian scan via the grid smoothing algorithm listed in the paper</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>data to condition the model on</p> required <code>seed_params</code> <p>An initial guess for the seed parameters formed by common sense. If None or incomplete, is filled using the statmodels find_seed method.</p> <code>None</code> <code>interp_scale</code> <p>What scale to perform interpolation between the test lags at, 'log' or 'linear'</p> <code>'log'</code> <p>Returns:</p> Type Description <code>ArrayN</code> <p>Array of lags of len self.Nlags</p> Source code in <code>litmus/fitting_methods.py</code> <pre><code>def make_grid(self, data, seed_params=None, interp_scale='log') -&gt; _types.ArrayN:\n    \"\"\"\n    Generates a grid of test lags for use in the hessian scan via the grid smoothing algorithm listed in the paper\n    :param data: data to condition the model on\n    :param seed_params: An initial guess for the seed parameters formed by common sense. If None or incomplete, is filled using the statmodels find_seed method.\n    :param interp_scale: What scale to perform interpolation between the test lags at, 'log' or 'linear'\n    :return: Array of lags of len self.Nlags\n    \"\"\"\n\n    assert interp_scale in ['log', 'linear'], \"Interp scale was %s, must be in 'log' or 'linear'\" % interp_scale\n\n    if not self.is_ready: self.readyup()\n\n    if 'lag' in self.stat_model.fixed_params():\n        lags = np.array([np.mean(self.stat_model.prior_ranges['lag'])])\n        self.Nlags = 1\n        self.readyup()\n        return lags\n\n    # If no seed parameters specified, use stored\n    if seed_params is None:\n        seed_params = self.estmap_params\n\n    # If these params are incomplete, use find_seed to complete them\n    if seed_params.keys() != self.stat_model.paramnames():\n        seed_params, llstart = self.stat_model.find_seed(data, guesses=self.init_samples, fixed=seed_params)\n\n    self.msg_debug(\"Making Grid with interp scale %s\" % interp_scale)\n    lags = np.linspace(*self.stat_model.prior_ranges['lag'], int(self.Nlags * self.grid_firstdepth) + 1,\n                       endpoint=False)[1:]\n    lag_terp = np.linspace(*self.stat_model.prior_ranges['lag'], self.grid_Nterp)\n\n    log_density_all, lags_all = np.empty(shape=(1,)), np.empty(shape=(1,))\n    for i in range(self.grid_depth):\n        self.msg_debug(\"Pass number:\\t %i Any Errors?\\t %r\" % (\n            i, np.any([*np.isnan(log_density_all), *np.isinf(log_density_all)]))\n                       )\n        params = _utils.dict_extend(seed_params, {'lag': lags})\n        log_density_all = np.concatenate([log_density_all, self.stat_model.log_density(params, data)])\n        lags_all = np.concatenate([lags_all, lags])\n        check = np.isinf(log_density_all) + np.isnan(log_density_all)\n\n        # Check for broken nodes then argsort\n        I = np.where(check == False)[0]\n        log_density_all, lags_all = log_density_all[I], lags_all[I]\n        I = lags_all.argsort()\n        log_density_all, lags_all = log_density_all[I], lags_all[I]\n\n        if interp_scale == 'linear':\n\n            density = np.exp(log_density_all - log_density_all.max())\n\n            # Linearly interpolate the density profile\n            density_terp = np.interp(lag_terp, lags_all, density, left=0, right=0)\n            density_terp /= density_terp.sum()\n\n\n        elif interp_scale == 'log':\n\n            density = np.exp(log_density_all - log_density_all.max())\n\n            # Linearly interpolate the density profile\n            log_density_terp = np.interp(lag_terp, log_density_all - log_density_all.max(), density,\n                                         left=log_density_all[0], right=log_density_all[-1])\n            density_terp = np.exp(log_density_terp - log_density_terp.max())\n            density_terp /= density_terp.sum()\n\n        gets = np.linspace(0, 1, self.grid_Nterp)\n        percentiles = np.cumsum(density_terp) * self.grid_bunching + gets * (1 - self.grid_bunching)\n        percentiles /= percentiles.max()\n\n        lags = np.interp(np.linspace(0, 1, self.Nlags), percentiles, lag_terp,\n                         left=lag_terp.min(),\n                         right=lag_terp.max()\n                         )\n\n    return lags\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.hessian_scan.prefit","title":"<code>prefit(lc_1: lightcurve, lc_2: lightcurve, seed: int = None)</code>","text":"Source code in <code>litmus/fitting_methods.py</code> <pre><code>def prefit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None):\n    # -------------------\n    fitting_procedure.prefit(**locals())\n    seed = self._tempseed\n    # -------------------\n\n    data = self.stat_model.lc_to_data(lc_1, lc_2)\n\n    # ----------------------------------\n    # Estimate the MAP\n\n    self.estmap_params = self.estimate_MAP(lc_1, lc_2, seed)\n    self.estmap_tol = self.stat_model.opt_tol(self.estmap_params, data,\n                                              integrate_axes=self.stat_model.free_params())\n    self.msg_run(\"Estimated to be within \u00b1%.2e\u03c3 of local optimum\" % self.estmap_tol)\n    # ----------------------------------\n\n    # Make a grid\n\n    lags = self.make_grid(data, seed_params=self.estmap_params)\n    if self.split_lags:\n        split_index = abs(lags - self.estmap_params['lag']).argmin()\n        lags_left, lags_right = lags[:split_index], lags[split_index:]\n        lags = np.concatenate([lags_right, lags_left[::-1]])\n        if self.reverse: lags = lags = np.concatenate([lags_left[::-1], lags_right])\n    elif self.reverse:\n        lags = lags[::-1]\n    self.lags = lags\n\n    self.has_prefit = True\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.hessian_scan.fit","title":"<code>fit(lc_1: lightcurve, lc_2: lightcurve, seed: int = None)</code>","text":"Source code in <code>litmus/fitting_methods.py</code> <pre><code>def fit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None):\n    # -------------------\n    fitting_procedure.fit(**locals())\n    seed = self._tempseed\n    # -------------------\n    # Setup + prefit if not run\n    self.msg_run(\"Starting Hessian Scan\")\n\n    data = self.stat_model.lc_to_data(lc_1, lc_2)\n\n    if not self.has_prefit:\n        self.prefit(lc_1, lc_2, seed)\n    best_params = self.estmap_params.copy()\n\n    # ----------------------------------\n    # Create scanner and perform setup\n    params_toscan = self.params_toscan\n    lags_forscan = self.lags.copy()\n\n    solver, runsolver, [converter, deconverter, optfunc, runsolver_jit] = self.stat_model._scanner(data,\n                                                                                                   optim_params=params_toscan,\n                                                                                                   optim_kwargs=self.optimizer_args,\n                                                                                                   return_aux=True\n                                                                                                   )\n    self.solver = solver\n    x0, y0 = converter(best_params)\n    state = solver.init_state(x0, y0, data)\n\n    # ----------------------------------\n    # Sweep over lags\n    scanned_optima, grads, Hs = [], [], []\n    tols, Zs, Ints, tgrads = [], [], [], []\n    for i, lag in enumerate(lags_forscan):\n        self.msg_run(\":\" * 23)\n        self.msg_run(\"Scanning at lag=%.2f ...\" % lag)\n\n        # Get current param site in packed-function friendly terms\n        opt_params, aux_data, state = runsolver_jit(solver, best_params | {'lag': lag}, state)\n\n        # --------------\n        # Check if the optimization has succeeded or broken\n\n        l_1 = self.stat_model.log_density(best_params | {'lag': lag}, data)\n        l_2 = self.stat_model.log_density(opt_params | {'lag': lag}, data)\n        bigdrop = l_2 - l_1 &lt; -self.LL_threshold\n        diverged = np.any(np.isinf(np.array([x for x in self.stat_model.to_uncon(opt_params).values()])))\n\n        self.msg_run(\"Change of %.2f against %.2f\" % (l_2 - l_1, self.LL_threshold))\n\n        if not bigdrop and not diverged:\n            self.converged[i] = True\n\n            is_good = [True, True, True]\n\n            # ======\n            # Check position &amp; Grad\n            try:\n                uncon_params = self.stat_model.to_uncon(opt_params)\n                log_height = self.stat_model.log_density_uncon(uncon_params, data)\n            except:\n                self.msg_err(\"Something wrong!\")\n                is_good[0] = False\n\n            # ======\n            # Check tolerances &amp; hessians\n            try:\n                H = self.stat_model.log_density_uncon_hess(uncon_params, data, keys=params_toscan)\n                assert np.linalg.det(H), \"Error in H calc\"\n                tol = self.stat_model.opt_tol(opt_params, data, integrate_axes=params_toscan)\n            except:\n                self.msg_err(\"Something wrong in Hessian / Tolerance!:\")\n                is_good[1] = False\n\n            # ======\n            # Get evidence\n            try:\n                laplace_int = self.stat_model.laplace_log_evidence(opt_params, data,\n                                                                   integrate_axes=params_toscan,\n                                                                   constrained=self.constrained_domain)\n                tgrad = self.stat_model.uncon_grad_lag(opt_params) if not self.constrained_domain else 0\n                Z = laplace_int + tgrad\n                assert not np.isnan(Z), \"Error in Z calc\"\n            except:\n                self.msg_err(\"Something wrong in Evidence!:\")\n                is_good[2] = False\n\n            # Check and save if good\n            if np.all(is_good):\n                self.msg_run(\n                    \"Seems to have converged at iteration %i / %i with tolerance %.2e\" % (i, self.Nlags, tol))\n\n                if tol &lt; 1.0:\n                    best_params = opt_params\n                else:\n                    self.msg_run(\"Possibly stuck in a furrow. Resetting start params\")\n                    best_params = self.estmap_params.copy()\n\n                scanned_optima.append(opt_params.copy())\n                tols.append(tol)\n\n                grads.append(aux_data['grad'])\n                Hs.append(H)\n                Ints.append(laplace_int)\n                tgrads.append(tgrad)\n                Zs.append(Z)\n            else:\n                self.msg_run(\"Seems to have severely diverged at iteration %i / %i\" % (i, self.Nlags))\n                reason = [\"Eval\", \"hessian/tol\", \"evidence\"]\n                for a, b in zip(reason, is_good):\n                    self.msg_run(\"%s:\\t%r\" % (a, b))\n\n        else:\n            self.converged[i] = False\n            self.msg_run(\"Unable to converge at iteration %i / %i\" % (i, self.Nlags),\n                         \"\\nLarge Drop?:\\t\", bigdrop,\n                         \"\\nOptimizer Diverged:\\t\", diverged)\n\n    if sum(self.converged) == 0:\n        self.msg_err(\"All slices catastrophically diverged! Try different starting conditions and/or grid spacing\")\n\n    self.msg_run(\"Scanning Complete. Calculating laplace integrals...\")\n\n    # --------\n    # Save and apply\n    self.diagnostic_grads = grads\n    self.diagnostic_hessians = Hs\n    self.diagnostic_tgrads = np.array(tgrads).squeeze().flatten()\n    self.diagnostic_ints = np.array(Ints).squeeze().flatten()\n\n    self.scan_peaks = _utils.dict_combine(scanned_optima)\n    self.diagnostic_densities = self.stat_model.log_density(self.scan_peaks, data)\n    self.log_evidences = np.array(Zs).squeeze().flatten()\n    self.log_evidences_uncert = np.square(tols).squeeze().flatten()\n\n    self.msg_run(\"Hessian Scan Fitting complete.\", \"-\" * 23, \"-\" * 23, delim='\\n')\n    self.has_run = True\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.hessian_scan.refit","title":"<code>refit(lc_1: lightcurve, lc_2: lightcurve, seed: int = None)</code>","text":"Source code in <code>litmus/fitting_methods.py</code> <pre><code>def refit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None):\n    # -------------------\n    fitting_procedure.fit(**locals())\n    seed = self._tempseed\n    # -------------------\n\n    data = self.stat_model.lc_to_data(lc_1, lc_2)\n\n    peaks = _utils.dict_divide(self.scan_peaks)\n    I = np.arange(len(peaks))\n    select = np.argwhere(self.log_evidences_uncert &gt; self.opt_tol).squeeze()\n    if not (_utils.isiter(select)): select = np.array([select])\n\n    peaks, I = np.array(peaks)[select], I[select]\n\n    self.msg_run(\"Doing re-fitting of %i lags\" % len(peaks))\n    newtols = []\n    for j, i, peak in zip(range(len(I)), I, peaks):\n\n        self.msg_run(\":\" * 23, \"Refitting lag %i/%i at lag %.2f\" % (j, len(peaks), peak['lag']), delim='\\n')\n\n        ll_old = self.stat_model.log_density(peak, data)\n        old_tol = self.log_evidences_uncert[i]\n\n        new_peak = self.stat_model.scan(start_params=peak,\n                                        optim_params=self.params_toscan,\n                                        data=data,\n                                        optim_kwargs=self.optimizer_args,\n                                        precondition=self.precondition\n                                        )\n        ll_new = self.stat_model.log_density(new_peak, data)\n        if ll_old &gt; ll_new or np.isnan(ll_new):\n            self.msg_err(\"New peak bad (LL from %.2e to %.2e. Trying new start location\" % (ll_old, ll_new))\n            new_peak = self.stat_model.scan(start_params=self.estmap_params,\n                                            optim_params=self.params_toscan,\n                                            data=data,\n                                            optim_kwargs=self.optimizer_args,\n                                            precondition=self.precondition\n                                            )\n            ll_new = self.stat_model.log_density(new_peak, data)\n\n            if ll_old &gt; ll_new:\n                self.msg_err(\"New peak bad (LL from %.2e to %.2e. Trying new start location\" % (ll_old, ll_new))\n                continue\n\n        new_peak_uncon = self.stat_model.to_uncon(new_peak)\n        new_grad = _utils.dict_pack(new_grad, keys=self.params_toscan)\n        new_hessian = self.stat_model.log_density_uncon_hess(new_peak_uncon, data, keys=self.params_toscan)\n\n        try:\n            int = self.stat_model.laplace_log_evidence(new_peak, data, constrained=self.constrained_domain)\n            tgrad = self.stat_model.uncon_grad_lag(new_peak)\n            Z = tgrad + int\n            Hinv = np.linalg.inv(new_hessian)\n        except:\n            self.msg_run(\"Optimization failed on %i/%i\" % (j, len(peaks)))\n            continue\n\n        tol = self.stat_model.opt_tol(new_peak, data, self.params_toscan)\n\n        if tol &lt; old_tol:\n            self.diagnostic_tgrads[i] = tgrad\n            self.diagnostic_ints[i] = int\n            self.log_evidences[i] = tgrad + int\n\n            self.diagnostic_grads[i] = new_grad\n            self.diagnostic_hessians[i] = new_hessian\n            self.log_evidences_uncert[i] = tol ** 2\n            self.msg_run(\"Settled at new tol %.2e\" % tol)\n        else:\n            self.msg_run(\n                \"Something went wrong at this refit! Consider changing the optimizer_args and trying again\")\n    self.msg_run(\"Refitting complete.\")\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.hessian_scan.diagnostics","title":"<code>diagnostics(plot=True) -&gt; _types.Figure</code>","text":"Source code in <code>litmus/fitting_methods.py</code> <pre><code>def diagnostics(self, plot=True) -&gt; _types.Figure:\n\n    loss = self.log_evidences_uncert\n\n    lagplot = self.scan_peaks['lag']\n    I = self.scan_peaks['lag'].argsort()\n    lagplot = lagplot[I]\n    Y = self.estmap_tol[I]\n\n    # ---------\n    fig = plt.figure()\n    plt.ylabel(\"Loss Norm, $ \\\\vert \\Delta x / \\sigma_x \\\\vert$\")\n    plt.xlabel(\"Scan Lag No.\")\n    plt.plot(lagplot, loss, 'o-', c='k', label=\"Scan Losses\")\n    plt.scatter(self.estmap_params['lag'], Y, c='r', marker='x', s=40, label=\"Initial MAP Scan Loss\")\n    plt.axhline(self.opt_tol, ls='--', c='k', label=\"Nominal Tolerance Limit\")\n    plt.legend(loc='best')\n\n    fig.text(.5, .05, \"How far each optimization slice is from its peak. Lower is good.\", ha='center')\n    plt.yscale('log')\n    plt.grid()\n    plt.show()\n\n    return fig\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.hessian_scan.diagnostic_lagplot","title":"<code>diagnostic_lagplot(show=True) -&gt; _types.Figure</code>","text":"Source code in <code>litmus/fitting_methods.py</code> <pre><code>def diagnostic_lagplot(self, show=True) -&gt; _types.Figure:\n    f, (a1, a2) = plt.subplots(2, 1, sharex=True)\n\n    lags_forint, logZ_forint, density_forint = self._get_slices('lags', 'logZ', 'densities')\n\n    # ---------------------\n    for a in (a1, a2):\n        a.scatter(lags_forint, np.exp(logZ_forint - logZ_forint.max()), label=\"Evidence\")\n        a.scatter(lags_forint, np.exp(density_forint - density_forint.max()), label=\"Density\")\n        a.grid()\n\n    a2.set_yscale('log')\n    a1.legend()\n\n    # --------------\n    # Outputs\n    if show: plt.show()\n    return f\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.hessian_scan.get_evidence","title":"<code>get_evidence(seed: int = None, return_type='linear') -&gt; [float, float, float]</code>","text":"Source code in <code>litmus/fitting_methods.py</code> <pre><code>def get_evidence(self, seed: int = None, return_type='linear') -&gt; [float, float, float]:\n    # -------------------\n    fitting_procedure.get_evidence(**locals())\n    seed = self._tempseed\n    # -------------------\n\n    assert self.interp_scale in self._allowable_interpscales, \"Interp scale %s not recognised. Must be selection from %s\" % (\n        self.interp_scale, self._allowable_interpscales)\n\n    lags_forint, logZ_forint, logZ_uncert_forint = self._get_slices('lags', 'logZ', 'dlogZ')\n    minlag, maxlag = self.stat_model.prior_ranges['lag']\n\n    if maxlag - minlag == 0:\n        Z = np.exp(logZ_forint.max())\n        imax = logZ_forint.argmax()\n        uncert_plus, uncert_minus = logZ_uncert_forint[imax], logZ_uncert_forint[imax]\n\n\n    else:\n\n        if self.interp_scale == 'linear':\n            dlag = [*np.diff(lags_forint) / 2, 0]\n            dlag[1:] += np.diff(lags_forint) / 2\n            dlag[0] += lags_forint.min() - minlag\n            dlag[-1] += maxlag - lags_forint.max()\n\n            dlogZ = logZ_forint + np.log(dlag)\n            dZ = np.exp(dlogZ - dlogZ.max())\n            Z = dZ.sum() * np.exp(dlogZ.max())\n\n            # -------------------------------------\n            # Get Uncertainties\n\n            # todo Fix this to be generic and move outside of scope\n            # Estimate uncertainty from ~dt^2 error scaling\n            dlag_sub = [*np.diff(lags_forint[::2]) / 2, 0]\n            dlag_sub[1:] += np.diff(lags_forint[::2]) / 2\n            dlag_sub[0] += lags_forint.min() - minlag\n            dlag_sub[-1] += maxlag - lags_forint.max()\n\n            dlogZ_sub = logZ_forint[::2] + np.log(dlag_sub)\n            dZ_sub = np.exp(dlogZ_sub - dlogZ_sub.max())\n            Z_subsample = dZ_sub.sum() * np.exp(dlogZ_sub.max())\n            uncert_numeric = abs(Z - Z_subsample) / np.sqrt(17)\n\n            uncert_tol = np.square(dZ * logZ_uncert_forint).sum()\n            uncert_tol = np.sqrt(uncert_tol)\n            uncert_tol *= np.exp(dlogZ.max())\n\n\n        elif self.interp_scale == 'log':\n            # dZ = dXdY/dln|Y|\n            dlag = np.diff(lags_forint)\n            dY = np.diff(np.exp(logZ_forint - logZ_forint.max()))\n            dE = np.diff(logZ_forint)\n            dZ = dlag * dY / dE\n            Z = np.sum(dZ) * np.exp(logZ_forint.max())\n\n            uncert_tol = 4 * np.square(\n                np.exp(logZ_forint - logZ_forint.max())[:-1] - dZ / dE\n            ) * logZ_uncert_forint[:-1]\n            uncert_tol += np.square(logZ_uncert_forint[-1] * np.exp(logZ_forint - logZ_forint.max())[-1])\n            uncert_tol = np.sqrt(uncert_tol.sum())\n            uncert_tol *= np.exp(logZ_forint.max())\n\n            dlag_sub = np.diff(lags_forint[::2])\n            dY_sub = np.diff(np.exp(logZ_forint[::2] - logZ_forint.max()))\n            dE_sub = np.diff(logZ_forint[::2])\n            dZ_sub = dlag_sub * dY_sub / dE_sub\n            Z_subsample = np.sum(dZ_sub) * np.exp(logZ_forint.max())\n            uncert_numeric = abs(Z - Z_subsample) / np.sqrt(17)\n\n        self.msg_debug(\"Evidence Est: \\t %.2e\" % Z)\n        self.msg_debug(\n            \"Evidence uncerts: \\n Numeric: \\t %.2e \\n Convergence: \\t %.2e\" % (uncert_numeric, uncert_tol))\n\n        uncert_plus = uncert_numeric + uncert_tol.sum()\n        uncert_minus = uncert_numeric\n\n    if return_type == 'linear':\n        return np.array([Z, -uncert_minus, uncert_plus])\n    elif return_type == 'log':\n        return np.array([np.log(Z), np.log(1 - uncert_minus / Z), np.log(1 + uncert_plus / Z)])\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.hessian_scan.get_samples","title":"<code>get_samples(N: int = 1, seed: int = None, importance_sampling: bool = False) -&gt; {str: [float]}</code>","text":"Source code in <code>litmus/fitting_methods.py</code> <pre><code>def get_samples(self, N: int = 1, seed: int = None, importance_sampling: bool = False) -&gt; {str: [float]}:\n    # -------------------\n    fitting_procedure.get_samples(**locals())\n    seed = self._tempseed\n    # -------------------\n\n    assert self.interp_scale in self._allowable_interpscales, \"Interp scale %s not recognised. Must be selection from %s\" % (\n        self.interp_scale, self._allowable_interpscales)\n    lags_forint, logZ_forint, peaks, covars = self._get_slices('lags', 'logZ', 'peaks', 'covars')\n\n    if len(lags_forint) == 0:\n        self.msg_err(\"Zero good slices in evidence integral!\")\n        return (0, -np.inf, np.inf)\n\n    # Get weights and peaks etc\n    Npeaks = len(lags_forint)\n    minlag, maxlag = self.stat_model.prior_ranges['lag']\n\n    Y = np.exp(logZ_forint - logZ_forint.max()).squeeze()\n\n    dlag = [*np.diff(lags_forint) / 2, 0]\n    dlag[1:] += np.diff(lags_forint) / 2\n    dlag[0] += lags_forint.min() - minlag\n    dlag[-1] += maxlag - lags_forint.max()\n\n    if sum(dlag) == 0:\n        dlag = 1.0\n\n    weights = Y * dlag\n    weights /= weights.sum()\n\n    # Get hessians and peak locations\n    if Npeaks &gt; 1:\n        I = np.random.choice(range(Npeaks), N, replace=True, p=weights)\n    else:\n        I = np.zeros(N)\n\n    to_choose = [(I == i).sum() for i in range(Npeaks)]  # number of samples to draw from peak i\n\n    # Sweep over scan peaks and add scatter\n    outs = []\n    for i in range(Npeaks):\n        if to_choose[i] &gt; 0:\n            peak_uncon = self.stat_model.to_uncon(peaks[i])\n\n            # Get normal dist properties in uncon space in vector form\n            mu = _utils.dict_pack(peak_uncon, keys=self.params_toscan)\n            cov = covars[i]\n\n            # Generate samples\n            samps = np.random.multivariate_normal(mean=mu, cov=cov, size=to_choose[i])\n            samps = _utils.dict_unpack(samps.T, keys=self.params_toscan, recursive=False)\n            samps = _utils.dict_extend(peak_uncon, samps)\n\n            # Reconvert to constrained space\n            samps = self.stat_model.to_con(samps)\n\n            # -------------\n            # Add linear interpolation 'smudging' to lags\n            if Npeaks &gt; 1 and 'lag' in self.stat_model.free_params():\n\n                # Get nodes\n                tnow, ynow = lags_forint[i], Y[i]\n                if i == 0:\n                    yprev, ynext = ynow, Y[i + 1]\n                    tprev, tnext = min(self.stat_model.prior_ranges['lag']), lags_forint[i + 1]\n                elif i == Npeaks - 1:\n                    yprev, ynext = Y[i - 1], ynow\n                    tprev, tnext = lags_forint[i - 1], max(self.stat_model.prior_ranges['lag'])\n                else:\n                    yprev, ynext = Y[i - 1], Y[i + 1]\n                    tprev, tnext = lags_forint[i - 1], lags_forint[i + 1]\n                # --\n\n                # Perform CDF shift\n                Ti, Yi = [tprev, tnow, tnext], [yprev, ynow, ynext]\n                if self.interp_scale == 'linear':\n                    tshift = linscatter(Ti, Yi, N=to_choose[i])\n                elif self.interp_scale == 'log':\n                    tshift = expscatter(Ti, Yi, N=to_choose[i])\n                if np.isnan(tshift).any():\n                    self.msg_err(\"Something wrong with the lag shift at node %i in sample generation\" % i)\n                else:\n                    samps['lag'] += tshift\n            # -------------\n\n            if np.isnan(samps['lag']).any():\n                self.msg_err(\"Something wrong with the lags at node %i in sample generation\" % i)\n            else:\n                outs.append(samps)\n\n    outs = {key: np.concatenate([out[key] for out in outs]) for key in self.stat_model.paramnames()}\n    return (outs)\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.SVI_scan","title":"<code>SVI_scan(stat_model: stats_model, out_stream=sys.stdout, err_stream=sys.stderr, verbose=True, debug=False, **fit_params)</code>","text":"<p>               Bases: <code>hessian_scan</code></p> <p>An alternative to hessian_scan that fits each slice with stochastic variational inference instead of the laplace approximation. Typically slower, but more robust against numerical failure in low SNR signals and gives more accurate evidence estimates.</p> <p>Inherits all fitting parameters and their default values from hessian_scan, but gains new parameters</p> <p>Parameters:</p> Name Type Description Default <code>ELBO_threshold</code> <code>float</code> <p>If a slice log-evidence decreases by this amount or more, consider it a furrow. Defaults to 100.0.</p> required <code>ELBO_optimstep</code> <code>float</code> <p>Size of the stochastic optimisation step in adam. Defaults to 5E-3.</p> required <code>ELBO_particles</code> <code>int</code> <p>Number of particles for estimating the ELBO at each optimisation step. Defaults to 128.</p> required <code>ELBO_Nsteps</code> <code>int</code> <p>Number of steps to take in optimisation of the ELBO for each slice. Defaults to 128.</p> required <code>ELBO_Nsteps_init</code> <code>int</code> <p>Number of steps to take in finding the initial slice ELBO / solution. Defaults to 1_000.</p> required <code>ELBO_fraction</code> <code>int</code> <p>Fraction of the ELBO run (both slice and initial) to search for minimum variance estimate. Defaults to 0.25.</p> required Source code in <code>litmus/fitting_methods.py</code> <pre><code>def __init__(self, stat_model: stats_model,\n             out_stream=sys.stdout, err_stream=sys.stderr,\n             verbose=True, debug=False, **fit_params):\n\n    \"\"\"\n    Inherits all fitting parameters and their default values from hessian_scan, but gains new parameters\n\n    :param float ELBO_threshold: If a slice log-evidence decreases by this amount or more, consider it a furrow. Defaults to 100.0.\n    :param float ELBO_optimstep: Size of the stochastic optimisation step in adam. Defaults to 5E-3.\n    :param int ELBO_particles: Number of particles for estimating the ELBO at each optimisation step. Defaults to 128.\n    :param int ELBO_Nsteps: Number of steps to take in optimisation of the ELBO for each slice. Defaults to 128.\n    :param int ELBO_Nsteps_init: Number of steps to take in finding the initial slice ELBO / solution. Defaults to 1_000.\n    :param int ELBO_fraction: Fraction of the ELBO run (both slice and initial) to search for minimum variance estimate. Defaults to 0.25.\n    \"\"\"\n    args_in = {**locals(), **fit_params}\n    del args_in['self']\n    del args_in['__class__']\n    del args_in['fit_params']\n\n    if not hasattr(self, '_default_params'):\n        self._default_params = {}\n\n    self._default_params |= {\n        'ELBO_threshold': 100.0,\n        'ELBO_optimstep': 5E-3,\n        'ELBO_particles': 128,\n        'ELBO_Nsteps': 128,\n        'ELBO_Nsteps_init': 1_000,\n        'ELBO_fraction': 0.25,\n    }\n\n    super().__init__(**args_in)\n\n    # -----------------------------\n\n    self.name = \"SVI Scan Fitting Procedure\"\n\n    self.diagnostic_losses = []\n    self.diagnostic_loss_init = []\n    self.diagnostic_ints = []\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.SVI_scan.name","title":"<code>name = 'SVI Scan Fitting Procedure'</code>  <code>instance-attribute</code>","text":""},{"location":"fitting_methods/#litmus.fitting_methods.SVI_scan.diagnostic_losses","title":"<code>diagnostic_losses = []</code>  <code>instance-attribute</code>","text":""},{"location":"fitting_methods/#litmus.fitting_methods.SVI_scan.diagnostic_loss_init","title":"<code>diagnostic_loss_init = []</code>  <code>instance-attribute</code>","text":""},{"location":"fitting_methods/#litmus.fitting_methods.SVI_scan.diagnostic_ints","title":"<code>diagnostic_ints = []</code>  <code>instance-attribute</code>","text":""},{"location":"fitting_methods/#litmus.fitting_methods.SVI_scan.fit","title":"<code>fit(lc_1: lightcurve, lc_2: lightcurve, seed: int = None)</code>","text":"Source code in <code>litmus/fitting_methods.py</code> <pre><code>def fit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None):\n    # -------------------\n    fitting_procedure.fit(**locals())\n    seed = self._tempseed\n    # -------------------\n\n    self.msg_run(\"Starting SVI Scan\")\n\n    if not self.has_prefit:\n        self.prefit(lc_1, lc_2, seed)\n\n    data = self.stat_model.lc_to_data(lc_1, lc_2)\n\n    # ----------------------------------\n    # Estimate the MAP and its hessian for starting conditions\n\n    estmap_uncon = self.stat_model.to_uncon(self.estmap_params)\n\n    fix_param_dict_con = {key: self.estmap_params[key] for key in self.stat_model.fixed_params()}\n    fix_param_dict_uncon = {key: estmap_uncon[key] for key in self.stat_model.fixed_params()}\n\n    init_hess = -1 * self.stat_model.log_density_uncon_hess(estmap_uncon, data=data, keys=self.params_toscan)\n\n    # Convert these into SVI friendly objects and fit an SVI at the map\n    self.msg_run(\"Performing SVI slice at the MAP estimate\")\n    init_loc = _utils.dict_pack(estmap_uncon, keys=self.params_toscan)\n    init_tril = jnp.linalg.cholesky(jnp.linalg.inv(init_hess))\n\n    bad_starts = False\n    if np.isnan(init_loc).any() or np.isnan(init_tril).any():\n        self.msg_err(\"Issue with finding initial solver state for SVI. Proceeding /w numpyro defaults\")\n        bad_starts = True\n    # ----------------------------------\n    self.msg_debug(\"\\t Constructing slice model\")\n\n    def slice_function(data, lag):\n        \"\"\"\n        This is the conditional model that SVI will map\n        \"\"\"\n\n        params = {}\n        for key in self.stat_model.free_params():\n            if key != 'lag':\n                val = quickprior(self.stat_model, key)\n                params |= {key: val}\n        params |= {'lag': lag}\n        params |= fix_param_dict_con\n\n        with numpyro.handlers.block(hide=self.stat_model.paramnames()):\n            LL = self.stat_model._log_likelihood(params, data)\n\n        dilute = -np.log(self.stat_model.prior_ranges['lag'][1] - self.stat_model.prior_ranges['lag'][\n            0]) if 'lag' in self.stat_model.free_params() else 0.0\n        numpyro.factor('lag_prior', dilute)\n\n    # SVI settup\n    self.msg_debug(\"\\t Constructing and running optimizer and SVI guides\")\n    optimizer = numpyro.optim.Adam(step_size=self.ELBO_optimstep)\n    autoguide = numpyro.infer.autoguide.AutoMultivariateNormal(slice_function)\n    autosvi = numpyro.infer.SVI(slice_function, autoguide, optim=optimizer,\n                                loss=numpyro.infer.Trace_ELBO(self.ELBO_particles),\n                                )\n\n    self.msg_debug(\"\\t Running SVI\")\n    MAP_SVI_results = autosvi.run(jax.random.PRNGKey(seed), self.ELBO_Nsteps_init,\n                                  data=data, lag=self.estmap_params['lag'],\n                                  init_params={'auto_loc': init_loc,\n                                               'auto_scale_tril': init_tril\n                                               } if not bad_starts else None,\n                                  progress_bar=self.verbose\n                                  )\n\n    self.msg_debug(\"\\t Success. Extracting solution\")\n    BEST_loc, BEST_tril = MAP_SVI_results.params['auto_loc'], MAP_SVI_results.params['auto_scale_tril']\n\n    self.diagnostic_loss_init = MAP_SVI_results.losses\n\n    # ----------------------------------\n    # Main Scan\n\n    lags_forscan = self.lags\n    l_old = -np.inf\n\n    scanned_optima = []\n    ELBOS_tosave = []\n    ElBOS_uncert = []\n    diagnostic_hessians = []\n    diagnostic_losses = []\n\n    for i, lag in enumerate(lags_forscan):\n\n        self.msg_run(\":\" * 23, \"Doing SVI fit at lag=%.2f ...\" % lag)\n\n        svi_loop_result = autosvi.run(jax.random.PRNGKey(seed),\n                                      self.ELBO_Nsteps,\n                                      data=data, lag=lag,\n                                      init_params={'auto_loc': BEST_loc,\n                                                   'auto_scale_tril': BEST_tril\n                                                   },\n                                      progress_bar=self.verbose\n                                      )\n\n        NEW_loc, NEW_tril = svi_loop_result.params['auto_loc'], svi_loop_result.params['auto_scale_tril']\n\n        # --------------\n        # Check if the optimization has suceeded or broken\n\n        l_old = l_old\n        l_new = self._getELBO(svi_loop_result.losses)[0]\n        diverged = bool(np.isinf(NEW_loc).any() + np.isinf(NEW_tril).any())\n        big_drop = l_new - l_old &lt; - self.ELBO_threshold\n\n        self.msg_run(\n            \"From %.2f to %.2f, change of %.2f against %.2f\" % (l_old, l_new, l_new - l_old, self.ELBO_threshold))\n\n        if not big_drop and not diverged:\n            self.msg_run(\"Seems to have converged at iteration %i / %i\" % (i, self.Nlags))\n\n            self.converged[i] = True\n            l_old = l_new\n            BEST_loc, BEST_tril = NEW_loc, NEW_tril\n\n            uncon_params = self.stat_model.to_uncon(self.estmap_params | {'lag': lag}) | _utils.dict_unpack(NEW_loc,\n                                                                                                            self.params_toscan)\n            con_params = self.stat_model.to_con(uncon_params)\n            scanned_optima.append(con_params)\n\n            H = np.dot(NEW_tril, NEW_tril.T)\n            H = (H + H.T) / 2\n            H = jnp.linalg.inv(-H)\n            diagnostic_hessians.append(H)\n\n            diagnostic_losses.append(svi_loop_result.losses)\n\n            ELBO, uncert = self._getELBO(svi_loop_result.losses)\n            ELBOS_tosave.append(ELBO)\n            ElBOS_uncert.append(uncert)\n\n\n        else:\n            self.msg_run(\"Unable to converge at iteration %i / %i\" % (i, self.Nlags))\n            self.msg_debug(\"Reason for failure: \\n large ELBO drop: \\t %r \\n diverged: \\t %r\" % (\n                big_drop, diverged))\n\n    self.msg_run(\"Scanning Complete. Calculating ELBO integrals...\")\n    if sum(self.converged) == 0:\n        self.msg_err(\"All slices catastrophically diverged! Try different starting conditions and/or grid spacing\")\n\n    self.diagnostic_ints = np.array(ELBOS_tosave)\n\n    self.log_evidences_uncert = np.array(ElBOS_uncert)\n    self.diagnostic_losses = np.array(diagnostic_losses)\n    self.diagnostic_hessians = np.array(diagnostic_hessians)\n\n    self.scan_peaks = _utils.dict_combine(scanned_optima)\n    self.diagnostic_densities = self.stat_model.log_density(self.scan_peaks, data)\n\n    # ---------------------------------------------------------------------------------\n    # For each of these peaks, estimate the evidence\n    # todo - vmap and parallelize\n\n    Zs, tgrads = [], []\n    for j, params in enumerate(scanned_optima):\n        Z = self.diagnostic_ints[j]\n        tgrad = self.stat_model.uncon_grad_lag(params) if not self.constrained_domain else 0\n        tgrad = 0\n        tgrads.append(tgrad)\n        Zs.append(Z + tgrad)\n\n    self.log_evidences = np.array(Zs)\n    self.diagnostic_tgrads = np.array(tgrads)\n    self.has_run = True\n\n    self.msg_run(\"SVI Fitting complete.\", \"-\" * 23, \"-\" * 23, delim='\\n')\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.SVI_scan.refit","title":"<code>refit(lc_1: lightcurve, lc_2: lightcurve, seed: int = None)</code>","text":"Source code in <code>litmus/fitting_methods.py</code> <pre><code>def refit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None):\n    # TODO - fill this out\n\n    return\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.SVI_scan.diagnostics","title":"<code>diagnostics(plot=True) -&gt; _types.Figure</code>","text":"Source code in <code>litmus/fitting_methods.py</code> <pre><code>def diagnostics(self, plot=True) -&gt; _types.Figure:\n\n    f, (a2, a1) = plt.subplots(2, 1)\n\n    for i, x in enumerate(self.diagnostic_losses):\n        a1.plot(x - (self.diagnostic_ints[i]), c='k', alpha=0.25)\n    a2.plot(self.diagnostic_loss_init, c='k')\n\n    a1.axvline(int((1 - self.ELBO_fraction) * self.ELBO_Nsteps), c='k', ls='--')\n\n    a1.set_yscale('symlog')\n    a2.set_yscale('symlog')\n    a1.grid(), a2.grid()\n\n    a1.set_xlim(0, self.ELBO_Nsteps)\n    a2.set_xlim(0, self.ELBO_Nsteps_init)\n\n    a1.set_title(\"Scan SVIs\")\n    a2.set_title(\"Initial MAP SVI\")\n\n    f.supylabel(\"Loss - loss_final (log scale)\")\n    a1.set_xlabel(\"iteration Number\"), a2.set_xlabel(\"iteration Number\")\n\n    txt = \"Trace plots of ELBO convergence. All lines should be flat by the right hand side.\\n\" \\\n          \"Top panel is for initial guess and need only be flat. Bottom panel should be flat within\" \\\n          \"averaging range, i.e. to the right of dotted line.\"\n    f.supxlabel('$\\begin{center}X-axis\\\\*\\textit{\\small{%s}}\\end{center}$' % txt)\n\n    f.tight_layout()\n\n    if plot: plt.show()\n\n    return f\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.JAVELIKE","title":"<code>JAVELIKE(stat_model: stats_model, out_stream=sys.stdout, err_stream=sys.stderr, verbose=True, debug=False, **fit_params)</code>","text":"<p>               Bases: <code>fitting_procedure</code></p> <p>A direct MCMC implementation using the AEIS in the style of JAVELIN Note that, because NumPyro fits in the unconstrained domain while JAVELIN fits in the constrained domain, the behaviour of the two will be slightly different near the prior boundaries.</p> <p>Note that this is for example / comparison only, and should not be used for actual fitting as it cannot handle the multimodal distributions of seasonal lightcurves</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>Size of the stretch in the stretch-move. Defaults to 2.0.</p> required <code>num_chains</code> <code>float</code> <p>Num live points in the AEIS ensemble. Defaults to 256.</p> required <code>num_samples</code> <code>float</code> <p>Samples per live point in the AEIS chain. Defaults to 200_000 // 256 per chain total (i.e. 200_000 total)</p> required <code>num_warmup</code> <code>float</code> <p>Number of warmup samples per chain. Defaults to 5_000.</p> required Source code in <code>litmus/fitting_methods.py</code> <pre><code>def __init__(self, stat_model: stats_model,\n             out_stream=sys.stdout, err_stream=sys.stderr,\n             verbose=True, debug=False, **fit_params):\n\n    \"\"\"\n    :param float alpha: Size of the stretch in the stretch-move. Defaults to 2.0.\n    :param float num_chains: Num live points in the AEIS ensemble. Defaults to 256.\n    :param float num_samples: Samples per live point in the AEIS chain. Defaults to 200_000 // 256 per chain total (i.e. 200_000 total)\n    :param float num_warmup: Number of warmup samples per chain. Defaults to 5_000.\n    \"\"\"\n    args_in = {**locals(), **fit_params}\n    del args_in['self']\n    del args_in['__class__']\n    del args_in['fit_params']\n\n    if not hasattr(self, '_default_params'):\n        self._default_params = {}\n    self._default_params |= {\n        'alpha': 2.0,\n        'num_chains': 256,\n        'num_samples': 200_000 // 256,\n        'num_warmup': 5_000,\n    }\n\n    self.sampler: numpyro.infer.MCMC = None\n    \"\"\"NumPyro MCMC wrapper\"\"\"\n    self.kernel: numpyro.infer.AEIS = None\n    \"\"\"numpyro MCMC sampler kernel to use\"\"\"\n    self.limited_model: Callable = None\n    \"\"\"The function to deploy the AEIS against\"\"\"\n\n    super().__init__(**args_in)\n\n    # -----------------------------\n\n    self.name = \"AEIS JAVELIN Emulator fitting Procedure\"\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.JAVELIKE.sampler","title":"<code>sampler: numpyro.infer.MCMC = None</code>  <code>instance-attribute</code>","text":"<p>NumPyro MCMC wrapper</p>"},{"location":"fitting_methods/#litmus.fitting_methods.JAVELIKE.kernel","title":"<code>kernel: numpyro.infer.AEIS = None</code>  <code>instance-attribute</code>","text":"<p>numpyro MCMC sampler kernel to use</p>"},{"location":"fitting_methods/#litmus.fitting_methods.JAVELIKE.limited_model","title":"<code>limited_model: Callable = None</code>  <code>instance-attribute</code>","text":"<p>The function to deploy the AEIS against</p>"},{"location":"fitting_methods/#litmus.fitting_methods.JAVELIKE.name","title":"<code>name = 'AEIS JAVELIN Emulator fitting Procedure'</code>  <code>instance-attribute</code>","text":""},{"location":"fitting_methods/#litmus.fitting_methods.JAVELIKE.readyup","title":"<code>readyup()</code>","text":"Source code in <code>litmus/fitting_methods.py</code> <pre><code>def readyup(self):\n\n    fixed_vals = {key: self.stat_model.prior_ranges[key][0] for key in self.stat_model.fixed_params()}\n\n    def limited_model(data):\n        with numpyro.handlers.block(hide=self.stat_model.fixed_params()):\n            params = {key: val for key, val in zip(self.stat_model.paramnames(), self.stat_model.prior())}\n\n        params |= fixed_vals\n        with numpyro.handlers.block(hide=self.stat_model.paramnames()):\n            LL = self.stat_model._log_density(params, data)\n\n        numpyro.factor('ll', LL)\n\n    self.limited_model = limited_model\n\n    self.kernel = numpyro.infer.AIES(self.limited_model,\n                                     moves={numpyro.infer.AIES.StretchMove(a=self.alpha): 1.0}\n                                     )\n\n    self.sampler = numpyro.infer.MCMC(self.kernel,\n                                      num_warmup=self.num_warmup,\n                                      num_samples=self.num_samples,\n                                      num_chains=self.num_chains,\n                                      chain_method='vectorized',\n                                      progress_bar=self.verbose)\n\n    self.is_ready = True\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.JAVELIKE.prefit","title":"<code>prefit(lc_1: lightcurve, lc_2: lightcurve, seed: int = None)</code>","text":"Source code in <code>litmus/fitting_methods.py</code> <pre><code>def prefit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None):\n    if seed is None: seed = self.seed\n    if not self.is_ready: self.readyup()\n\n    self.msg_run(\"Running warmup with %i chains and %i samples\" % (self.num_chains, self.num_warmup))\n    # self.sampler.warmup(jax.random.PRNGKey(seed), self.stat_model.lc_to_data(lc_1, lc_2))\n\n    self.has_prefit = True\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.JAVELIKE.fit","title":"<code>fit(lc_1: lightcurve, lc_2: lightcurve, seed: int = None)</code>","text":"Source code in <code>litmus/fitting_methods.py</code> <pre><code>def fit(self, lc_1: lightcurve, lc_2: lightcurve, seed: int = None):\n    if seed is None: seed = self.seed\n    if not self.is_ready: self.readyup()\n    if not self.has_prefit: self.prefit(lc_1, lc_2, seed=seed)\n\n    self.msg_run(\"Running sampler with %i chains and %i samples\" % (self.num_chains, self.num_samples))\n\n    self.sampler.run(jax.random.PRNGKey(seed), self.stat_model.lc_to_data(lc_1, lc_2))\n    self.has_run = True\n</code></pre>"},{"location":"fitting_methods/#litmus.fitting_methods.JAVELIKE.get_samples","title":"<code>get_samples(N: int = None, seed: int = None, importance_sampling: bool = False) -&gt; {str: [float]}</code>","text":"Source code in <code>litmus/fitting_methods.py</code> <pre><code>def get_samples(self, N: int = None, seed: int = None, importance_sampling: bool = False) -&gt; {str: [float]}:\n    if seed is None: seed = self.seed\n    if not self.has_run:\n        self.msg_err(\"Can't get samples before running!\")\n    if importance_sampling:\n        self.msg_err(\"JAVELIKE Already distributed according to posterior (ideally)\")\n    samps = self.sampler.get_samples()\n\n    if not (N is None):\n        M = _utils.dict_dim(samps)[1]\n        if M &gt; N: self.msg_err(\"Tried to get %i sub-samples from chain of %i total samples.\" % (M, N))\n\n        I = np.random.choice(np.arange(M), N, replace=True)\n        samps = {key: samps[key][I] for key in samps.keys()}\n    return (samps)\n</code></pre>"},{"location":"gettingstarted/","title":"Getting Started","text":""},{"location":"gettingstarted/#installation","title":"Installation","text":""},{"location":"gettingstarted/#simple-installation","title":"Simple Installation","text":"<p>First make sure you have a recent version of python running (<code>3.10</code>-<code>3.12</code>) and then install directly from the git repo:</p> <pre><code>pip install \"git+https://github.com/HughMcDougall/litmus\"\n</code></pre>"},{"location":"gettingstarted/#explicit-installation","title":"Explicit Installation","text":"<p>If you find the above doesn't work, try first installing the dependencies one by one, starting with the commonplace python packages:</p> <pre><code>pip install numpy matplotlib scikit-learn\n</code></pre> <p>Then the <code>JAX</code> ecosystem and <code>numpyro</code> utilities:</p> <pre><code>pip install jax jaxlib jaxopt\npip install numpyro tinygp\n</code></pre> <p>For plotting utilties we need chainconsumer, which needs a newer version of scipy, which in turn requires a newish version of python.</p> <p>Requires Using python <code>3.11</code>-<code>3.12</code>:</p> <pre><code>pip install scipy\npip install chainconsumer\n</code></pre> <p>Nested Sampling If you want to make use of <code>jaxns</code> nested sampling, you'll need to install it with:</p> <pre><code>pip install etils tensorflow_probability\npip install jaxns\n</code></pre> <p>Note: You have bump into some trouble installing <code>tensorflow_probability</code> if you don't have <code>cmake</code> installed.</p>"},{"location":"gettingstarted/#usage","title":"Usage","text":""},{"location":"gettingstarted/#first-timers","title":"First Timers","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nimport litmus\n</code></pre> <pre><code>mymock = litmus.mocks.mock(3)\nlc_1, lc_2 = mymock.lc_1, mymock.lc_2\nmymock.plot()\n</code></pre> <p>Now, choose a model and set its priors(at time of writing only <code>GP_simple</code>, which models both lightcurves as scaled and shifted damped random walks, is implemented). For example suppose we know want to narrow our lag search range to <code>[0,100] days</code>, and know that the lightcurves are normalized to have zero mean:</p> <pre><code>my_model = litmus.models.GP_simple()\nmy_model.set_priors(\n    {\n    'lag': [0,100]\n    'mean': [0,0]\n    'rel_mean': [0,0]\n    }\n)\n</code></pre> <p>Now we choose a fitting method and tune it accordingly.</p> <pre><code>fitting_method = litmus.fitting_methods.hessian_scan(stat_model=my_model,\n                                                  Nlags=32,\n                                                  init_samples=5_000,\n                                                  grid_bunching=0.8,\n                                                  optimizer_args={'tol': 1E-3,\n                                                                  'maxiter': 256,\n                                                                  'increase_factor': 1.1,\n                                                                  },\n                                                  optimizer_args_init={'tol': 1E-4,\n                                                                  'maxiter': 1024,\n                                                                  'increase_factor': 1.01,\n                                                                  },\n                                                  reverse=False,\n                                                  debug=False\n                                                  )\n</code></pre> <p>Finally, wrap this in a <code>LITMUS</code> object, adding the lightcurves to it, which makes running and getting results out as simple as a single line of code:</p> <pre><code>litmus_handler = litmus.LITMUS(fitting_method)\n\nlitmus_handler.add_lightcurve(lc_1)\nlitmus_handler.add_lightcurve(lc_2)\n</code></pre> <p>Now, fire off the fitting procedure:</p> <pre><code>litmus_handler.fit()\n</code></pre> <p>And finally plot the model parameters to see how we did:</p> <pre><code>litmus_handler.lag_plot()\nlitmus_handler.plot_parameters()\nlitmus_handler.plot_diagnostics()\n</code></pre>"},{"location":"lightcurve/","title":"<code>litmus.lightcurve</code>","text":"<p>               Bases: <code>object</code></p> <p>A wrapper class for lightcurves. Construct /w array-like inputs for time, signal and error (optional) like:    lightcurve(T, Y, E) or:     lightcurve(T, Y) Which yields E=0 for all {T,Y}</p> <p>Supports array-like addition and float-like addition / multiplication</p> Source code in <code>litmus/lightcurve.py</code> <pre><code>def __init__(self, T, Y, E=None):\n    self.T = np.array(T, dtype=np.float64)\n    self.Y = np.array(Y, dtype=np.float64)\n    if E is None:\n        self.E = np.zeros_like(T)\n    else:\n        self.E = E\n\n    self._data = np.vstack(self.values()).T\n\n    self._norm_mean, self._norm_amp = 0.0, 1.0\n    self.normalized = False\n</code></pre>"},{"location":"lightcurve/#litmus.lightcurve.T","title":"<code>T = np.array(T, dtype=np.float64)</code>  <code>instance-attribute</code>","text":""},{"location":"lightcurve/#litmus.lightcurve.Y","title":"<code>Y = np.array(Y, dtype=np.float64)</code>  <code>instance-attribute</code>","text":""},{"location":"lightcurve/#litmus.lightcurve.E","title":"<code>E = np.zeros_like(T)</code>  <code>instance-attribute</code>","text":""},{"location":"lightcurve/#litmus.lightcurve.normalized","title":"<code>normalized = False</code>  <code>instance-attribute</code>","text":""},{"location":"lightcurve/#litmus.lightcurve.keys","title":"<code>keys() -&gt; [str, str, str]</code>","text":"<p>Returns the string-like names of the lightcurve's attributes</p> Source code in <code>litmus/lightcurve.py</code> <pre><code>def keys(self) -&gt; [str, str, str]:\n    \"\"\"\n    Returns the string-like names of the lightcurve's attributes\n    \"\"\"\n    return [\"T\", \"Y\", \"E\"]\n</code></pre>"},{"location":"lightcurve/#litmus.lightcurve.values","title":"<code>values() -&gt; (_types.ArrayN, _types.ArrayN, _types.ArrayN)</code>","text":"<p>Returns the lightcurves' value series' in the order of keys</p> Source code in <code>litmus/lightcurve.py</code> <pre><code>def values(self) -&gt; (_types.ArrayN, _types.ArrayN, _types.ArrayN):\n    \"\"\"\n    Returns the lightcurves' value series' in the order of keys\n    \"\"\"\n    return [self[key] for key in self.keys()]\n</code></pre>"},{"location":"lightcurve/#litmus.lightcurve.normalize","title":"<code>normalize() -&gt; _types.Self</code>","text":"<p>Esimates the mean and amplitude of the lighturve assuming uncorrelated measurements Returns a lightcurve object with this normalization</p> Source code in <code>litmus/lightcurve.py</code> <pre><code>def normalize(self) -&gt; _types.Self:\n    \"\"\"\n    Esimates the mean and amplitude of the lighturve assuming uncorrelated measurements\n    Returns a lightcurve object with this normalization\n    \"\"\"\n\n    if self.normalized: return self\n\n    # Check if have errorbars\n    no_errs = False\n    E = self.E\n    if max(E) == 0:\n        no_errs = True\n        E = np.ones_like(self.E)\n\n    # Get initial guess assuming no scatter\n    w = E ** -2\n    mean0 = np.average(self.Y, weights=w)\n    var0 = np.average((self.Y - mean0) ** 2, weights=w)\n\n    if no_errs:\n        meanbest, varbest = mean0, var0\n    else:\n        L = lambda X: ((self.Y - X[0]) ** 2 / (self.E ** 2 + X[1]) + np.log(self.E ** 2 + X[1])).sum()\n        meanbest, varbest = optimize.minimize(L, np.array([mean0, var0]), method='Nelder-Mead').x\n\n    # Make and return copy\n    out = copy(self)\n    out -= meanbest\n    out /= np.sqrt(varbest)\n\n    # If any errors, revert to simple estiamte\n    if np.any(np.isnan(out._data)):\n        meanbest, varbest = mean0, var0\n        out = copy(self)\n        out -= meanbest\n        out /= np.sqrt(varbest)\n\n    # Store normalizing values for later\n    out._norm_mean = meanbest\n    out._norm_amp = np.sqrt(varbest)\n\n    out.normalized = True\n\n    return out\n</code></pre>"},{"location":"lightcurve/#litmus.lightcurve.unnormalize","title":"<code>unnormalize() -&gt; _types.Self</code>","text":"<p>Reverses the effects of lightcurve.normalize(). Returns a lightcurve object with mean and amplitude prior to normalize()</p> Source code in <code>litmus/lightcurve.py</code> <pre><code>def unnormalize(self) -&gt; _types.Self:\n    \"\"\"\n    Reverses the effects of lightcurve.normalize().\n    Returns a lightcurve object with mean and amplitude prior to normalize()\n    \"\"\"\n    out = copy(self)\n    out *= self._norm_amp\n    out += self._norm_mean\n    out._norm_amp = 1.0\n    out._norm_mean = 0.0\n    out.normalized = False\n    return out\n</code></pre>"},{"location":"lightcurve/#litmus.lightcurve.delayed_copy","title":"<code>delayed_copy(lag=0.0, Tmin=None, Tmax=None) -&gt; _types.Self</code>","text":"<p>Returns a copy subsampled to only datapoints in the domain T in [Tmin,Tmax] and offset by lag</p> Source code in <code>litmus/lightcurve.py</code> <pre><code>def delayed_copy(self, lag=0.0, Tmin=None, Tmax=None) -&gt; _types.Self:\n    \"\"\"\n    Returns a copy subsampled to only datapoints in the domain T in [Tmin,Tmax] and offset by lag\n    \"\"\"\n    if Tmin is None: Tmin = self.T.min()\n    if Tmax is None: Tmax = self.T.max()\n    I = np.where((self.T + lag &gt; Tmin) * (self.T + lag &lt; Tmax))[0]\n\n    return (lightcurve(T=self.T[I] + lag,\n                       Y=self.Y[I],\n                       E=self.E[I]\n                       ))\n</code></pre>"},{"location":"lightcurve/#litmus.lightcurve.trimmed_copy","title":"<code>trimmed_copy(Tmin=None, Tmax=None) -&gt; _types.Self</code>","text":"<p>Returns a copy subsampled to only datapoints in the domain T in [Tmin,Tmax]</p> Source code in <code>litmus/lightcurve.py</code> <pre><code>def trimmed_copy(self, Tmin=None, Tmax=None) -&gt; _types.Self:\n    \"\"\"\n    Returns a copy subsampled to only datapoints in the domain T in [Tmin,Tmax]\n    \"\"\"\n    return self.delayed_copy(0, Tmin, Tmax)\n</code></pre>"},{"location":"lightcurve/#litmus.lightcurve.concatenate","title":"<code>concatenate(other)</code>","text":"Source code in <code>litmus/lightcurve.py</code> <pre><code>def concatenate(self, other):\n    T1, T2 = self.T, other.T\n    Y1, Y2 = self.Y, other.Y\n    E1, E2 = self.E, other.E\n    T, Y, E = np.concatenate([T1, T2]), np.concatenate([Y1, Y2]), np.concatenate([E1, E2])\n    return lightcurve(T, Y, E)\n</code></pre>"},{"location":"lightcurve/#litmus.lightcurve.plot","title":"<code>plot(axis=None, show=True, **kwargs) -&gt; None</code>","text":"<p>Plots an errorbar series to a matplotlib axis. If show=True, will plt.show() after plotting. If axis is None, will create a new figure. Pass in any plotting kwargs for plt.errorbar at **kwargs</p> Source code in <code>litmus/lightcurve.py</code> <pre><code>def plot(self, axis=None, show=True, **kwargs) -&gt; None:\n    \"\"\"\n    Plots an errorbar series to a matplotlib axis.\n    If show=True, will plt.show() after plotting.\n    If axis is None, will create a new figure.\n    Pass in any plotting kwargs for plt.errorbar at **kwargs\n    \"\"\"\n    if axis is None:\n        plt.figure()\n        axis = plt.gca()\n        axis.set_xlabel(\"T\")\n        axis.set_ylabel(\"Y\")\n    axis.errorbar(self.T, self.Y, self.E, fmt='none', **kwargs)\n\n    if show: plt.show()\n</code></pre>"},{"location":"lightcurve/#litmus.lightcurve.copy","title":"<code>copy()</code>","text":"Source code in <code>litmus/lightcurve.py</code> <pre><code>def copy(self):\n    return copy(self)\n</code></pre>"},{"location":"lin_scatter/","title":"<code>litmus.lin_scatter</code>","text":""},{"location":"lin_scatter/#litmus.lin_scatter.linscatter","title":"<code>linscatter(X: np.array, Y: np.array, N: int) -&gt; np.ndarray[Any, np.float64]</code>","text":"<p>For some time series {X,Y}, returns N samples that are distributed along X with linear interpolation</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>X values</p> required <code>Y</code> <code>array</code> <p>Y values</p> required <code>N</code> <code>int</code> <p>Number of samples</p> required <p>Returns:</p> Type Description <code>ndarray[Any, float64]</code> <p>1D numpy array of samples along X</p> Source code in <code>litmus/lin_scatter.py</code> <pre><code>def linscatter(X: np.array, Y: np.array, N: int) -&gt; np.ndarray[Any, np.float64]:\n    \"\"\"\n    For some time series {X,Y}, returns N samples that are distributed along X with linear interpolation\n    :param X: X values\n    :param Y: Y values\n    :param N: Number of samples\n    :return: 1D numpy array of samples along X\n    \"\"\"\n\n    dx = np.array([X[0] - X[1], X[2] - X[1]])\n    dy = np.array([Y[0] - Y[1], Y[2] - Y[1]])\n    dx, dy = dx[dx.argsort()], dy[dx.argsort()]\n\n    weight_leftright = abs((Y[1] + dy / 2.0) * dx)\n    weight_leftright /= weight_leftright.sum()\n\n    leftright = np.random.choice([0, 1], replace=True, size=N, p=weight_leftright)\n\n    DX, DY = dx[leftright], dy[leftright]\n    YBAR = Y[1] + DY / 2\n    c1, c2 = YBAR / DY, Y[1] / DY\n\n    CDF = np.random.rand(N)\n\n    # todo - this throws an error because is evaluates the first branch in full. Find a way to suppress.\n    Xshift = np.where(DY != 0,\n                      np.sign(DY) * np.sqrt(CDF * c1 * 2 + (c2) ** 2) - c2,\n                      CDF\n                      )\n    Xshift = Xshift * DX\n\n    return Xshift\n</code></pre>"},{"location":"lin_scatter/#litmus.lin_scatter.expscatter","title":"<code>expscatter(X: np.array, Y: np.array, N) -&gt; np.ndarray[Any, np.float64]</code>","text":"<p>For some time series {X,Y}, returns N samples that are distributed along X with log-linear interpolation</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>X values</p> required <code>Y</code> <code>array</code> <p>Y values</p> required <code>N</code> <p>Number of samples</p> required <p>Returns:</p> Type Description <code>ndarray[Any, float64]</code> <p>1D numpy array of samples along X</p> Source code in <code>litmus/lin_scatter.py</code> <pre><code>def expscatter(X: np.array, Y: np.array, N) -&gt; np.ndarray[Any, np.float64]:\n    \"\"\"\n    For some time series {X,Y}, returns N samples that are distributed along X with log-linear interpolation\n    :param X: X values\n    :param Y: Y values\n    :param N: Number of samples\n    :return: 1D numpy array of samples along X\n    \"\"\"\n\n    dx = np.array([X[0] - X[1], X[2] - X[1]])\n    dy = np.array([Y[0] - Y[1], Y[2] - Y[1]])\n    dE = np.log(np.array([Y[0] / Y[1], Y[2] / Y[1]]))\n    dx, dy, dE = dx[dx.argsort()], dy[dx.argsort()], dE[dx.argsort()]\n\n    weight_leftright = abs(dx * dy / dE)\n\n    if dy[0] == 0: weight_leftright[0] = abs(dx[0] * Y[1])\n    if dy[1] == 0: weight_leftright[1] = abs(dx[1] * Y[1])\n\n\n    weight_leftright /= weight_leftright.sum()\n\n\n    leftright = np.random.choice([0, 1], replace=True, size=N, p=weight_leftright)\n\n    DX, DY, DE = dx[leftright], dy[leftright], dE[leftright]\n\n    CDF = np.random.rand(N)\n\n    # todo - this throws an error because is evaluates the first branch in full. Find a way to suppress.\n    Xshift = np.where(DY != 0,\n                      np.log(CDF * DY / Y[1] + 1) / DE,\n                      CDF\n                      )\n\n    Xshift = Xshift * DX\n\n    return Xshift\n</code></pre>"},{"location":"litmusclass/","title":"<code>litmus.litmusclass</code>","text":"<p>litmus.py</p> <p>Contains the main litmus object class, which acts as a user-friendly interface with the models statistical models and fitting procedure. In future versions, this will also give access to the GUI.</p> <p>todo     - This entire class to be re-done to take multiple models instead of multiple lightcurves     - Possibly add hdf5 saving to chain output     - Maybe add save_litmus() /w pickling?     - Need to have better handling of the \"fitting method inherit\" feature, especially with refactor / redo     -</p>"},{"location":"litmusclass/#litmus.litmusclass.LITMUS","title":"<code>LITMUS(fitproc: fitting_procedure = None)</code>","text":"<p>               Bases: <code>logger</code></p> <p>A front-facing UI class for interfacing with the fitting procedures.</p> Source code in <code>litmus/litmusclass.py</code> <pre><code>def __init__(self, fitproc: fitting_procedure = None):\n\n    logger.__init__(self)\n    # ----------------------------\n\n    if fitproc is None:\n        self.msg_err(\"Didn't set a fitting method, using GP_simple\")\n        self.model = models.GP_simple()\n\n        self.msg_err(\"Didn't set a fitting method, using hessian scan\")\n\n        fitproc = fitting_methods.hessian_scan(stat_model=self.model)\n\n    self.model = fitproc.stat_model\n    self.fitproc = fitproc\n\n    # ----------------------------\n    self.lightcurves = []\n    self.data = None\n\n    self.Nsamples = 50_000\n    self.samples = {}\n    self.prior_samples = self.model.prior_sample(self.Nsamples)\n    self.C = ChainConsumer()\n\n    self.C.set_override(ChainConfig(smooth=0, linewidth=2, plot_cloud=True, shade_alpha=0.5))\n\n    # self.C.add_chain(Chain(samples=DataFrame.from_dict(self.prior_samples), name=\"Prior\", color='gray'))\n    if self.fitproc.has_run:\n        self.samples = self.fitproc.get_samples(self.Nsamples)\n        self.samples = self.fitproc.get_samples(self.Nsamples)\n        self.C.add_chain(Chain(samples=DataFrame.from_dict(self.samples), name=\"Lightcurves %i-%i\"))\n        self.msg_err(\"Warning! LITMUS object built on pre-run fitting_procedure. May have unexpected behaviour.\")\n\n    return\n</code></pre>"},{"location":"litmusclass/#litmus.litmusclass.LITMUS.model","title":"<code>model = fitproc.stat_model</code>  <code>instance-attribute</code>","text":""},{"location":"litmusclass/#litmus.litmusclass.LITMUS.fitproc","title":"<code>fitproc = fitproc</code>  <code>instance-attribute</code>","text":""},{"location":"litmusclass/#litmus.litmusclass.LITMUS.lightcurves","title":"<code>lightcurves = []</code>  <code>instance-attribute</code>","text":""},{"location":"litmusclass/#litmus.litmusclass.LITMUS.data","title":"<code>data = None</code>  <code>instance-attribute</code>","text":""},{"location":"litmusclass/#litmus.litmusclass.LITMUS.Nsamples","title":"<code>Nsamples = 50000</code>  <code>instance-attribute</code>","text":""},{"location":"litmusclass/#litmus.litmusclass.LITMUS.samples","title":"<code>samples = {}</code>  <code>instance-attribute</code>","text":""},{"location":"litmusclass/#litmus.litmusclass.LITMUS.prior_samples","title":"<code>prior_samples = self.model.prior_sample(self.Nsamples)</code>  <code>instance-attribute</code>","text":""},{"location":"litmusclass/#litmus.litmusclass.LITMUS.C","title":"<code>C = ChainConsumer()</code>  <code>instance-attribute</code>","text":""},{"location":"litmusclass/#litmus.litmusclass.LITMUS.add_lightcurve","title":"<code>add_lightcurve(lc: lightcurve)</code>","text":"<p>Add a lightcurve 'lc' to the LITMUS object</p> Source code in <code>litmus/litmusclass.py</code> <pre><code>def add_lightcurve(self, lc: lightcurve):\n    \"\"\"\n    Add a lightcurve 'lc' to the LITMUS object\n    \"\"\"\n    self.lightcurves.append(lc)\n    return\n</code></pre>"},{"location":"litmusclass/#litmus.litmusclass.LITMUS.remove_lightcurve","title":"<code>remove_lightcurve(i: int) -&gt; None</code>","text":"<p>Remove lightcurve of index 'i' from the LITMUS object</p> Source code in <code>litmus/litmusclass.py</code> <pre><code>def remove_lightcurve(self, i: int) -&gt; None:\n    \"\"\"\n    Remove lightcurve of index 'i' from the LITMUS object\n    \"\"\"\n    N = len(self.lightcurves)\n\n    if i &lt; N:\n        del self.lightcurves[i]\n    else:\n        self.msg_err(\"Tried to delete lightcurve %i but only have %i lightcurves. Skipping\" % (i, N))\n    return\n</code></pre>"},{"location":"litmusclass/#litmus.litmusclass.LITMUS.prefit","title":"<code>prefit(i=0, j=1)</code>","text":"<p>Performs the full fit for the chosen stats model and fitting method.</p> Source code in <code>litmus/litmusclass.py</code> <pre><code>def prefit(self, i=0, j=1):\n    \"\"\"\n    Performs the full fit for the chosen stats model and fitting method.\n    \"\"\"\n\n    lc_1, lc_2 = self.lightcurves[i], self.lightcurves[j]\n    self.data = self.model.lc_to_data(lc_1, lc_2)\n\n    self.fitproc.prefit(lc_1, lc_2)\n</code></pre>"},{"location":"litmusclass/#litmus.litmusclass.LITMUS.fit","title":"<code>fit(i=0, j=1) -&gt; None</code>","text":"<p>Performs the full fit for the chosen stats model and fitting method.</p> Source code in <code>litmus/litmusclass.py</code> <pre><code>def fit(self, i=0, j=1) -&gt; None:\n    \"\"\"\n    Performs the full fit for the chosen stats model and fitting method.\n    \"\"\"\n\n    lc_1, lc_2 = self.lightcurves[i], self.lightcurves[j]\n    self.data = self.model.lc_to_data(lc_1, lc_2)\n\n    self.fitproc.fit(lc_1, lc_2)\n\n    self.samples = self.fitproc.get_samples(self.Nsamples)\n    self.C.add_chain(Chain(samples=DataFrame.from_dict(self.samples), name=\"Lightcurves %i-%i\" % (i, j)))\n</code></pre>"},{"location":"litmusclass/#litmus.litmusclass.LITMUS.save_chain","title":"<code>save_chain(path: str = None, headings: bool = True) -&gt; None</code>","text":"<p>Saves the litmus's output chains to a .csv file at \"path\" If headings=True (default) then the names of the parameters will be written to the first row of the tile</p>"},{"location":"litmusclass/#litmus.litmusclass.LITMUS.save_chain--todo-this-needs-updating","title":"todo - this needs updating","text":"Source code in <code>litmus/litmusclass.py</code> <pre><code>def save_chain(self, path: str = None, headings: bool = True) -&gt; None:\n    \"\"\"\n    Saves the litmus's output chains to a .csv file at \"path\"\n    If headings=True (default) then the names of the parameters will be written to the first row of the tile\n    #todo - this needs updating\n    \"\"\"\n    if path is None:\n        path = \"./%s_%s.csv\" % (self.model.name, self.fitproc.name)\n        if path[-4:] != \".csv\": path += \".csv\"\n\n    rows = zip(*self.samples.values())\n    with open(path, mode=\"w\", newline=\"\") as file:\n        writer = csv.writer(file)\n        # Write header\n        if headings: writer.writerow(self.samples.keys())\n        # Write rows\n        writer.writerows(rows)\n</code></pre>"},{"location":"litmusclass/#litmus.litmusclass.LITMUS.read_chain","title":"<code>read_chain(path: str, header: _types.Iterable[str] | None = None)</code>","text":""},{"location":"litmusclass/#litmus.litmusclass.LITMUS.read_chain--todo-needs-updating","title":"todo needs updating","text":"Source code in <code>litmus/litmusclass.py</code> <pre><code>def read_chain(self, path: str, header: _types.Iterable[str] | None = None):\n    \"\"\"\n    #todo needs updating\n    \"\"\"\n    # Reading the CSV into a DataFrame\n    df = pd.read_csv(path)\n\n    if header is None:\n        keys = df.columns\n    else:\n        keys = header.copy()\n\n    # Converting DataFrame to dictionary of numpy arrays\n    out = {col: df[col].to_numpy() for col in keys}\n\n    if out.keys() &lt;= set(self.fitproc.stat_model.paramnames()):\n        self.samples = out\n        self.msg_run(\"Loaded chain /w headings\", *keys)\n    else:\n        self.msg_err(\"Tried to load chain with different parameter names to model\")\n</code></pre>"},{"location":"litmusclass/#litmus.litmusclass.LITMUS.config","title":"<code>config(**kwargs)</code>","text":"<p>Quick and easy way to pass arguments to the chainconsumer object. Allows editing while prote</p> Source code in <code>litmus/litmusclass.py</code> <pre><code>def config(self, **kwargs):\n    '''\n    Quick and easy way to pass arguments to the chainconsumer object.\n    Allows editing while prote\n    '''\n    self.C.set_override(ChainConfig(**kwargs))\n</code></pre>"},{"location":"litmusclass/#litmus.litmusclass.LITMUS.plot_lightcurves","title":"<code>plot_lightcurves(model_no: int = 0, Nsamples: int = 1, Tspan: None | list[float, float] = None, Nplot: int = 1024, dir: str | None = None, show: bool = True) -&gt; matplotlib.figure.Figure()</code>","text":"<p>Plots the interpolated lightcurves for one of the fitted models</p> <p>Parameters:</p> Name Type Description Default <code>model_no</code> <code>int</code> <p>Which model to plot the lightcurves for</p> <code>0</code> <code>Nsamples</code> <code>int</code> <p>Number of posterior samples to draw from when plotting</p> <code>1</code> <code>Tspan</code> <code>None | list[float, float]</code> <p>Span of time values to plot over. If None, will use the max / min times of lc_1 and lc_2</p> <code>None</code> <code>Nplot</code> <code>int</code> <p>Number of points in the interpolated lightcurve</p> <code>1024</code> <code>dir</code> <code>str | None</code> <p>If not None, will save to this filepath</p> <code>None</code> <code>show</code> <code>bool</code> <p>If True, will plt.show() the plot</p> <code>True</code> Source code in <code>litmus/litmusclass.py</code> <pre><code>def plot_lightcurves(self, model_no: int = 0, Nsamples: int = 1, Tspan: None | list[float, float] = None,\n                     Nplot: int = 1024,\n                     dir: str | None = None, show: bool = True) -&gt; matplotlib.figure.Figure():\n    \"\"\"\n    Plots the interpolated lightcurves for one of the fitted models\n    :param model_no: Which model to plot the lightcurves for\n    :parameter Nsamples: Number of posterior samples to draw from when plotting\n    :parameter Tspan: Span of time values to plot over. If None, will use the max / min times of lc_1 and lc_2\n    :parameter Nplot: Number of points in the interpolated lightcurve\n    :parameter dir: If not None, will save to this filepath\n    :parameter show: If True, will plt.show() the plot\n    \"\"\"\n\n    self.msg_err(\"plot_lightcurve() not yet implemented\")\n    fig = plt.figure()\n    return fig\n</code></pre>"},{"location":"litmusclass/#litmus.litmusclass.LITMUS.plot_parameters","title":"<code>plot_parameters(model_no: int | None = None, Nsamples: int = None, CC_kwargs: dict = {}, truth: dict = None, params: [str] = None, show: bool = True, prior_extents: bool = False, dir: str | None = None) -&gt; matplotlib.figure.Figure</code>","text":"<p>Creates a nicely formatted chainconsumer plot of the parameters</p> <p>Parameters:</p> Name Type Description Default <code>model_no</code> <code>int | None</code> <p>Which model to plot the lightcurves for. If None, will plot for all</p> <code>None</code> <code>Nsamples</code> <code>int</code> <p>Number of posterior samples to draw from when plotting</p> <code>None</code> <code>CC_kwargs</code> <code>dict</code> <p>Keyword arguments to pass to the chainconsumer constructor</p> <code>{}</code> <code>truth</code> <code>dict</code> <p>Dictionary of parameter names to truth values</p> <code>None</code> <code>params</code> <code>[str]</code> <p>List of parameters to plot</p> <code>None</code> <code>show</code> <code>bool</code> <p>If True, will show the plot</p> <code>True</code> <code>prior_extents</code> <code>bool</code> <p>If True, will use the model prior range for the axes limits (Defaults to false if multiple models used)</p> <code>False</code> <code>dir</code> <code>str | None</code> <p>If not None, will save to this filepath</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Returns the matplotlib figure</p> Source code in <code>litmus/litmusclass.py</code> <pre><code>def plot_parameters(self, model_no: int | None = None, Nsamples: int = None, CC_kwargs: dict = {},\n                    truth: dict = None, params: [str] = None,\n                    show: bool = True,\n                    prior_extents: bool = False, dir: str | None = None) -&gt; matplotlib.figure.Figure:\n    \"\"\"\n    Creates a nicely formatted chainconsumer plot of the parameters\n    :param model_no: Which model to plot the lightcurves for. If None, will plot for all\n    :param Nsamples: Number of posterior samples to draw from when plotting\n    :parameter CC_kwargs: Keyword arguments to pass to the chainconsumer constructor\n    :parameter truth: Dictionary of parameter names to truth values\n    :parameter params: List of parameters to plot\n    :parameter show: If True, will show the plot\n    :parameter prior_extents: If True, will use the model prior range for the axes limits (Defaults to false if multiple models used)\n    :parameter dir: If not None, will save to this filepath\n    :return: Returns the matplotlib figure\n    \"\"\"\n\n    if Nsamples is not None and Nsamples != self.Nsamples:\n        C = ChainConsumer()\n        samples = self.fitproc.get_samples(Nsamples, **CC_kwargs)\n        C.add_chain(Chain(samples=DataFrame.from_dict(samples), name='samples'))\n    else:\n        C = self.C\n\n    if prior_extents:\n        _config = PlotConfig(extents=self.model.prior_ranges, summarise=True,\n                             **CC_kwargs)\n    else:\n        _config = PlotConfig(summarise=True,\n                             **CC_kwargs)\n    C.plotter.set_config(_config)\n    if params is None: params = self.model.free_params()\n    params_toplot = [param for param in self.model.free_params() if\n                     self.samples[param].ptp() != 0 and param in params]\n    if len(params_toplot) == 0:\n        fig = plt.figure()\n        if show: plt.show()\n        return fig\n\n    if truth is not None:\n        truth_toplot = {key: val for key, val in zip(truth.keys(), truth.values()) if key in params_toplot}\n        truth = Truth(location=truth_toplot)\n        C.add_truth(truth)\n\n    try:\n        fig = C.plotter.plot(columns=params_toplot\n                             )\n    except:\n        fig = plt.figure()\n        fig.text(0.5, 0.5, \"Something wrong with plotter\")\n    fig.tight_layout()\n    if show: fig.show()\n\n    if dir is not None:\n        plt.savefig(dir)\n\n    return fig\n</code></pre>"},{"location":"litmusclass/#litmus.litmusclass.LITMUS.lag_plot","title":"<code>lag_plot(Nsamples: int = None, show: bool = True, extras: bool = True, prior_extents=False, dir: str | None = None) -&gt; matplotlib.figure.Figure</code>","text":"<p>Creates a nicely formatted chainconsumer plot of the marginalized lag plot</p> <p>Parameters:</p> Name Type Description Default <code>Nsamples</code> <code>int</code> <p>Number of posterior samples to draw from when plotting</p> <code>None</code> <code>show</code> <code>bool</code> <p>If True, will show the plot</p> <code>True</code> <code>extras</code> <code>bool</code> <p>If True, will add any fitting method specific extras to the plot</p> <code>True</code> <code>dir</code> <code>str | None</code> <p>If not None, will save to this filepath</p> <code>None</code> <code>prior_extents</code> <p>If True, will use the model prior range for the axes limits (Defaults to false if multiple models used)  Returns the matplotlib figure</p> <code>False</code> Source code in <code>litmus/litmusclass.py</code> <pre><code>def lag_plot(self, Nsamples: int = None, show: bool = True, extras: bool = True, prior_extents=False,\n             dir: str | None = None, ) -&gt; matplotlib.figure.Figure:\n    \"\"\"\n    Creates a nicely formatted chainconsumer plot of the marginalized lag plot\n    :param Nsamples: Number of posterior samples to draw from when plotting\n    :parameter show: If True, will show the plot\n    :parameter extras: If True, will add any fitting method specific extras to the plot\n    :parameter dir: If not None, will save to this filepath\n    :parameter prior_extents: If True, will use the model prior range for the axes limits (Defaults to false if multiple models used)\n\n    Returns the matplotlib figure\n    \"\"\"\n    if 'lag' not in self.model.free_params():\n        self.msg_err(\"Can't plot lags for a model without lags.\")\n        return\n\n    if Nsamples is not None and Nsamples != self.Nsamples:\n        C = ChainConsumer()\n        samples = self.fitproc.get_samples(Nsamples)\n        C.add_chain(Chain(samples=DataFrame.from_dict(samples), name=\"lags\"))\n    else:\n        C = self.C\n\n    _config = PlotConfig(extents=self.model.prior_ranges, summarise=True)\n    C.plotter.set_config(_config)\n    fig = C.plotter.plot_distributions(columns=['lag'], figsize=(8, 4))\n    if prior_extents: fig.axes[0].set_xlim(*self.model.prior_ranges['lag'])\n    fig.axes[0].set_ylim(*fig.axes[0].get_ylim())\n    fig.tight_layout()\n\n    fig.axes[0].grid()\n\n    # Method specific plotting of fun stuff\n    if extras:\n        if isinstance(self.fitproc, fitting_methods.hessian_scan):\n            X, logY = self.fitproc._get_slices('lags', 'logZ')\n\n            if self.fitproc.interp_scale == 'linear':\n                Y = np.exp(logY - logY.max())\n                Y /= np.trapz(Y, X)\n                fig.axes[0].plot(X, Y)\n\n            elif self.fitproc.interp_scale == 'log':\n                Xterp = np.linspace(*self.model.prior_ranges['lag'], self.Nsamples)\n                logYterp = np.interp(Xterp, X, logY, left=logY[0], right=logY[-1])\n                Yterp = np.exp(logYterp - logYterp.max())\n                Yterp /= np.trapz(Yterp, Xterp)\n                fig.axes[0].plot(Xterp, Yterp)\n\n            plt.scatter(self.fitproc.lags, np.zeros_like(self.fitproc.lags), c='red', s=20)\n            plt.scatter(X, np.zeros_like(X), c='black', s=20)\n    if dir is not None:\n        plt.savefig(dir)\n    if show: fig.show()\n    return (fig)\n</code></pre>"},{"location":"litmusclass/#litmus.litmusclass.LITMUS.diagnostic_plots","title":"<code>diagnostic_plots(dir: str | None = None, show: bool = False, **kwargs)</code>","text":"<p>Generates a diagnostic plot window</p> <p>Parameters:</p> Name Type Description Default <code>dir</code> <code>str | None</code> <p>If not None, will save to this filepath</p> <code>None</code> <code>show</code> <code>bool</code> <p>If True, will show the plot  If dir!=None, will plt.savefig to the filepath 'dir' with **kwargs</p> <code>False</code> Source code in <code>litmus/litmusclass.py</code> <pre><code>def diagnostic_plots(self, dir: str | None = None, show: bool = False, **kwargs):\n    \"\"\"\n    Generates a diagnostic plot window\n    :param dir: If not None, will save to this filepath\n    :param show: If True, will show the plot\n\n    If dir!=None, will plt.savefig to the filepath 'dir' with **kwargs\n    \"\"\"\n    if hasattr(self.fitproc, \"diagnostics\"):\n        self.fitproc.diagnostics()\n    else:\n        self.msg_err(\"diagnostic_plots() not yet implemented for fitting method %s\" % (self.fitproc.name))\n\n    if dir is not None:\n        plt.savefig(dir, **kwargs)\n\n    if show: plt.show()\n\n    return\n</code></pre>"},{"location":"litmusclass/#litmus.litmusclass.suppress_stdout","title":"<code>suppress_stdout()</code>","text":"Source code in <code>litmus/_utils.py</code> <pre><code>@contextmanager\ndef suppress_stdout():\n    ipython = get_ipython()\n\n    if ipython and hasattr(ipython, 'kernel'):  # Likely in a Jupyter notebook\n        with io.capture_output() as captured:\n            yield\n    else:\n        # Standard Python environment\n        original_stdout_fd = os.dup(sys.stdout.fileno())\n        with open(os.devnull, 'w') as devnull:\n            os.dup2(devnull.fileno(), sys.stdout.fileno())\n            try:\n                yield\n            finally:\n                os.dup2(original_stdout_fd, sys.stdout.fileno())\n                os.close(original_stdout_fd)\n</code></pre>"},{"location":"litmusclass/#litmus.litmusclass.isiter","title":"<code>isiter(x: any) -&gt; bool</code>","text":"<p>Checks to see if an object is itterable</p> Source code in <code>litmus/_utils.py</code> <pre><code>def isiter(x: any) -&gt; bool:\n    \"\"\"\n    Checks to see if an object is itterable\n    \"\"\"\n    if type(x) == dict:\n        return len(x[list(x.keys())[0]]) &gt; 1\n    try:\n        iter(x)\n    except:\n        return (False)\n    else:\n        return (True)\n</code></pre>"},{"location":"litmusclass/#litmus.litmusclass.isiter_dict","title":"<code>isiter_dict(DICT: dict) -&gt; bool</code>","text":"<p>like isiter but for a dictionary. Checks only the first element in DICT.keys</p> Source code in <code>litmus/_utils.py</code> <pre><code>def isiter_dict(DICT: dict) -&gt; bool:\n    \"\"\"\n    like isiter but for a dictionary. Checks only the first element in DICT.keys\n    \"\"\"\n\n    key = list(DICT.keys())[0]\n    if isiter(DICT[key]):\n        return True\n    else:\n        return False\n</code></pre>"},{"location":"litmusclass/#litmus.litmusclass.dict_dim","title":"<code>dict_dim(DICT: dict) -&gt; (int, int)</code>","text":"<p>Checks the first element of a dictionary and returns its length</p> Source code in <code>litmus/_utils.py</code> <pre><code>def dict_dim(DICT: dict) -&gt; (int, int):\n    \"\"\"\n    Checks the first element of a dictionary and returns its length\n    \"\"\"\n\n    if isiter_dict(DICT):\n        firstkey = list(DICT.keys())[0]\n        return (len(list(DICT.keys())), len(DICT[firstkey]))\n    else:\n        return (len(list(DICT.keys())), 1)\n</code></pre>"},{"location":"litmusclass/#litmus.litmusclass.dict_pack","title":"<code>dict_pack(DICT: dict, keys=None, recursive=True, H=None, d0={}) -&gt; np.array</code>","text":"<p>Packs a dictionary into an array format</p> <p>Parameters:</p> Name Type Description Default <code>DICT</code> <code>dict</code> <p>the dict to unpack</p> required <code>keys</code> <p>the order in which to index the keyed elements. If none, will use DICT.keys(). Can be partial</p> <code>None</code> <code>recursive</code> <p>whether to recurse into arrays</p> <code>True</code> <code>H</code> <p>Matrix to scale parameters by</p> <code>None</code> <code>d0</code> <p>Value to offset by before packing</p> <code>{}</code> <p>Returns:</p> Type Description <code>array</code> <p>(nkeys x len_array) np.arrayobject  X = H (d-d0)</p> Source code in <code>litmus/_utils.py</code> <pre><code>def dict_pack(DICT: dict, keys=None, recursive=True, H=None, d0={}) -&gt; np.array:\n    \"\"\"\n    Packs a dictionary into an array format\n    :param DICT: the dict to unpack\n    :param keys: the order in which to index the keyed elements. If none, will use DICT.keys(). Can be partial\n    :param recursive: whether to recurse into arrays\n    :param H: Matrix to scale parameters by\n    :param d0: Value to offset by before packing\n    :return: (nkeys x len_array) np.arrayobject\n\n    X = H (d-d0)\n    \"\"\"\n\n    nokeys = True if keys is None else 0\n    keys = keys if keys is not None else DICT.keys()\n\n    if d0 is {}: d0 = {key:0 for key in keys}\n\n    for key in keys:\n        if key in DICT.keys() and key not in d0.keys(): d0 |= {key: 0.0}\n\n    if recursive and type(list(DICT.values())[0]) == dict:\n        out = np.array(\n            [dict_pack(DICT[key] - d0[key], keys=keys if not nokeys else None, recursive=recursive) for key in keys])\n    else:\n        if isiter(DICT[list(keys)[0]]):\n            out = np.array([[DICT[key][i] - d0[key] for i in range(dict_dim(DICT)[1])] for key in keys])\n        else:\n            out = np.array([DICT[key] - d0[key] for key in keys])\n\n    return (out)\n</code></pre>"},{"location":"litmusclass/#litmus.litmusclass.dict_unpack","title":"<code>dict_unpack(X: np.array, keys: [str], recursive=True, Hinv=None, x0=None) -&gt; np.array</code>","text":"<p>Unpacks an array into a dict</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>Array to unpack</p> required <code>keys</code> <code>[str]</code> <p>keys to unpack with</p> required <p>Returns:</p> Type Description <code>array</code> <p>Hinv(X) + x0</p> Source code in <code>litmus/_utils.py</code> <pre><code>def dict_unpack(X: np.array, keys: [str], recursive=True, Hinv=None, x0=None) -&gt; np.array:\n    \"\"\"\n    Unpacks an array into a dict\n    :param X: Array to unpack\n    :param keys: keys to unpack with\n    :return:\n\n    Hinv(X) + x0\n    \"\"\"\n    if Hinv is not None: assert Hinv.shape[0] == len(keys), \"Size of H must be equal to number of keys in dict_unpack\"\n\n    if recursive and isiter(X[0]):\n        out = {key: dict_unpack(X[i], keys, recursive) for i, key in enumerate(list(keys))}\n    else:\n        X = X.copy()\n        if Hinv is not None:\n            X = np.dot(Hinv, X)\n        if x0 is not None:\n            X += x0\n        out = {key: X[i] for i, key in enumerate(list(keys))}\n\n    return (out)\n</code></pre>"},{"location":"litmusclass/#litmus.litmusclass.dict_sortby","title":"<code>dict_sortby(A: dict, B: dict, match_only=True) -&gt; dict</code>","text":"<p>Sorts dict A to match keys of dict B.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>dict</code> <p>Dict to be sorted</p> required <code>B</code> <code>dict</code> <p>Dict whose keys are will provide the ordering</p> required <code>match_only</code> <p>If true, returns only for keys common to both A and B. Else, append un-sorted entries to end</p> <code>True</code> <p>Returns:</p> Type Description <code>dict</code> <p>{key: A[key] for key in B if key in A}</p> Source code in <code>litmus/_utils.py</code> <pre><code>def dict_sortby(A: dict, B: dict, match_only=True) -&gt; dict:\n    \"\"\"\n    Sorts dict A to match keys of dict B.\n\n    :param A: Dict to be sorted\n    :param B: Dict whose keys are will provide the ordering\n    :param match_only: If true, returns only for keys common to both A and B. Else, append un-sorted entries to end\n    :return: {key: A[key] for key in B if key in A}\n    \"\"\"\n    out = {key: A[key] for key in B if key in A}\n    if not match_only:\n        out |= {key: A[key] for key in A if key not in B}\n    return (out)\n</code></pre>"},{"location":"litmusclass/#litmus.litmusclass.dict_extend","title":"<code>dict_extend(A: dict, B: dict = None) -&gt; dict</code>","text":"<p>Extends all single-length entries of a dict to match the length of a non-singular element</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>dict</code> <p>Dictionary whose elements are to be extended</p> required <code>B</code> <code>dict</code> <p>(optional) the array to extend by, equivalent to dict_extend(A|B)</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dict A with any singleton elements extended to the longest entry in A or B</p> Source code in <code>litmus/_utils.py</code> <pre><code>def dict_extend(A: dict, B: dict = None) -&gt; dict:\n    \"\"\"\n    Extends all single-length entries of a dict to match the length of a non-singular element\n    :param A: Dictionary whose elements are to be extended\n    :param B: (optional) the array to extend by, equivalent to dict_extend(A|B)\n    :return: Dict A with any singleton elements extended to the longest entry in A or B\n    \"\"\"\n\n    out = A.copy()\n    if B is not None: out |= B\n\n    to_extend = [key for key in out if not isiter(out[key])]\n    to_leave = [key for key in out if isiter(out[key])]\n\n    if len(to_extend) == 0: return out\n    if len(to_leave) == 0: return out\n\n    N = len(out[to_leave[0]])\n    for key in to_leave[1:]:\n        assert len(out[key]) == N, \"Tried to dict_extend() a dictionary with inhomogeneous lengths\"\n\n    for key in to_extend:\n        out[key] = np.array([A[key]] * N)\n\n    return (out)\n</code></pre>"},{"location":"litmusclass/#litmus.litmusclass.dict_combine","title":"<code>dict_combine(X: [dict]) -&gt; {str: [float]}</code>","text":"<p>Combines an array, list etc. of dictionaries into a dictionary of arrays</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>[dict]</code> <p>1D Iterable of dicts</p> required <p>Returns:</p> Type Description <code>{str: [float]}</code> <p>Dict of 1D iterables</p> Source code in <code>litmus/_utils.py</code> <pre><code>def dict_combine(X: [dict]) -&gt; {str: [float]}:\n    \"\"\"\n    Combines an array, list etc. of dictionaries into a dictionary of arrays\n\n    :param X: 1D Iterable of dicts\n    :return: Dict of 1D iterables\n    \"\"\"\n\n    N = len(X)\n    keys = X[0].keys()\n\n    out = {key: np.zeros(N) for key in keys}\n    for n in range(N):\n        for key in keys:\n            out[key][n] = X[n][key]\n    return (out)\n</code></pre>"},{"location":"litmusclass/#litmus.litmusclass.dict_divide","title":"<code>dict_divide(X: dict) -&gt; [dict]</code>","text":"<p>Splits dict of arrays into array of dicts. Opposite of dict_combine</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>dict</code> <p>Dict of 1D iterables</p> required <p>Returns:</p> Type Description <code>[dict]</code> <p>1D Iterable of dicts</p> Source code in <code>litmus/_utils.py</code> <pre><code>def dict_divide(X: dict) -&gt; [dict]:\n    \"\"\"\n    Splits dict of arrays into array of dicts. Opposite of dict_combine\n\n    :param X: Dict of 1D iterables\n    :return: 1D Iterable of dicts\n    \"\"\"\n\n    keys = list(X.keys())\n    N = len(X[keys[0]])\n\n    out = [{key: X[key][i] for key in X} for i in range(N)]\n\n    return (out)\n</code></pre>"},{"location":"litmusclass/#litmus.litmusclass.dict_split","title":"<code>dict_split(X: dict, keys: [str]) -&gt; (dict, dict)</code>","text":"<p>Splits a dict in two based on keys</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>dict</code> <p>Dict to be split into A,B</p> required <code>keys</code> <code>[str]</code> <p>Keys to be present in A, but not in B</p> required <p>Returns:</p> Type Description <code>(dict, dict)</code> <p>tuple of dicts (A,B)</p> Source code in <code>litmus/_utils.py</code> <pre><code>def dict_split(X: dict, keys: [str]) -&gt; (dict, dict):\n    \"\"\"\n    Splits a dict in two based on keys\n\n    :param X: Dict to be split into A,B\n    :param keys: Keys to be present in A, but not in B\n    :return: tuple of dicts (A,B)\n    \"\"\"\n    assert type(X) is dict, \"input to dict_split() must be of type dict\"\n    assert isiter(keys) and type(keys[0])==str, \"in dict_split() keys must be list of strings\"\n    A = {key: X[key] for key in keys}\n    B = {key: X[key] for key in X.keys() if key not in keys}\n    return (A, B)\n</code></pre>"},{"location":"litmusclass/#litmus.litmusclass.pack_function","title":"<code>pack_function(func, packed_keys: [str], fixed_values: dict = {}, invert: bool = False, jit: bool = False, H: np.array = None, d0: dict = {}) -&gt; _types.FunctionType</code>","text":"<p>Re-arranges a function that takes dict arguments to tak array-like arguments instead, so as to be autograd friendly Takes a function f(D:dict, arg, kwargs) and returns f(X, D2, args, **kwargs), D2 is all elements of D not listed in 'packed_keys' or fixed_values.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <p>Function to be unpacked</p> required <code>packed_keys</code> <code>[str]</code> <p>Keys in 'D' to be packed in an array</p> required <code>fixed_values</code> <code>dict</code> <p>Elements of 'D' to be fixed</p> <code>{}</code> <code>invert</code> <code>bool</code> <p>If true, will 'flip' the function upside down</p> <code>False</code> <code>jit</code> <code>bool</code> <p>If true, will 'jit' the function</p> <code>False</code> <code>H</code> <code>array</code> <p>(optional) scaling matrix to reparameterize H with</p> <code>None</code> <code>d0</code> <code>dict</code> <p>(optional) If given, will center the reparameterized  function at x0</p> <code>{}</code> Source code in <code>litmus/_utils.py</code> <pre><code>def pack_function(func, packed_keys: ['str'], fixed_values: dict = {}, invert: bool = False, jit: bool = False,\n                  H: np.array = None, d0: dict = {}) -&gt; _types.FunctionType:\n    \"\"\"\n    Re-arranges a function that takes dict arguments to tak array-like arguments instead, so as to be autograd friendly\n    Takes a function f(D:dict, *arg, **kwargs) and returns f(X, D2, *args, **kwargs), D2 is all elements of D not\n    listed in 'packed_keys' or fixed_values.\n\n    :param func: Function to be unpacked\n    :param packed_keys: Keys in 'D' to be packed in an array\n    :param fixed_values: Elements of 'D' to be fixed\n    :param invert:  If true, will 'flip' the function upside down\n    :param jit: If true, will 'jit' the function\n    :param H: (optional) scaling matrix to reparameterize H with\n    :param d0: (optional) If given, will center the reparameterized  function at x0\n    \"\"\"\n\n    if H is not None:\n        assert H.shape[0] == len(packed_keys), \"Scaling matrix H must be same length as packed_keys\"\n    else:\n        H = jnp.eye(len(packed_keys))\n    d0 = {key: 0.0 for key in packed_keys} | d0\n    x0 = dict_pack(d0, packed_keys)\n\n    # --------\n\n    sign = -1 if invert else 1\n\n    # --------\n    def new_func(X, unpacked_params={}, *args, **kwargs):\n        X = jnp.dot(H, X - x0)\n        packed_dict = {key: x for key, x in zip(packed_keys, X)}\n        packed_dict |= unpacked_params\n        packed_dict |= fixed_values\n\n        out = func(packed_dict, *args, **kwargs)\n        return (sign * out)\n\n    # --------\n    if jit: new_func = jax.jit(new_func)\n\n    return (new_func)\n</code></pre>"},{"location":"litmusclass/#litmus.litmusclass.randint","title":"<code>randint()</code>","text":"<p>Quick utility to generate a random integer</p> Source code in <code>litmus/_utils.py</code> <pre><code>def randint():\n    \"\"\"\n    Quick utility to generate a random integer\n    \"\"\"\n    return (np.random.randint(0, sys.maxsize // 1024))\n</code></pre>"},{"location":"logging/","title":"<code>litmus.logging</code>","text":"<p>logging.py</p> <p>Collection of logging functions in a handy inheritable class, to make it easier to edit later in one place</p>"},{"location":"logging/#litmus.logging.logger","title":"<code>logger(out_stream=sys.stdout, err_stream=sys.stderr, verbose: bool = True, debug: bool = False)</code>","text":"<p>Object class that contains methods for printing to debug, verbose and error streams. Internal behaviour may change in future releases</p> <p>Parameters:</p> Name Type Description Default <code>out_stream</code> <p>Stream to print run and debug messages to</p> <code>stdout</code> <code>err_stream</code> <p>Stream to print error messages to</p> <code>stderr</code> <code>verbose</code> <code>bool</code> <p>Whether to print msg_run statements</p> <code>True</code> <code>debug</code> <code>bool</code> <p>Whether to print msg_debug statements</p> <code>False</code> Source code in <code>litmus/logging.py</code> <pre><code>def __init__(self, out_stream=sys.stdout, err_stream=sys.stderr, verbose: bool = True, debug: bool = False):\n    \"\"\"\n    :param out_stream: Stream to print run and debug messages to\n    :param err_stream: Stream to print error messages to\n    :param verbose: Whether to print msg_run statements\n    :param debug: Whether to print msg_debug statements\n    \"\"\"\n    # ----------------------------\n\n    self.out_stream = out_stream\n    self.err_stream = err_stream\n    self.verbose = verbose\n    self.debug = debug\n</code></pre>"},{"location":"logging/#litmus.logging.logger.out_stream","title":"<code>out_stream = out_stream</code>  <code>instance-attribute</code>","text":""},{"location":"logging/#litmus.logging.logger.err_stream","title":"<code>err_stream = err_stream</code>  <code>instance-attribute</code>","text":""},{"location":"logging/#litmus.logging.logger.verbose","title":"<code>verbose = verbose</code>  <code>instance-attribute</code>","text":""},{"location":"logging/#litmus.logging.logger.debug","title":"<code>debug = debug</code>  <code>instance-attribute</code>","text":""},{"location":"logging/#litmus.logging.logger.msg_err","title":"<code>msg_err(*x: str, end='\\n', delim=' ')</code>","text":"<p>Messages for when something has broken or been called incorrectly</p> Source code in <code>litmus/logging.py</code> <pre><code>def msg_err(self, *x: str, end='\\n', delim=' '):\n    \"\"\"\n    Messages for when something has broken or been called incorrectly\n    \"\"\"\n    if True:\n        for a in x:\n            print(a, file=self.err_stream, end=delim)\n\n    print(end, end='')\n    return\n</code></pre>"},{"location":"logging/#litmus.logging.logger.msg_run","title":"<code>msg_run(*x: str, end='\\n', delim=' ')</code>","text":"<p>Standard messages about when things are running</p> Source code in <code>litmus/logging.py</code> <pre><code>def msg_run(self, *x: str, end='\\n', delim=' '):\n    \"\"\"\n    Standard messages about when things are running\n    \"\"\"\n    if self.verbose:\n        for a in x:\n            print(a, file=self.out_stream, end=delim)\n\n        print(end, end='')\n    return\n</code></pre>"},{"location":"logging/#litmus.logging.logger.msg_debug","title":"<code>msg_debug(*x: str, end='\\n', delim=' ')</code>","text":"<p>Explicit messages to help debug when things are behaving strangely</p> Source code in <code>litmus/logging.py</code> <pre><code>def msg_debug(self, *x: str, end='\\n', delim=' '):\n    \"\"\"\n    Explicit messages to help debug when things are behaving strangely\n    \"\"\"\n    if self.debug:\n        for a in x:\n            print(a, file=self.out_stream, end=delim)\n\n        print(end, end='')\n    return\n</code></pre>"},{"location":"mocks/","title":"<code>litmus.mocks</code>","text":"<p>Some handy sets of mock data for use in testing</p> <p>HM Apr 2024</p>"},{"location":"mocks/#litmus.mocks.mock_A","title":"<code>mock_A = mock(season=None, lag=300)</code>  <code>module-attribute</code>","text":"<p>Instead of a GP this mock has a clear bell-curve like hump. This works as a generous test-case of lag recovery methods</p>"},{"location":"mocks/#litmus.mocks.mock_B","title":"<code>mock_B = mock(lag=256, maxtime=360 * 5, E=[0.01, 0.01], seed=1, season=180)</code>  <code>module-attribute</code>","text":"<p>A standard oz-des like seasonal GP</p>"},{"location":"mocks/#litmus.mocks.mock_C","title":"<code>mock_C = mock(lag=128, maxtime=360 * 5, E=[0.01, 0.01], season=None)</code>  <code>module-attribute</code>","text":"<p>A mock with no seasonal windowing</p>"},{"location":"mocks/#litmus.mocks.mock","title":"<code>mock(seed: int = 0, **kwargs)</code>","text":"<p>               Bases: <code>logger</code></p> <p>Handy class for making mock data. When calling with init, args can be passed as keyword arguments</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>seed for randomization</p> <code>0</code> <code>tau</code> <code>float</code> <p>Timescale of the GP to be simulated</p> required <code>cadence</code> <code>float</code> <p>Mean cadence of the signals, either both or [signal 1, signal 2]. Defaults to [7 days, 30 days].</p> required <code>cadence_var</code> <code>float</code> <p>std of cadence of the signals, either both or [signal 1, signal 2]. Defaults to [1 day, 5 days].</p> required <code>season</code> <code>float</code> <p>Average season length. Defaults to 180 days.</p> required <code>season_var</code> <code>float</code> <p>std of season length. Defaults to 14 days.</p> required <code>N</code> <code>int</code> <p>Number of datapoints for the underlying realisation. Defaults to 2048.</p> required <code>maxtime</code> <code>float</code> <p>Max time of the underlying simulation. Defaults to 5 years.</p> required <code>lag</code> <code>float</code> <p>Lag for signal 2. Defaults to 30 days.</p> required <code>E</code> <code>float</code> <p>Mean measurement error for the signals, either both or [signal 1, signal 2]. Defaults to 1% and 10%.</p> required <code>E_var</code> <code>float</code> <p>Std of measurement error for the signals, either both or [signal 1, signal 2]. Defaults to 0%</p> required Source code in <code>litmus/mocks.py</code> <pre><code>def __init__(self, seed: int = 0, **kwargs):\n    \"\"\"\n    :param seed: seed for randomization\n    :param float tau: Timescale of the GP to be simulated\n    :param float cadence: Mean cadence of the signals, either both or [signal 1, signal 2]. Defaults to [7 days, 30 days].\n    :param float cadence_var: std of cadence of the signals, either both or [signal 1, signal 2]. Defaults to [1 day, 5 days].\n    :param float season: Average season length. Defaults to 180 days.\n    :param float season_var: std of season length. Defaults to 14 days.\n    :param int N: Number of datapoints for the underlying realisation. Defaults to 2048.\n    :param float maxtime: Max time of the underlying simulation. Defaults to 5 years.\n    :param float lag: Lag for signal 2. Defaults to 30 days.\n    :param float E: Mean measurement error for the signals, either both or [signal 1, signal 2]. Defaults to 1% and 10%.\n    :param float E_var: Std of measurement error for the signals, either both or [signal 1, signal 2]. Defaults to 0%\n    \"\"\"\n    defaultkwargs = {'tau': 400.0,\n                     'cadence': [7, 30],\n                     'cadence_var': [1, 5],\n                     'season': 180,\n                     'season_var': 14,\n                     'N': 2048,\n                     'maxtime': 360 * 5,\n                     'lag': 30,\n                     'E': [0.01, 0.1],\n                     'E_var': [0.0, 0.0]\n                     }\n\n    logger.__init__(self)\n\n    self.seed: int = seed\n    self.lc, self.lc_1, self.lc_2 = None, None, None\n    self.lag:float = 0.0\n    kwargs = defaultkwargs | kwargs\n    self.args = {}\n\n    for key in ['cadence', 'cadence_var', 'E', 'E_var']:\n        if not (isiter(kwargs[key])): kwargs[key] = [kwargs[key], kwargs[key]]\n    for key, var in zip(kwargs.keys(), kwargs.values()):\n        self.__setattr__(key, var)\n        self.args[key] = var\n\n    self.generate(seed=seed)\n    return\n</code></pre>"},{"location":"mocks/#litmus.mocks.mock.seed","title":"<code>seed: int = seed</code>  <code>instance-attribute</code>","text":""},{"location":"mocks/#litmus.mocks.mock.lag","title":"<code>lag: float = 0.0</code>  <code>instance-attribute</code>","text":""},{"location":"mocks/#litmus.mocks.mock.args","title":"<code>args = {}</code>  <code>instance-attribute</code>","text":""},{"location":"mocks/#litmus.mocks.mock.generate_true","title":"<code>generate_true(seed: int = 0) -&gt; (_types.ArrayN, _types.ArrayN)</code>","text":"<p>Generates an underlying true DRW signal and stores in the self attribute self.lc</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>seed for random generation</p> <code>0</code> <p>Returns:</p> Type Description <code>(ArrayN, ArrayN)</code> <p>Array tuple (T,Y), underlying curve extending to maxtime + 2 * lag</p> Source code in <code>litmus/mocks.py</code> <pre><code>def generate_true(self, seed: int = 0) -&gt; (_types.ArrayN, _types.ArrayN):\n    \"\"\"\n    Generates an underlying true DRW signal and stores in the self attribute self.lc\n    :param seed: seed for random generation\n    :return: Array tuple (T,Y), underlying curve extending to maxtime + 2 * lag\n    \"\"\"\n    T = np.linspace(0.0, self.maxtime + self.lag * 2, self.N)\n    Y = gp_realization(T, tau=self.tau, seed=seed).Y\n    self.lc = lightcurve(T, Y)  # .trim(Tmin=0, Tmax=self.maxtime)\n    return (T, Y)\n</code></pre>"},{"location":"mocks/#litmus.mocks.mock.generate","title":"<code>generate(seed: int = 0) -&gt; (lightcurve, lightcurve)</code>","text":"<p>Generates a mock and sampled light-curve including a delayed response and stores in the self-attributes self.lc_1 and self.lc_2. Also returns as tuple (lc, lc_1, lc_2)</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>seed for random generation</p> <code>0</code> <p>Returns:</p> Type Description <code>(lightcurve, lightcurve)</code> <p>lightcurve object</p> Source code in <code>litmus/mocks.py</code> <pre><code>def generate(self, seed: int = 0) -&gt; (lightcurve, lightcurve):\n    \"\"\"\n    Generates a mock and sampled light-curve including a delayed response and stores in the self-attributes\n    self.lc_1 and self.lc_2. Also returns as tuple (lc, lc_1, lc_2)\n    :param seed: seed for random generation\n    :return: lightcurve object\n    \"\"\"\n\n    T, Y = self.generate_true(seed=seed)\n\n    T1 = mock_cadence(self.maxtime, seed, cadence=self.cadence[0], cadence_var=self.cadence_var[0],\n                      season=self.season, season_var=self.season_var,\n                      N=self.N)\n    T2 = mock_cadence(self.maxtime, seed, cadence=self.cadence[1], cadence_var=self.cadence_var[1],\n                      season=self.season, season_var=self.season_var,\n                      N=self.N)\n\n    Y1, Y2 = subsample(T, Y, T1), subsample(T + self.lag, Y, T2)\n    E1, E2 = [np.random.randn(len(x)) * ev + e for x, ev, e in zip([T1, T2], self.E_var, self.E)]\n\n    Y1 += np.random.randn(len(T1)) * abs(E1)\n    Y2 += np.random.randn(len(T2)) * abs(E2)\n\n    self.lc_1 = lightcurve(T1, Y1, E1)\n    self.lc_2 = lightcurve(T2, Y2, E2)\n\n    return (self.lc_1, self.lc_2)\n</code></pre>"},{"location":"mocks/#litmus.mocks.mock.copy","title":"<code>copy(seed: int = None, **kwargs) -&gt; _types.Self</code>","text":"<p>Returns a copy of the mock while over-writing certain params.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>int seed for random generation</p> <code>None</code> <code>kwargs</code> <p>kwargs to pass to the new lightcurve object, will overwrite self.kwargs in the copy</p> <code>{}</code> <p>Returns:</p> Type Description <code>Self</code> <p>A copy of self with kwargs and seed changed accordingly</p> Source code in <code>litmus/mocks.py</code> <pre><code>def copy(self, seed: int = None, **kwargs) -&gt; _types.Self:\n    \"\"\"\n    Returns a copy of the mock while over-writing certain params.\n    :param seed: int seed for random generation\n    :param kwargs: kwargs to pass to the new lightcurve object, will overwrite self.kwargs in the copy\n    :return: A copy of self with kwargs and seed changed accordingly\n    \"\"\"\n    if seed is None:\n        seed = self.seed\n\n    out = mock(seed=seed, **(self.args | kwargs))\n    return (out)\n</code></pre>"},{"location":"mocks/#litmus.mocks.mock.swap_response","title":"<code>swap_response(other: lightcurve) -&gt; None</code>","text":"<p>Swaps the response lightcurves between this mock and its target. Over-writes target and self</p> Source code in <code>litmus/mocks.py</code> <pre><code>def swap_response(self, other: lightcurve) -&gt; None:\n    \"\"\"\n    Swaps the response lightcurves between this mock and its target.\n    Over-writes target and self\n    \"\"\"\n\n    self.lc_2, other.lc_2 = other.lc_2, self.lc_2\n    self.lc, other.lc = None, None\n    return\n</code></pre>"},{"location":"mocks/#litmus.mocks.mock.plot","title":"<code>plot(axis: matplotlib.axes.Axes = None, true_args: dict = {}, series_args: dict = {}, show: bool = True) -&gt; _types.Figure</code>","text":"<p>Plots the lightcurves and subsamples</p> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>Axes</code> <p>matplotlib axis to plot to. If none will create new</p> <code>None</code> <code>true_args</code> <code>dict</code> <p>matplotlib plotting kwargs for the true underlying lightcurve</p> <code>{}</code> <code>series_args</code> <code>dict</code> <p>matplotlib plotting kwargs for the observations</p> <code>{}</code> <code>show</code> <code>bool</code> <p>If true will use plt.show() at the end fo the plot</p> <code>True</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plot figure</p> Source code in <code>litmus/mocks.py</code> <pre><code>def plot(self, axis: matplotlib.axes.Axes = None, true_args: dict = {}, series_args: dict = {},\n         show: bool = True) -&gt; _types.Figure:\n    \"\"\"\n    Plots the lightcurves and subsamples\n    :param axis: matplotlib axis to plot to. If none will create new\n    :param true_args: matplotlib plotting kwargs for the true underlying lightcurve\n    :param series_args: matplotlib plotting kwargs for the observations\n    :param show: If true will use plt.show() at the end fo the plot\n    :return: Plot figure\n    \"\"\"\n\n    # -----------------\n    # Make / get axis\n    if axis is None:\n        f = plt.figure()\n        axis = plt.gca()\n        axis.grid()\n        axis.set_xlabel(\"Time (days)\")\n        axis.set_ylabel(\"Signal Strength\")\n\n    # -----------------\n    # Plot underlying curves\n    true_args = {'lw': 0.5, 'c': ['tab:blue', 'tab:orange'], 'alpha': 0.3, 'label': ['True Signal', 'Response'],\n                 } | true_args\n    true_args_1 = true_args.copy()\n    true_args_2 = true_args.copy()\n\n    for key, val in zip(true_args.keys(), true_args.values()):\n        if isiter(val) and len(val) &gt; 1:\n            true_args_1[key] = true_args[key][0]\n            true_args_2[key] = true_args[key][1]\n        else:\n            true_args_1[key] = true_args[key]\n            true_args_2[key] = true_args[key]\n\n    if self.lc is not None:\n        lc_true_1, lc_true_2 = self.lc.delayed_copy(0, 0, self.maxtime), self.lc.delayed_copy(self.lag, 0,\n                                                                                              self.maxtime)\n\n        axis.plot(lc_true_1.T, lc_true_1.Y, **true_args_1)\n        axis.plot(lc_true_2.T, lc_true_2.Y, **true_args_2)\n\n    # -----------------\n    # Plot errorbars\n    series_args = {'c': ['tab:blue', 'tab:orange'], 'alpha': 1.0, 'capsize': 2, 'lw': 1.5,\n                   'label': [\"Signal\", \"Response\"]} | series_args\n    series_args_1 = series_args.copy()\n    series_args_2 = series_args.copy()\n\n    for key, val in zip(series_args.keys(), series_args.values()):\n        if isiter(val) and len(val) &gt; 1:\n            series_args_1[key] = series_args[key][0]\n            series_args_2[key] = series_args[key][1]\n        else:\n            series_args_1[key] = series_args[key]\n            series_args_2[key] = series_args[key]\n\n    axis.errorbar(self.lc_1.T, self.lc_1.Y, self.lc_1.E, fmt='none',\n                  **series_args_1\n                  )\n    axis.errorbar(self.lc_2.T, self.lc_2.Y, self.lc_2.E, fmt='none',\n                  **series_args_2\n                  )\n\n    series_args_1.pop('capsize'), series_args_2.pop('capsize')\n    axis.scatter(self.lc_1.T, self.lc_1.Y,\n                 **(series_args_1 | {'s': 3, 'label': None})\n                 )\n    axis.scatter(self.lc_2.T, self.lc_2.Y,\n                 **(series_args_2 | {'s': 3, 'label': None})\n                 )\n\n    if show: plt.show()\n    return axis.get_figure()\n</code></pre>"},{"location":"mocks/#litmus.mocks.mock.corrected_plot","title":"<code>corrected_plot(params: dict = {}, axis: matplotlib.axis.Axis = None, true_args: dict = {}, series_args: dict = {}, show: bool = False) -&gt; _types.Figure</code>","text":"<p>A copy of plot that offsets the displayed signals by self.lag to bring them into alignment.</p> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>Axis</code> <p>matplotlib axis to plot to. If none will create new</p> <code>None</code> <code>true_args</code> <code>dict</code> <p>matplotlib plotting kwargs for the true underlying lightcurve</p> <code>{}</code> <code>series_args</code> <code>dict</code> <p>matplotlib plotting kwargs for the observations</p> <code>{}</code> <code>show</code> <code>bool</code> <p>If true will use plt.show() at the end fo the plot</p> <code>False</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib figure</p> Source code in <code>litmus/mocks.py</code> <pre><code>def corrected_plot(self, params: dict = {}, axis: matplotlib.axis.Axis = None, true_args: dict = {},\n                   series_args: dict = {}, show: bool = False) -&gt; _types.Figure:\n    \"\"\"\n    A copy of plot that offsets the displayed signals by self.lag to bring them into alignment.\n    :param axis: matplotlib axis to plot to. If none will create new\n    :param true_args: matplotlib plotting kwargs for the true underlying lightcurve\n    :param series_args: matplotlib plotting kwargs for the observations\n    :param show: If true will use plt.show() at the end fo the plot\n    :return: matplotlib figure\n    \"\"\"\n    params = self.params() | params\n    corrected = self.copy()\n\n    corrected.lc_2.T -= params['lag']\n    corrected.lc_2 += params['rel_mean']\n    corrected.lc_2 *= params['rel_amp']\n\n    if 'alpha' in true_args.keys():\n        if isiter(true_args['alpha']):\n            true_args['alpha'][1] = 0.0\n        else:\n            true_args['alpha'] = [true_args['alpha'], 0.0]\n    else:\n        true_args |= {'alpha': [0.3, 0.0]}\n\n    corrected.plot(axis=axis, true_args=true_args, series_args=series_args, show=show)\n</code></pre>"},{"location":"mocks/#litmus.mocks.mock.params","title":"<code>params()</code>","text":"<p>Helper utility that returns numpyro-like parameters.</p> <p>Returns:</p> Type Description <p>Dict of param sites for gp_simple.</p> Source code in <code>litmus/mocks.py</code> <pre><code>def params(self):\n    \"\"\"\n    Helper utility that returns numpyro-like parameters.\n    :return: Dict of param sites for gp_simple.\n    \"\"\"\n    out = {\n        'lag': self.lag,\n        'logtau': np.log(self.tau),\n        'logamp': 0.0,\n        'rel_amp': 1.0,\n        'mean': 0.0,\n        'rel_mean': 0.0,\n    }\n    return (out)\n</code></pre>"},{"location":"mocks/#litmus.mocks.mock_cadence","title":"<code>mock_cadence(maxtime, seed: int = 0, cadence: float = 7, cadence_var: float = 1, season: float = 180, season_var: float = 14, N: int = 1024)</code>","text":"<p>Returns time series X values for a mock signal</p> <p>Parameters:</p> Name Type Description Default <code>maxtime</code> <p>Length of simulation</p> required <code>seed</code> <code>int</code> <p>Seed for randomization</p> <code>0</code> <code>cadence</code> <code>float</code> <p>Average cadence of observations</p> <code>7</code> <code>cadence_var</code> <code>float</code> <p>Standard deviation of the cadence</p> <code>1</code> <code>season</code> <code>float</code> <p>Average length of the observation season (default 180 days)</p> <code>180</code> <code>season_var</code> <code>float</code> <p>Standard deviation of the season length (default 14 days)</p> <code>14</code> <code>N</code> <code>int</code> <p>Number of observations used prior to trimming. This is auto-tuned and is deprecated</p> <code>1024</code> <p>Returns:</p> Type Description <p>returns as array of sample times</p> Source code in <code>litmus/mocks.py</code> <pre><code>def mock_cadence(maxtime, seed: int = 0, cadence: float = 7, cadence_var: float = 1, season: float = 180,\n                 season_var: float = 14, N: int = 1024):\n    \"\"\"\n    Returns time series X values for a mock signal\n    :param maxtime: Length of simulation\n    :param seed: Seed for randomization\n    :param cadence: Average cadence of observations\n    :param cadence_var: Standard deviation of the cadence\n    :param season: Average length of the observation season (default 180 days)\n    :param season_var: Standard deviation of the season length (default 14 days)\n    :param N: Number of observations used prior to trimming. This is auto-tuned and is deprecated\n\n    :return: returns as array of sample times\n    \"\"\"\n\n    np.random.seed(seed)\n\n    assert N &gt; 0, \"Invalid N. Must be &lt;=0\"\n\n    # Generate Measurements\n    while True:\n        diffs = np.random.randn(N) * cadence_var / np.sqrt(2) + cadence\n        T = np.cumsum(diffs)\n        if T.max() &lt;= maxtime:\n            N *= 2\n        else:\n            break\n    T = T[np.where((T &lt; (maxtime * 2)))[0]]\n    T += np.random.randn(len(T)) * cadence_var / np.sqrt(2)\n\n    # Make windowing function\n    if season is not None and season != 0:\n\n        no_seasons = int(maxtime / season)\n        window = np.zeros(len(T))\n        for n in range(no_seasons):\n            if n % 2 == 0: continue\n            tick = np.tanh((T - season * n) / season_var)\n            tick -= np.tanh((T - season * (n + 1)) / season_var)\n            tick /= 2\n            window += tick\n\n        R = np.random.rand(len(window))\n\n        T = T[np.where((R &lt; window) * (T &lt; maxtime))[0]]\n    else:\n        T = T[np.where(T &lt; maxtime)[0]]\n\n    return (T)\n</code></pre>"},{"location":"mocks/#litmus.mocks.subsample","title":"<code>subsample(T: _types.ArrayN, Y: _types.ArrayN, Tsample: _types.ArrayN) -&gt; _types.ArrayN</code>","text":"<p>Linearly interpolates between X and Y and returns interped Y's at positions Xsample</p> <p>Parameters:</p> Name Type Description Default <code>T</code> <code>ArrayN</code> <p>Time values of time series to be interpolated</p> required <code>Y</code> <code>ArrayN</code> <p>Y values of time series to be interpolated</p> required <code>Tsample</code> <code>ArrayN</code> <p>Times to subample at</p> required <p>Returns:</p> Type Description <code>ArrayN</code> <p>Elements of Y interpolated to times Tsample</p> Source code in <code>litmus/mocks.py</code> <pre><code>def subsample(T: _types.ArrayN, Y: _types.ArrayN, Tsample: _types.ArrayN) -&gt; _types.ArrayN:\n    \"\"\"\n    Linearly interpolates between X and Y and returns interped Y's at positions Xsample\n\n    :param T: Time values of time series to be interpolated\n    :param Y: Y values of time series to be interpolated\n    :param Tsample: Times to subample at\n    :return: Elements of Y interpolated to times Tsample\n\n    \"\"\"\n    out = np.interp(Tsample, T, Y)\n    return (out)\n</code></pre>"},{"location":"mocks/#litmus.mocks.outly","title":"<code>outly(Y, q) -&gt; _types.ArrayN</code>","text":"<p>Returns a copy of Y with fraction 'q' elements replaced with unit - normally distributed outliers</p> Source code in <code>litmus/mocks.py</code> <pre><code>def outly(Y, q) -&gt; _types.ArrayN:\n    \"\"\"\n    Returns a copy of Y with fraction 'q' elements replaced with\n    unit - normally distributed outliers\n    \"\"\"\n    I = np.random.rand(len(Y)) &lt; q\n    Y[I] = np.random.randn() * len(I)\n    return (Y)\n</code></pre>"},{"location":"mocks/#litmus.mocks.gp_realization","title":"<code>gp_realization(T, err: _types.Union[float, _types.ArrayN] = 0.0, tau: float = 400.0, basekernel: tinygp.kernels.quasisep = tinygp.kernels.quasisep.Exp, seed=None) -&gt; lightcurve</code>","text":"<p>Generates a gaussian process at times T and errors err</p> <p>Parameters:</p> Name Type Description Default <code>T</code> <p>Time of observations</p> required <code>err</code> <code>Union[float, ArrayN]</code> <p>Measurements uncertainty at observations. Must be float or array of same length as T</p> <code>0.0</code> <code>tau</code> <code>float</code> <p>Timescale of the kernel</p> <code>400.0</code> <code>basekernel</code> <code>quasisep</code> <p>Kernel of the GP. Any tinyGP quasisep kernel</p> <code>Exp</code> <code>seed</code> <code>None</code> <p>Returns:</p> Type Description <code>lightcurve</code> <p>Returns as lightcurve object</p> Source code in <code>litmus/mocks.py</code> <pre><code>def gp_realization(T, err: _types.Union[float, _types.ArrayN] = 0.0, tau: float = 400.0,\n                   basekernel: tinygp.kernels.quasisep = tinygp.kernels.quasisep.Exp,\n                   seed=None) -&gt; lightcurve:\n    '''\n    Generates a gaussian process at times T and errors err\n\n    :param T: Time of observations\n    :param err: Measurements uncertainty at observations. Must be float or array of same length as T\n    :param tau: Timescale of the kernel\n    :param basekernel: Kernel of the GP. Any tinyGP quasisep kernel\n    :param seed:\n\n    :return: Returns as lightcurve object\n    '''\n    if seed is None: seed = randint()\n\n    # -----------------\n    # Generate errors\n    N = len(T)\n    if isiter(err):\n        E = err\n    else:\n        E = np.random.randn(N) * np.sqrt(err) + err\n    E = abs(E)\n\n    gp = GaussianProcess(basekernel(scale=tau), T)\n    Y = gp.sample(jax.random.PRNGKey(seed))\n\n    return (lightcurve(T=T, Y=Y, E=E))\n</code></pre>"},{"location":"models/","title":"<code>litmus.models</code>","text":"<p>Contains NumPyro generative models.</p> <p>HM 24</p>"},{"location":"models/#litmus.models.stats_model","title":"<code>stats_model(prior_ranges=None, out_stream=sys.stdout, err_stream=sys.stderr, verbose=True, debug=False)</code>","text":"<p>               Bases: <code>logger</code></p> <p>Base class for bayesian generative models. Includes a series of utilities for evaluating likelihoods, gradients etc., as well as various</p> <p>On init, takes dict `prior_ranges' of the uniform boundaries of the parameter priors, or a single (float/int) value if the value is fixed.</p> <p>Also takes logging arg from the litmus.logging.logger object.</p> Source code in <code>litmus/models.py</code> <pre><code>def __init__(self, prior_ranges=None,\n             out_stream=sys.stdout,\n             err_stream=sys.stderr,\n             verbose=True,\n             debug=False,\n             ):\n    logger.__init__(self, out_stream=out_stream, err_stream=err_stream, verbose=verbose, debug=debug)\n\n    self._protected_keys = []\n\n    # Setting prior boundaries\n    if not hasattr(self, \"_default_prior_ranges\"):\n        self._default_prior_ranges: dict[str, list[float, float]] = {\n            'lag': _default_config['lag'],\n        }\n\n    # ------------------\n    # Wrapped function signature declarations\n    self._log_density_jit: _types.Callable[[dict, _types.Any], float] = lambda params, data: 0.0\n    self._log_density_uncon_jit: _types.Callable[[dict, _types.Any], float] = lambda params, data: 0.0\n    self._log_likelihood_jit: _types.Callable[[dict, _types.Any], float] = lambda params, data: 0.0\n    self._log_likelihood_jit: _types.Callable[[dict, _types.Any], float] = lambda params, data: 0.0\n\n    self._log_density_grad: _types.Callable[[dict, _types.Any], _types.ArrayM] = lambda params, data: np.array(\n        [0.0])\n    self._log_density_uncon_grad: _types.Callable[[dict, _types.Any], _types.ArrayM] = lambda params, data: np.array([0.0])\n\n    self._log_likelihood_grad: _types.Callable[[dict, _types.Any], _types.ArrayM] = lambda params, data: np.array([0.0])\n    self._log_likelihood_grad: _types.Callable[[dict, _types.Any], _types.ArrayM] = lambda params, data: np.array([0.0])\n\n    self._log_density_hess: _types.Callable[[dict, _types.Any], _types.ArrayMxM] = lambda params, data: np.array([[0.0]])\n    self._log_density_uncon_hess: _types.Callable[[dict, _types.Any], _types.ArrayMxM] = lambda params, data: np.array([[0.0]])\n\n\n    self._log_likelihood_hess: _types.Callable[[dict, _types.Any], _types.ArrayMxM] = lambda params, data: np.array([[0.0]])\n    self._log_likelihood_hess: _types.Callable[[dict, _types.Any], _types.ArrayMxM] = lambda params, data: np.array([[0.0]])\n    # ------------------\n\n    # Attributes\n    self.prior_ranges: dict[str, list[float, float]] = {} | self._default_prior_ranges\n    \"\"\"Keyed dict like {key: [max,min] } of bounds for parameter uniform priors\"\"\"\n    self.prior_volume = 1.0\n    \"\"\"Volume of the prior, i.e. prod(max_i-min_i) for in in params\"\"\"\n    self.name = type(self).__name__\n    \"\"\"Name of the model for print strings\"\"\"\n\n    # Update with args\n    self.set_priors(self._default_prior_ranges | prior_ranges) if prior_ranges is not None else self.set_priors(\n        self._default_prior_ranges)\n\n    self._prep_funcs()\n</code></pre>"},{"location":"models/#litmus.models.stats_model.prior_ranges","title":"<code>prior_ranges: dict[str, list[float, float]] = {} | self._default_prior_ranges</code>  <code>instance-attribute</code>","text":"<p>Keyed dict like {key: [max,min] } of bounds for parameter uniform priors</p>"},{"location":"models/#litmus.models.stats_model.prior_volume","title":"<code>prior_volume = 1.0</code>  <code>instance-attribute</code>","text":"<p>Volume of the prior, i.e. prod(max_i-min_i) for in in params</p>"},{"location":"models/#litmus.models.stats_model.name","title":"<code>name = type(self).__name__</code>  <code>instance-attribute</code>","text":"<p>Name of the model for print strings</p>"},{"location":"models/#litmus.models.stats_model.set_priors","title":"<code>set_priors(prior_ranges: dict) -&gt; None</code>","text":"<p>Sets the stats model prior ranges for uniform priors. Does some sanity checking to avoid negative priors.</p> <p>Parameters:</p> Name Type Description Default <code>prior_ranges</code> <code>dict</code> <p>keyed dict of {key: [minval, maxval]} for ranges or {key: fixedval} for fixed values</p> required Source code in <code>litmus/models.py</code> <pre><code>def set_priors(self, prior_ranges: dict) -&gt; None:\n    \"\"\"\n    Sets the stats model prior ranges for uniform priors. Does some sanity checking to avoid negative priors.\n\n    :param prior_ranges: keyed dict of {key: [minval, maxval]} for ranges or {key: fixedval} for fixed values\n    \"\"\"\n\n    badkeys = [key for key in prior_ranges.keys() if key not in self._default_prior_ranges.keys()]\n\n    for key, val in zip(prior_ranges.keys(), prior_ranges.values()):\n        if key in badkeys:\n            continue\n\n        if _utils.isiter(val):\n            a, b = val\n        else:\n            try:\n                a, b = val, val\n            except:\n                raise \"Bad input shape in set_priors for key %s\" % key  # todo - make this go to std.err\n\n        self.prior_ranges[key] = [float(a), float(b)]\n\n    # Calc and set prior volume\n    # Todo - Make this more general. Revisit if we separate likelihood + prior\n    prior_volume = 1.0\n    for key in self.prior_ranges:\n        a, b = self.prior_ranges[key]\n        if b != a:\n            prior_volume *= b - a\n    self.prior_volume = prior_volume\n\n    self._prep_funcs()\n\n    return\n</code></pre>"},{"location":"models/#litmus.models.stats_model.prior","title":"<code>prior() -&gt; [float]</code>","text":"<p>A NumPyro callable prior.</p> <p>Returns:</p> Type Description <code>[float]</code> <p>Values of the parameters as sampled from the prior</p> Source code in <code>litmus/models.py</code> <pre><code>def prior(self) -&gt; [float, ]:\n    \"\"\"\n    A NumPyro callable prior.\n    :returns: Values of the parameters as sampled from the prior\n    \"\"\"\n    lag = numpyro.sample('lag', dist.Uniform(self.prior_ranges['lag'][0], self.prior_ranges['lag'][1]))\n    return lag\n</code></pre>"},{"location":"models/#litmus.models.stats_model.model_function","title":"<code>model_function(data)</code>","text":"<p>A NumPyro callable function. Does not return</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>Data to condition the model on</p> required Source code in <code>litmus/models.py</code> <pre><code>def model_function(self, data):\n    \"\"\"\n    A NumPyro callable function. Does not return\n    :param data: Data to condition the model on\n    \"\"\"\n    lag = self.prior()\n</code></pre>"},{"location":"models/#litmus.models.stats_model.lc_to_data","title":"<code>lc_to_data(lc_1: lightcurve, lc_2: lightcurve) -&gt; dict</code>","text":"<p>Converts light-curves into the data format required for the model. For most models this will return as some sort of sorted dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>lc_1</code> <code>lightcurve</code> <p>First lightcurve object</p> required <code>lc_2</code> <code>lightcurve</code> <p>Second lightcurve object</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Varies from model to model, by default will be a keyed dict: {'T': Time values of observations series, 'Y': Signal strength values of observations series, 'E': Uncertainty values of values in Y, 'bands': int array identifying which lightcurve (0,1) that the observations belong to }</p> Source code in <code>litmus/models.py</code> <pre><code>def lc_to_data(self, lc_1: lightcurve, lc_2: lightcurve) -&gt; dict:\n    \"\"\"\n    Converts light-curves into the data format required for the model. For most models this will return as some sort\n    of sorted dictionary.\n    :param lc_1: First lightcurve object\n    :param lc_2: Second lightcurve object\n    :return: Varies from model to model, by default will be a keyed dict:\n        {'T': Time values of observations series,\n         'Y': Signal strength values of observations series,\n         'E': Uncertainty values of values in Y,\n         'bands': int array identifying which lightcurve (0,1) that the observations belong to\n        }\n    \"\"\"\n\n    T = jnp.array([*lc_1.T, *lc_2.T])\n    Y = jnp.array([*lc_1.Y, *lc_2.Y])\n    E = jnp.array([*lc_1.E, *lc_2.E])\n    bands = jnp.array([*np.zeros(lc_1.N), *np.ones(lc_2.N)]).astype(int)\n\n    I = T.argsort()\n\n    T, Y, E, bands = T[I], Y[I], E[I], bands[I]\n\n    data = {'T': T,\n            'Y': Y,\n            'E': E,\n            'bands': bands\n            }\n\n    return data\n</code></pre>"},{"location":"models/#litmus.models.stats_model.to_uncon","title":"<code>to_uncon(params) -&gt; dict[str, float]</code>","text":"<p>Converts model parametes from \"real\" constrained domain values into HMC friendly unconstrained values.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <p>keyed dict of parameters in constrained domain</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>keyed dict of parameters in unconstrained domain</p> Source code in <code>litmus/models.py</code> <pre><code>def to_uncon(self, params) -&gt; dict[str, float]:\n    \"\"\"\n    Converts model parametes from \"real\" constrained domain values into HMC friendly unconstrained values.\n\n    :param params: keyed dict of parameters in constrained domain\n    :return: keyed dict of parameters in unconstrained domain\n    \"\"\"\n    out = numpyro.infer.util.unconstrain_fn(self.prior, params=params, model_args=(), model_kwargs={})\n    return out\n</code></pre>"},{"location":"models/#litmus.models.stats_model.to_con","title":"<code>to_con(params) -&gt; dict[str, float]</code>","text":"<p>Converts model parametes back into \"real\" constrained domain values.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <p>keyed dict of parameters in unconstrained domain</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>keyed dict of parameters in constrained domain</p> Source code in <code>litmus/models.py</code> <pre><code>def to_con(self, params) -&gt; dict[str, float]:\n    \"\"\"\n    Converts model parametes back into \"real\" constrained domain values.\n    :param params: keyed dict of parameters in unconstrained domain\n    :return: keyed dict of parameters in constrained domain\n    \"\"\"\n    out = numpyro.infer.util.constrain_fn(self.prior, params=params, model_args=(), model_kwargs={})\n    return out\n</code></pre>"},{"location":"models/#litmus.models.stats_model.uncon_grad","title":"<code>uncon_grad(params) -&gt; float</code>","text":"<p>Evaluates the log of det(Jac) by evaluating pi(x) and pi'(x'). Used for correcting integral elements between constrained and unconstrained space</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <p>Model parameters in constrained domain</p> required <p>Returns:</p> Type Description <code>float</code> <p>float of det(Jacobian)</p> Source code in <code>litmus/models.py</code> <pre><code>def uncon_grad(self, params) -&gt; float:\n    \"\"\"\n    Evaluates the log of det(Jac) by evaluating pi(x) and pi'(x').\n    Used for correcting integral elements between constrained and unconstrained space\n\n    :param params: Model parameters in constrained domain\n    :return: float of det(Jacobian)\n    \"\"\"\n    con_dens = numpyro.infer.util.log_density(self.prior, (), {}, params)[0]\n\n    up = self.to_uncon(params)\n    uncon_dens = -numpyro.infer.util.potential_energy(self.prior, (), {}, up)\n    out = con_dens - uncon_dens\n    return out\n</code></pre>"},{"location":"models/#litmus.models.stats_model.uncon_grad_lag","title":"<code>uncon_grad_lag(params) -&gt; float</code>","text":"<p>Returns the log-jacobian correction for the constrained / unconstrained correction for the lag parameter Assumes a uniform distribution for the lag prior</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <p>Model parameters in constrained domain</p> required <p>Returns:</p> Type Description <code>float</code> <p>float of det(Jacobian) for lag_uncon &lt;-&gt; lag_con</p> Source code in <code>litmus/models.py</code> <pre><code>def uncon_grad_lag(self, params) -&gt; float:\n    \"\"\"\n    Returns the log-jacobian correction for the constrained / unconstrained correction for the lag parameter\n    Assumes a uniform distribution for the lag prior\n\n    :param params: Model parameters in constrained domain\n    :return: float of det(Jacobian) for lag_uncon &lt;-&gt; lag_con\n    \"\"\"\n\n    from numpyro.infer.util import transform_fn\n\n    if 'lag' not in self.paramnames(): return 0\n    if np.ptp(self.prior_ranges['lag']) == 0: return 0\n\n    lagdist = dist.Uniform(*self.prior_ranges['lag'])\n    lag_con = params['lag']\n\n    transforms = {\"lag\": numpyro.distributions.biject_to(lagdist.support)}\n\n    def tform(x):\n        out = transform_fn(transforms, params | {'lag': x}, invert=True)['lag']\n        return out\n\n    tform = jax.grad(tform)\n    out = np.log(abs(tform(lag_con)))\n    return out\n</code></pre>"},{"location":"models/#litmus.models.stats_model.paramnames","title":"<code>paramnames() -&gt; [str]</code>","text":"<p>Returns the names of all model parameters. Purely for brevity of code.</p> <p>Returns:</p> Type Description <code>[str]</code> <p>list of param names in order listed in prior_ranges</p> Source code in <code>litmus/models.py</code> <pre><code>def paramnames(self) -&gt; [str]:\n    \"\"\"\n    Returns the names of all model parameters. Purely for brevity of code.\n\n    :return: list of param names in order listed in prior_ranges\n    \"\"\"\n    return list(self.prior_ranges.keys())\n</code></pre>"},{"location":"models/#litmus.models.stats_model.fixed_params","title":"<code>fixed_params() -&gt; [str]</code>","text":"<p>Returns the names of all fixed model parameters. Purely for brevity.</p> <p>Returns:</p> Type Description <code>[str]</code> <p>list of param names in order listed in prior_ranges</p> Source code in <code>litmus/models.py</code> <pre><code>def fixed_params(self) -&gt; [str]:\n    \"\"\"\n    Returns the names of all fixed model parameters. Purely for brevity.\n\n    :return: list of param names in order listed in prior_ranges\n    \"\"\"\n    is_fixed = {key: np.ptp(self.prior_ranges[key]) == 0 for key in self.prior_ranges.keys()}\n    out = [key for key in is_fixed.keys() if is_fixed[key]]\n    return out\n</code></pre>"},{"location":"models/#litmus.models.stats_model.free_params","title":"<code>free_params() -&gt; [str]</code>","text":"<p>Returns the names of all free model parameters. Purely for brevity of code.</p> <p>Returns:</p> Type Description <code>[str]</code> <p>list of param names in order listed in prior_ranges</p> Source code in <code>litmus/models.py</code> <pre><code>def free_params(self) -&gt; [str]:\n    \"\"\"\n    Returns the names of all free model parameters. Purely for brevity of code.\n\n    :return: list of param names in order listed in prior_ranges\n    \"\"\"\n    is_fixed = {key: np.ptp(self.prior_ranges[key]) == 0 for key in self.prior_ranges.keys()}\n    out = [key for key in is_fixed.keys() if not is_fixed[key]]\n    return out\n</code></pre>"},{"location":"models/#litmus.models.stats_model.dim","title":"<code>dim() -&gt; int</code>","text":"<p>Quick and easy call for the number of model parameters.</p> <p>Returns:</p> Type Description <code>int</code> <p>number of model parameters as int</p> Source code in <code>litmus/models.py</code> <pre><code>def dim(self) -&gt; int:\n    \"\"\"\n    Quick and easy call for the number of model parameters.\n\n    :return: number of model parameters as int\n    \"\"\"\n    return len(self.free_params())\n</code></pre>"},{"location":"models/#litmus.models.stats_model.log_density","title":"<code>log_density(params, data, use_vmap=False) -&gt; _types.ArrayN</code>","text":"<p>Returns the log density of the joint distribution at some constrained space position 'params' and conditioned on some 'data'. data must match the output of the model's lc_to_data(), and params is either a keyed dict of parameter values or a key dict of arrays of values.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <p>model params in constrained space as keyed dict</p> required <code>data</code> <p>data to condition the model on</p> required <code>use_vmap</code> <p>whether to use vmap instead of unconstrained space (not implemented)</p> <code>False</code> <p>Returns:</p> Type Description <code>ArrayN</code> <p>Returns as array of floats</p> Source code in <code>litmus/models.py</code> <pre><code>def log_density(self, params, data, use_vmap=False) -&gt; _types.ArrayN:\n    \"\"\"\n    Returns the log density of the joint distribution at some constrained space position 'params' and conditioned\n    on some 'data'. data must match the output of the model's lc_to_data(), and params is either a keyed dict of\n    parameter values or a key dict of arrays of values.\n\n    :param params: model params in constrained space as keyed dict\n    :param data: data to condition the model on\n    :param use_vmap: whether to use vmap instead of unconstrained space (not implemented)\n\n    :return: Returns as array of floats\n\n    \"\"\"\n\n    if _utils.isiter_dict(params):\n        N = _utils.dict_dim(params)[1]\n        out = np.zeros(N)\n        for i in range(N):\n            p = {key: params[key][i] for key in params.keys()}\n            out[i] = self._log_density_jit(p, data)\n    else:\n        out = np.array([self._log_density_jit(params, data)])\n\n    return out\n</code></pre>"},{"location":"models/#litmus.models.stats_model.log_likelihood","title":"<code>log_likelihood(params, data, use_vmap=False) -&gt; _types.ArrayN</code>","text":"<p>Returns the log likelihood at some constrained space position 'params' and conditioned on some 'data'. data must match the output of the model's lc_to_data(), and params is either a keyed dict of parameter values or a key dict of arrays of values.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <p>model params in constrained space as keyed dict</p> required <code>data</code> <p>data to condition the model on</p> required <code>use_vmap</code> <p>whether to use vmap instead of unconstrained space (not implemented)</p> <code>False</code> <p>Returns:</p> Type Description <code>ArrayN</code> <p>Returns as array of floats</p> Source code in <code>litmus/models.py</code> <pre><code>def log_likelihood(self, params, data, use_vmap=False) -&gt; _types.ArrayN:\n    \"\"\"\n    Returns the log likelihood at some constrained space position 'params' and conditioned\n    on some 'data'. data must match the output of the model's lc_to_data(), and params is either a keyed dict of\n    parameter values or a key dict of arrays of values.\n\n    :param params: model params in constrained space as keyed dict\n    :param data: data to condition the model on\n    :param use_vmap: whether to use vmap instead of unconstrained space (not implemented)\n\n    :return: Returns as array of floats\n    \"\"\"\n    if _utils.isiter_dict(params):\n        N = _utils.dict_dim(params)[1]\n        out = np.zeros(N)\n        for i in range(N):\n            p = {key: params[key][i] for key in params.keys()}\n            out[i] = self._log_likelihood(p, data)\n    else:\n        out = self._log_likelihood(params, data)\n\n    return out\n</code></pre>"},{"location":"models/#litmus.models.stats_model.log_density_uncon","title":"<code>log_density_uncon(params, data, use_vmap=False) -&gt; _types.ArrayN</code>","text":"<p>Returns the log density of the joint distribution at some unconstrained space position 'params' and conditioned on some 'data'. data must match the output of the model's lc_to_data(), and params is either a keyed dict of parameter values or a key dict of arrays of values.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <p>model params in constrained space as keyed dict</p> required <code>data</code> <p>data to condition the model on</p> required <code>use_vmap</code> <p>whether to use vmap instead of unconstrained space (not implemented)</p> <code>False</code> <p>Returns:</p> Type Description <code>ArrayN</code> <p>Returns as array of floats</p> Source code in <code>litmus/models.py</code> <pre><code>def log_density_uncon(self, params, data, use_vmap=False) -&gt; _types.ArrayN:\n    \"\"\"\n    Returns the log density of the joint distribution at some unconstrained space position 'params' and conditioned\n    on some 'data'. data must match the output of the model's lc_to_data(), and params is either a keyed dict of\n    parameter values or a key dict of arrays of values.\n\n    :param params: model params in constrained space as keyed dict\n    :param data: data to condition the model on\n    :param use_vmap: whether to use vmap instead of unconstrained space (not implemented)\n\n    :return: Returns as array of floats\n    \"\"\"\n\n    if _utils.isiter_dict(params):\n        N = _utils.dict_dim(params)[1]\n        out = np.zeros(N)\n        for i in range(N):\n            p = {key: params[key][i] for key in params.keys()}\n            out[i] = self._log_density_uncon_jit(p, data)\n    else:\n        out = self._log_density_uncon_jit(params, data)\n\n    return out\n</code></pre>"},{"location":"models/#litmus.models.stats_model.log_prior","title":"<code>log_prior(params, data=None, use_vmap=False) -&gt; _types.ArrayN</code>","text":"<p>Returns the log density of the prior  at some constrained space position 'params' Params is either a keyed dict of parameter values or a key dict of arrays of values. Returns as array of floats use_vmap currently not implemented with no side effect</p> Source code in <code>litmus/models.py</code> <pre><code>def log_prior(self, params, data=None, use_vmap=False) -&gt; _types.ArrayN:\n    \"\"\"\n    Returns the log density of the prior  at some constrained space position 'params'\n    Params is either a keyed dict of parameter values or a key dict of arrays of values.\n    Returns as array of floats\n    use_vmap currently not implemented with no side effect\n    \"\"\"\n\n    if _utils.isiter_dict(params):\n        N = _utils.dict_dim(params)[1]\n        out = np.zeros(N)\n        for i in range(N):\n            p = {key: params[key][i] for key in params.keys()}\n            out[i] = self._log_prior_jit(p)\n    else:\n        out = self._log_prior_jit(params)\n\n    return out\n</code></pre>"},{"location":"models/#litmus.models.stats_model.log_density_grad","title":"<code>log_density_grad(params, data, use_vmap=False, keys=None) -&gt; dict[str, float]</code>","text":"<p>Returns the gradient of the log density of the joint distribution at some constrained space position 'params', conditionded on some 'data' matching the format of the model's lc_to_data() output.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <p>model params in constrained space as keyed dict</p> required <code>data</code> <p>data to condition the model on</p> required <code>use_vmap</code> <p>whether to use vmap instead of unconstrained space (not implemented)</p> <code>False</code> <code>keys</code> <p>list of keys / parameters to take the gradient over</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Returns as keyed dict of grads along each axsi or keyed dict of array of similar values</p> Source code in <code>litmus/models.py</code> <pre><code>def log_density_grad(self, params, data, use_vmap=False, keys=None) -&gt; dict[str, float]:\n    \"\"\"\n    Returns the gradient of the log density of the joint distribution at some constrained space position 'params',\n    conditionded on some 'data' matching the format of the model's lc_to_data() output.\n\n    :param params: model params in constrained space as keyed dict\n    :param data: data to condition the model on\n    :param use_vmap: whether to use vmap instead of unconstrained space (not implemented)\n    :param keys: list of keys / parameters to take the gradient over\n\n    :return: Returns as keyed dict of grads along each axsi or keyed dict of array of similar values\n\n    \"\"\"\n\n    if _utils.isiter_dict(params):\n        m, N = _utils.dict_dim(params)\n        out = {key: np.zeros([N]) for key in params.keys()}\n        for i in range(N):\n            p = {key: params[key][i] for key in params.keys()}\n            grads = self._log_density_grad(p, data)\n            for key in params.keys():\n                out[key][i] = grads[key]\n    else:\n        out = self._log_density_grad(params, data)\n\n    return out\n</code></pre>"},{"location":"models/#litmus.models.stats_model.log_density_uncon_grad","title":"<code>log_density_uncon_grad(params, data, use_vmap=False, keys=None, asdict=False) -&gt; float</code>","text":"<p>Returns the gradient of the log density of the joint distribution at some unconstrained space position 'params', conditionded on some 'data' matching the format of the model's lc_to_data() output.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <p>model params in constrained space as keyed dict</p> required <code>data</code> <p>data to condition the model on</p> required <code>use_vmap</code> <p>whether to use vmap instead of unconstrained space (not implemented)</p> <code>False</code> <code>keys</code> <p>list of keys / parameters to take the gradient over</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>Returns as keyed dict of grads along each axsi or keyed dict of array of similar values</p> Source code in <code>litmus/models.py</code> <pre><code>def log_density_uncon_grad(self, params, data, use_vmap=False, keys=None, asdict=False) -&gt; float:\n    \"\"\"\n    Returns the gradient of the log density of the joint distribution at some unconstrained space position 'params',\n    conditionded on some 'data' matching the format of the model's lc_to_data() output.\n\n    :param params: model params in constrained space as keyed dict\n    :param data: data to condition the model on\n    :param use_vmap: whether to use vmap instead of unconstrained space (not implemented)\n    :param keys: list of keys / parameters to take the gradient over\n\n    :return: Returns as keyed dict of grads along each axsi or keyed dict of array of similar values\n    \"\"\"\n\n    if _utils.isiter_dict(params):\n        m, N = _utils.dict_dim(params)\n        out = {key: np.zeros([N]) for key in params.keys()}\n        for i in range(N):\n            p = {key: params[key][i] for key in params.keys()}\n            grads = self._log_density_uncon_grad(p, data)\n            for key in params.keys():\n                out[key][i] = grads[key]\n    else:\n        out = self._log_density_uncon_grad(params, data)\n\n    return out\n</code></pre>"},{"location":"models/#litmus.models.stats_model.log_prior_grad","title":"<code>log_prior_grad(params, data=None, use_vmap=False, keys=None) -&gt; dict[str, float]</code>","text":"<p>Returns the gradient of the log prior of the prior at some constrained space position 'params' Params is either a keyed dict of parameter values or a key dict of arrays of values.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <p>model params in constrained space as keyed dict</p> required <code>data</code> <p>data to condition the model on</p> <code>None</code> <code>use_vmap</code> <p>whether to use vmap instead of unconstrained space (not implemented)</p> <code>False</code> <code>keys</code> <p>list of keys / parameters to take the gradient over</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Returns as keyed dict of grads along each axsi or keyed dict of array of similar values</p> Source code in <code>litmus/models.py</code> <pre><code>def log_prior_grad(self, params, data=None, use_vmap=False, keys=None) -&gt; dict[str, float]:\n    \"\"\"\n    Returns the gradient of the log prior of the prior at some constrained space position 'params'\n    Params is either a keyed dict of parameter values or a key dict of arrays of values.\n\n    :param params: model params in constrained space as keyed dict\n    :param data: data to condition the model on\n    :param use_vmap: whether to use vmap instead of unconstrained space (not implemented)\n    :param keys: list of keys / parameters to take the gradient over\n\n    :return: Returns as keyed dict of grads along each axsi or keyed dict of array of similar values\n    \"\"\"\n\n    if _utils.isiter(params):\n        m, N = _utils.dict_dim(params)\n        out = np.zeros(N)\n        for i in range(N):\n            p = {key: params[key][i] for key in params.keys()}\n            out[i, :] = self._log_prior_grad(p)\n    else:\n        out = self._log_prior_grad(params)\n\n    return out\n</code></pre>"},{"location":"models/#litmus.models.stats_model.log_density_hess","title":"<code>log_density_hess(params, data, use_vmap=False, keys=None) -&gt; _types.ArrayNxMxM</code>","text":"<p>Returns the hessian matrix of the log joint distribution at some constrained space position 'params', conditioned on some 'data' matching the output of the model's lc_to_data() output.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <p>model params in constrained space as keyed dict</p> required <code>data</code> <p>data to condition the model on</p> required <code>use_vmap</code> <p>whether to use vmap instead of unconstrained space (not implemented)</p> <code>False</code> <code>keys</code> <p>The params to slice and sort the hessian matrices.</p> <code>None</code> <p>Returns:</p> Type Description <code>ArrayNxMxM</code> <p>Returns in order / dimension: [num param sites, num keys, num keys]</p> Source code in <code>litmus/models.py</code> <pre><code>def log_density_hess(self, params, data, use_vmap=False, keys=None) -&gt; _types.ArrayNxMxM:\n    \"\"\"\n    Returns the hessian matrix of the log joint distribution at some constrained space position 'params',\n    conditioned on some 'data' matching the output of the model's lc_to_data() output.\n\n    :param params: model params in constrained space as keyed dict\n    :param data: data to condition the model on\n    :param use_vmap: whether to use vmap instead of unconstrained space (not implemented)\n    :param keys: The params to slice and sort the hessian matrices.\n\n    :return: Returns in order / dimension: [num param sites, num keys, num keys]\n    \"\"\"\n\n    if keys is None: keys = params.keys()\n\n    if _utils.isiter_dict(params):\n        m, N = _utils.dict_dim(params)\n        m = len(keys)\n        out = np.zeros([N, m, m])\n        for i in range(N):\n            p = {key: params[key][i] for key in keys}\n            hess_eval = self._log_density_hess(p, data)\n            for j, key1 in enumerate(keys):\n                for k, key2 in enumerate(keys):\n                    out[i, j, k] = hess_eval[key1][key2]\n    else:\n        m = len(keys)\n        out = np.zeros([m, m])\n        hess_eval = self._log_density_hess(params, data)\n        for j, key1 in enumerate(keys):\n            for k, key2 in enumerate(keys):\n                out[j, k] = hess_eval[key1][key2]\n\n    return out\n</code></pre>"},{"location":"models/#litmus.models.stats_model.log_density_uncon_hess","title":"<code>log_density_uncon_hess(params, data, use_vmap=False, keys=None) -&gt; _types.ArrayNxMxM</code>","text":"<p>Returns the hessian matrix of the log joint distribution at some unconstrained space position 'params', conditioned on some 'data' matching the output of the model's lc_to_data() output.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <p>model params in constrained space as keyed dict</p> required <code>data</code> <p>data to condition the model on</p> required <code>use_vmap</code> <p>whether to use vmap instead of unconstrained space (not implemented)</p> <code>False</code> <code>keys</code> <p>The params to slice and sort the hessian matrices.</p> <code>None</code> <p>Returns:</p> Type Description <code>ArrayNxMxM</code> <p>Returns in order / dimension: [num param sites, num keys, num keys]</p> Source code in <code>litmus/models.py</code> <pre><code>def log_density_uncon_hess(self, params, data, use_vmap=False, keys=None) -&gt; _types.ArrayNxMxM:\n    \"\"\"\n    Returns the hessian matrix of the log joint distribution at some unconstrained space position 'params',\n    conditioned on some 'data' matching the output of the model's lc_to_data() output.\n\n    :param params: model params in constrained space as keyed dict\n    :param data: data to condition the model on\n    :param use_vmap: whether to use vmap instead of unconstrained space (not implemented)\n    :param keys: The params to slice and sort the hessian matrices.\n\n    :return: Returns in order / dimension: [num param sites, num keys, num keys]\n    \"\"\"\n\n    if keys is None: keys = params.keys()\n\n    if _utils.isiter_dict(params):\n        m, N = _utils.dict_dim(params)\n        m = len(keys)\n        out = np.zeros([N, m, m])\n        for i in range(N):\n            p = {key: params[key][i] for key in keys}\n            hess_eval = self._log_density_uncon_hess(p, data)\n            for j, key1 in enumerate(keys):\n                for k, key2 in enumerate(keys):\n                    out[i, j, k] = hess_eval[key1][key2]\n    else:\n        m = len(keys)\n        out = np.zeros([m, m])\n        hess_eval = self._log_density_uncon_hess(params, data)\n        for j, key1 in enumerate(keys):\n            for k, key2 in enumerate(keys):\n                out[j, k] = hess_eval[key1][key2]\n\n    return out\n</code></pre>"},{"location":"models/#litmus.models.stats_model.log_prior_hess","title":"<code>log_prior_hess(params, data=None, use_vmap=False, keys=None) -&gt; _types.ArrayNxMxM</code>","text":"<p>Returns the hessian matrix of the log prior of the prior at some constrained space position 'params' Params is either a keyed dict of parameter values or a key dict of arrays of values.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <p>model params in constrained space as keyed dict</p> required <code>data</code> <p>data to condition the model on</p> <code>None</code> <code>use_vmap</code> <p>whether to use vmap instead of unconstrained space (not implemented)</p> <code>False</code> <code>keys</code> <p>The params to slice and sort the hessian matrices.</p> <code>None</code> <p>Returns:</p> Type Description <code>ArrayNxMxM</code> <p>Returns in order / dimension: [num param sites, num keys, num keys]</p> Source code in <code>litmus/models.py</code> <pre><code>def log_prior_hess(self, params, data=None, use_vmap=False, keys=None) -&gt; _types.ArrayNxMxM:\n    \"\"\"\n    Returns the hessian matrix of the log prior of the prior at some constrained space position 'params'\n    Params is either a keyed dict of parameter values or a key dict of arrays of values.\n\n    :param params: model params in constrained space as keyed dict\n    :param data: data to condition the model on\n    :param use_vmap: whether to use vmap instead of unconstrained space (not implemented)\n    :param keys: The params to slice and sort the hessian matrices.\n\n    :return: Returns in order / dimension: [num param sites, num keys, num keys]\n    \"\"\"\n\n    if keys is None: keys = params.keys()\n\n    if _utils.isiter_dict(params):\n        m, N = _utils.dict_dim(params)\n        m = len(keys)\n        out = np.zeros([N, m, m])\n        for i in range(N):\n            p = {key: params[key][i] for key in keys}\n            hess_eval = self._log_prior_hess(p)\n            for j, key1 in enumerate(keys):\n                for k, key2 in enumerate(keys):\n                    out[i, j, k] = hess_eval[key1][key2]\n    else:\n        m = len(keys)\n        out = np.zeros([m, m])\n        hess_eval = self._log_prior_hess(params)\n        for j, key1 in enumerate(keys):\n            for k, key2 in enumerate(keys):\n                out[j, k] = hess_eval[key1][key2]\n\n    return out\n</code></pre>"},{"location":"models/#litmus.models.stats_model.scan","title":"<code>scan(start_params, data, optim_params=None, use_vmap=False, optim_kwargs={}, precondition='diag') -&gt; dict[str, float]</code>","text":"<p>Beginning at position 'start_params', optimize parameters in 'optim_params' to find maximum. optim_kwargs will overwrite defaults and be passed directly to jaxopt.BFGS object</p> <p>Currently using jaxopt with optim_kwargs:     'stepsize': 0.0,     'min_stepsize': 1E-5,     'increase_factor': 1.2,     'maxiter': 1024,     'linesearch': 'backtracking',     'verbose': False,</p> <p>Parameters:</p> Name Type Description Default <code>start_params</code> <p>Position in constrained parameter spac to begin the search at</p> required <code>data</code> <p>data to condition the model on</p> required <code>optim_params</code> <p>parameters to optimize over</p> <code>None</code> <code>use_vmap</code> <p>Whether to use jax vmapping over multiple start_params (not implemented)</p> <code>False</code> <code>optim_kwargs</code> <p>kwargs for the jaxopt.BFGS optimiser</p> <code>{}</code> <code>precondition</code> <p>Type of preconditioning to use in optimisation. Should be \"cholesky\", \"eig\", \"half-eig\", \"diag\" or \"none\"</p> <code>'diag'</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> Source code in <code>litmus/models.py</code> <pre><code>def scan(self, start_params, data, optim_params=None, use_vmap=False, optim_kwargs={}, precondition='diag') -&gt; dict[\n    str, float]:\n    \"\"\"\n    Beginning at position 'start_params', optimize parameters in 'optim_params' to find maximum.\n    optim_kwargs will overwrite defaults and be passed directly to jaxopt.BFGS object\n\n    Currently using jaxopt with optim_kwargs:\n        'stepsize': 0.0,\n        'min_stepsize': 1E-5,\n        'increase_factor': 1.2,\n        'maxiter': 1024,\n        'linesearch': 'backtracking',\n        'verbose': False,\n\n    :param start_params: Position in constrained parameter spac to begin the search at\n    :param data: data to condition the model on\n    :param optim_params: parameters to optimize over\n    :param use_vmap: Whether to use jax vmapping over multiple start_params (not implemented)\n    :param optim_kwargs: kwargs for the jaxopt.BFGS optimiser\n    :param precondition: Type of preconditioning to use in optimisation. Should be \"cholesky\", \"eig\", \"half-eig\", \"diag\" or \"none\"\n\n    :returns:\n    \"\"\"\n\n    optimizer_args = {\n        'stepsize': 0.0,\n        'min_stepsize': 1E-5,\n        'increase_factor': 1.2,\n        'maxiter': 256,\n        'linesearch': 'backtracking',\n        'verbose': False,\n    }\n\n    optimizer_args |= optim_kwargs\n\n    # Convert to unconstrained domain\n    start_params_uncon = self.to_uncon(start_params)\n\n    if optim_params is None:\n        optim_params = [name for name in self.paramnames() if\n                        self.prior_ranges[name][0] != self.prior_ranges[name][1]\n                        ]\n    if len(optim_params) == 0: return start_params\n\n    # Get all split into fixed and free params\n    x0, y0 = _utils.dict_split(start_params_uncon, optim_params)\n    x0 = _utils.dict_pack(x0)\n\n    # -------------------------------------\n    # Build preconditioning matrix\n    H = self.log_density_uncon_hess(start_params_uncon, data, keys=optim_params)\n    H *= -1\n    if precondition == \"cholesky\":\n        H = np.linalg.cholesky(np.linalg.inv(H))\n        Hinv = np.linalg.inv(H)\n\n    elif precondition == \"eig\":\n        D, P = np.linalg.eig(H)\n        if D.min() &lt; 0:\n            D[np.where(D &lt; 0)[0].min()] = 1.0\n        D, P = D.astype(float), P.astype(float)\n        D **= -0.5\n\n        H = np.dot(P, np.dot(np.diag(D), P.T))\n        Hinv = np.dot(P, np.dot(np.diag(D ** -1), P.T))\n\n    elif precondition == \"half-eig\":\n        D, P = np.linalg.eig(H)\n        if D.min() &lt; 0:\n            D[np.where(D &lt; 0)[0].min()] = 1.0\n        D, P = D.astype(float), P.astype(float)\n        D **= -0.5\n\n        H = np.dot(P, np.diag(D))\n        Hinv = np.dot(np.diag(D ** -1), P.T)\n\n    elif precondition == \"diag\":\n        D = np.diag(H) ** -0.5\n        D = np.where(D &gt; 0, D, 1.0)\n        H = np.diag(D)\n        Hinv = np.diag(1 / D)\n\n    else:\n        H = np.eye(len(optim_params))\n        Hinv = np.eye(len(optim_params))\n\n    self.msg_debug(\"Scaling matrix:\", H)\n    self.msg_debug(\"Inverse Scaling matrix:\", Hinv)\n\n    def optfunc(X):\n        Y = jnp.dot(H, X) + x0\n        params = y0 | {key: Y[i] for i, key in enumerate(optim_params)}\n        out = - self._log_density_uncon(params, data)\n        return out\n\n    X0 = np.zeros_like(x0)\n\n    self.msg_debug(\"At initial uncon position\", X0, \"with keys\", optim_params, \"eval for optfunc is\",\n                   optfunc(X0))\n\n    assert not np.isinf(optfunc(X0)), \"Something wrong with start positions in scan!\"\n\n    # =====================\n    # Jaxopt Work\n\n    # Build the optimizer\n    solver = jaxopt.BFGS(fun=optfunc,\n                         value_and_grad=False,\n                         jit=True,\n                         **optimizer_args\n                         )\n\n    # Debug safety check to see if something's breaking\n\n    if self.debug:\n        self.msg_debug(\"Creating and testing solver...\")\n        try:\n            init_state = solver.init_state(X0)\n            with _utils.suppress_stdout():  # TODO - Supressing of warnings, should be patched in newest jaxopt\n                solver.update(params=X0, state=init_state)\n            self.msg_debug(\"Jaxopt solver created and running fine\")\n        except:\n            self.msg_debug(\"Something went wrong in when making the jaxopt optimizer. Double check your inputs.\")\n\n    with _utils.suppress_stdout():  # TODO - Supressing of warnings, should be patched in newest jaxopt\n        sol, state = solver.run(init_params=X0)\n\n    out = np.dot(H, sol) + x0\n\n    # =====================\n    # Cleanup and return\n    self.msg_debug(\"At final uncon position\", out, \"with keys\", optim_params, \"eval for optfunc is\",\n                   optfunc(sol), \"a change of\", optfunc(sol) - optfunc(np.zeros_like(sol))\n                   )\n    assert optfunc(sol) &lt; optfunc(np.zeros_like(sol)), \"Optimization has diverged. Try changing the start params or widen your priors\"\n\n    # Unpack the results to a dict\n    out = {key: out[i] for i, key in enumerate(optim_params)}\n    out = out | y0  # Adjoin the fixed values\n\n    # Convert back to constrained domain\n    out = self.to_con(out)\n\n    out = {key: float(val) for key, val in zip(out.keys(), out.values())}\n\n    return out\n</code></pre>"},{"location":"models/#litmus.models.stats_model.laplace_log_evidence","title":"<code>laplace_log_evidence(params, data, integrate_axes=None, use_vmap=False, constrained=False) -&gt; float</code>","text":"<p>At some point 'params' in parameter space, gets the hessian in unconstrained space and uses to estimate the model evidence</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <p>Keyed dict with params in constrained / unconstrained parameter space</p> required <code>data</code> <p>data to condition the model on</p> required <code>integrate_axes</code> <p>Which axes to perform laplace approx for. If none, use all</p> <code>None</code> <code>use_vmap</code> <p>Whether to use jax vmapping over different params. (Not implemented)</p> <code>False</code> <code>constrained</code> <p>If true, perform laplace approx in constrained domain. Default to false</p> <code>False</code> <p>Returns:</p> Type Description <code>float</code> <p>laplace log evidence as float or array of floats</p> Source code in <code>litmus/models.py</code> <pre><code>def laplace_log_evidence(self, params, data, integrate_axes=None, use_vmap=False, constrained=False) -&gt; float:\n    \"\"\"\n    At some point 'params' in parameter space, gets the hessian in unconstrained space and uses to estimate the\n    model evidence\n    :param params: Keyed dict with params in constrained / unconstrained parameter space\n    :param data: data to condition the model on\n    :param integrate_axes: Which axes to perform laplace approx for. If none, use all\n    :param use_vmap: Whether to use jax vmapping over different params. (Not implemented)\n    :param constrained: If true, perform laplace approx in constrained domain. Default to false\n    :return: laplace log evidence as float or array of floats\n    \"\"\"\n\n    self.msg_debug(\"-------------\")\n    self.msg_debug(\"Laplace Evidence eval\")\n\n    self.msg_debug(\"Constrained params are:\", params)\n\n    if integrate_axes is None:\n        integrate_axes = self.paramnames()\n\n    # Get 'height' and curvature of Gaussian\n    if not constrained:\n        uncon_params = self.to_uncon(params)\n\n        self.msg_debug(\"Un-Constrained params are:\", uncon_params)\n\n        log_height = self.log_density_uncon(uncon_params, data)\n        hess = self.log_density_uncon_hess(uncon_params, data, keys=integrate_axes)\n    else:\n        log_height = self.log_density(params, data)\n        hess = self.log_density_hess(params, data, keys=integrate_axes)\n\n    dethess = np.linalg.det(-hess)\n\n    self.msg_debug(\"With determinant:\", dethess)\n\n    self.msg_debug(\"And log height: %.2f...\" % log_height)\n\n    D = len(integrate_axes)\n    out = np.log(2 * np.pi) * (D / 2) - np.log(dethess) / 2 + log_height\n\n    self.msg_debug(\"log-evidence is ~%.2f\" % out)\n    return out\n</code></pre>"},{"location":"models/#litmus.models.stats_model.laplace_log_info","title":"<code>laplace_log_info(params, data, integrate_axes=None, use_vmap=False, constrained=False)</code>","text":"<p>At some point 'params' in parameter space, gets the hessian in unconstrained space and uses to estimate the model information relative to the prior</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <p>site(s) to evaluate info at in the constrained domain</p> required <code>data</code> <p>data to condition the model on</p> required <code>integrate_axes</code> <p>free axes to perform laplace integral over. If none, use all</p> <code>None</code> <code>use_vmap</code> <p>Whether to use jax vmapping over different params. (Not implemented)</p> <code>False</code> <code>constrained</code> <p>If true perform laplace approx in the constrained domain</p> <code>False</code> <p>Returns:</p> Type Description <p>Laplace log info evaluated at params</p> Source code in <code>litmus/models.py</code> <pre><code>def laplace_log_info(self, params, data, integrate_axes=None, use_vmap=False, constrained=False):\n    \"\"\"\n    At some point 'params' in parameter space, gets the hessian in unconstrained space and uses to estimate the\n    model information relative to the prior\n    :param params: site(s) to evaluate info at in the constrained domain\n    :param data: data to condition the model on\n    :param integrate_axes: free axes to perform laplace integral over. If none, use all\n    :param use_vmap: Whether to use jax vmapping over different params. (Not implemented)\n    :param constrained: If true perform laplace approx in the constrained domain\n    :return: Laplace log info evaluated at params\n    \"\"\"\n\n    if integrate_axes is None:\n        integrate_axes = self.paramnames()\n\n    if not constrained:\n        uncon_params = self.to_uncon(params)\n\n        log_height = self.log_density_uncon(uncon_params, data)\n        hess = self.log_density_uncon_hess(uncon_params, data)\n    else:\n        log_height = self.log_density(params, data)\n        hess = self.log_density_hess(params, data)\n\n    I = np.where([key in integrate_axes for key in self.paramnames()])[0]\n\n    hess = hess[I, I]\n    if len(I) &gt; 1:\n        dethess = np.linalg.det(hess)\n    else:\n        dethess = hess\n\n    # todo - double check sign on the log term. Might be wrong\n    # todo - add case check for non-uniform priors.\n    D = len(integrate_axes)\n    out = -(np.log(2 * np.pi) + 1) * (D / 2) - np.log(-dethess) / 2 + np.log(self.prior_volume)\n    return out\n</code></pre>"},{"location":"models/#litmus.models.stats_model.opt_tol","title":"<code>opt_tol(params, data, integrate_axes=None, use_vmap=False, constrained=False)</code>","text":"Source code in <code>litmus/models.py</code> <pre><code>def opt_tol(self, params, data, integrate_axes=None, use_vmap=False, constrained=False):\n    if integrate_axes is None:\n        integrate_axes = self.paramnames()\n\n    # Get hessians and grads\n    if not constrained:\n        uncon_params = self.to_uncon(params)\n\n        hess = self.log_density_uncon_hess(uncon_params, data, keys=integrate_axes)\n        grad = self.log_density_uncon_grad(uncon_params, data, keys=integrate_axes)\n    else:\n        hess = self.log_density_hess(params, data, keys=integrate_axes)\n        grad = self.log_density_grad(params, data, keys=integrate_axes)\n\n    # todo - remove this when properly integrating keys argument into grad funcs\n    I = np.where([key in integrate_axes for key in grad.keys()])[0]\n    grad = np.array([float(x) for x in grad.values()])[I]\n    grad, hess = -grad, -hess\n\n    # ------------------------------------------------\n    # Calculate tolerances\n    if np.linalg.det(hess) &lt;= 0 or np.isnan(hess).any():\n        return np.inf\n\n    try:\n        Hinv = np.linalg.inv(hess)\n        loss = np.dot(grad,\n                      np.dot(\n                          Hinv, grad\n                      )\n                      )\n        return np.sqrt(abs(loss))\n\n    except:\n        return np.inf\n</code></pre>"},{"location":"models/#litmus.models.stats_model.prior_sample","title":"<code>prior_sample(num_samples: int = 1, seed: int = None) -&gt; dict</code>","text":"<p>Blind sampling from the prior without conditioning. Returns model parameters only</p> <p>Parameters:</p> Name Type Description Default <code>num_samples</code> <code>int</code> <p>Number of realizations to generate</p> <code>1</code> <code>seed</code> <code>int</code> <p>seed for random generation</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>keyed dict of parameters drawn from prior</p> Source code in <code>litmus/models.py</code> <pre><code>def prior_sample(self, num_samples: int = 1, seed: int = None) -&gt; dict:\n    \"\"\"\n    Blind sampling from the prior without conditioning. Returns model parameters only\n    :param num_samples: Number of realizations to generate\n    :param seed: seed for random generation\n    :return: keyed dict of parameters drawn from prior\n    \"\"\"\n\n    if seed == None: seed = _utils.randint()\n\n    pred = numpyro.infer.Predictive(self.prior,\n                                    num_samples=num_samples,\n                                    return_sites=self.paramnames()\n                                    )\n\n    params = pred(rng_key=jax.random.PRNGKey(seed))\n\n    if num_samples == 1:\n        params = {key: params[key][0] for key in params.keys()}\n    return params\n</code></pre>"},{"location":"models/#litmus.models.stats_model.realization","title":"<code>realization(data=None, num_samples: int = 1, seed: int = None)</code>","text":"<p>Generates realizations of the observables by blindly sampling from the prior</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>data to condition the lightcurve on</p> <code>None</code> <code>num_samples</code> <code>int</code> <p>Number of realizations to generate</p> <code>1</code> <code>seed</code> <code>int</code> <p>seed for random generation</p> <code>None</code> <p>Returns:</p> Type Description <p>keyed dict of the model observables</p> Source code in <code>litmus/models.py</code> <pre><code>def realization(self, data=None, num_samples: int = 1, seed: int = None):\n    \"\"\"\n    Generates realizations of the observables by blindly sampling from the prior\n    :param data: data to condition the lightcurve on\n    :param num_samples: Number of realizations to generate\n    :param seed: seed for random generation\n    :return: keyed dict of the model observables\n    \"\"\"\n    if seed == None: seed = _utils.randint()\n\n    pred = numpyro.infer.Predictive(self.model_function,\n                                    num_samples=num_samples,\n                                    return_sites=None\n                                    )\n\n    params = pred(rng_key=jax.random.PRNGKey(seed), data=data)\n    return params\n</code></pre>"},{"location":"models/#litmus.models.stats_model.make_lightcurves","title":"<code>make_lightcurves(data, params: dict, Tpred, num_samples: int = 1) -&gt; (lightcurve, lightcurve)</code>","text":"<p>Returns lightcurves at time 'T' for 'parameters' conditioned on 'data' over <code>num_samples</code> draws from `params'</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>Data to condition the model on</p> required <code>params</code> <code>dict</code> <p>keyed dictionary of parameters</p> required <code>Tpred</code> <p>Array of time values to predict the lightcurve at</p> required <code>num_samples</code> <code>int</code> <p>number of samples to draw from params use in integration to get covar / mu</p> <code>1</code> <p>Returns:</p> Type Description <code>(lightcurve, lightcurve)</code> <p>tuple of mean and covariance of LC's (loc_1, loc_2, covar_1, covar_2)</p> Source code in <code>litmus/models.py</code> <pre><code>def make_lightcurves(self, data, params: dict, Tpred, num_samples: int = 1) -&gt; (lightcurve, lightcurve):\n    \"\"\"\n    Returns lightcurves at time 'T' for 'parameters' conditioned on 'data' over `num_samples` draws from `params'\n\n    :param data: Data to condition the model on\n    :param params: keyed dictionary of parameters\n    :param Tpred: Array of time values to predict the lightcurve at\n    :param num_samples: number of samples to draw from params use in integration to get covar / mu\n    :return: tuple of mean and covariance of LC's (loc_1, loc_2, covar_1, covar_2)\n    \"\"\"\n\n    len_params = _utils.dict_dim(params)[1]\n    if num_samples &gt; len_params:\n        self.msg_err(\"Warning! Tried to call %i samples from only %i parameters in make_lightcurves\" % (\n            num_samples, len_params))\n\n    loc_1 = np.zeros_like(Tpred)\n    covar_1 = np.zeros([len(Tpred), len(Tpred)])\n    loc_2, covar_2 = loc_1.copy(), covar_1.copy()\n\n    if self._gen_lightcurve is stats_model._gen_lightcurve:\n        self.msg_err(\"Warning, called make_lightcurves on a stats_model that doesn't have implementation\")\n\n    if not _utils.isiter_dict(params):\n        loc_1, loc_2, covar_1, covar_2 = self.gen_lightcurve(data, params, jnp.array(Tpred))\n\n    else:\n        I = np.random.choice(range(len_params), num_samples, replace=True)\n        loc_1_all = np.tile(loc_1, (num_samples, 1)) * 0\n        loc_2_all = loc_1_all.copy()\n\n        for k, p_sample in enumerate([_utils.dict_divide(params)[i] for i in I]):\n            loc_1_i, loc_2_i, covar_1_i, covar_2_i = self.gen_lightcurve(data, p_sample, jnp.array(Tpred))\n            covar_1 += covar_1_i\n            covar_2 += covar_2_i\n            loc_1_all[k, :] = loc_1_i\n            loc_2_all[k, :] = loc_2_i\n        loc_1 = np.mean(loc_1_all, axis=0)\n        loc_2 = np.mean(loc_2_all, axis=0)\n        covar_1 = covar_1 / num_samples + np.diag(np.var(loc_1_all, axis=0))\n        covar_2 = covar_2 / num_samples + np.diag(np.var(loc_2_all, axis=0))\n\n    err_1, err_2 = np.diag(covar_1) ** 0.5, np.diag(covar_2) ** 0.5\n\n    outs = (lightcurve(Tpred, loc_1, err_1), lightcurve(Tpred, loc_2, err_2))\n\n    return outs\n</code></pre>"},{"location":"models/#litmus.models.stats_model.params_inprior","title":"<code>params_inprior(params) -&gt; bool</code>","text":"<p>Utility to check if model params fall within the uniform prior bounds</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <p>constrained space params to check validity of</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if site falls within prior boundaries, false if not</p> Source code in <code>litmus/models.py</code> <pre><code>def params_inprior(self, params) -&gt; bool:\n    \"\"\"\n    Utility to check if model params fall within the uniform prior bounds\n    :param params: constrained space params to check validity of\n    :return: True if site falls within prior boundaries, false if not\n    \"\"\"\n\n    isgood = {key: True for key in params.keys()}\n    for key in params.keys():\n        if key in self.fixed_params():\n            if np.any(params[key] != self.prior_ranges[key][0]):\n                isgood[key] = False\n            else:\n                isgood[key] = True\n        else:\n            if np.any(\n                    not ((params[key] &gt;= self.prior_ranges[key][0]) and (params[key] &lt; self.prior_ranges[key][1]))):\n                isgood[key] = False\n            else:\n                isgood[key] = True\n    return isgood\n</code></pre>"},{"location":"models/#litmus.models.stats_model.find_seed","title":"<code>find_seed(data, guesses=None, fixed={}) -&gt; (dict, float)</code>","text":"<p>Find a good initial seed. Unless otherwise over-written, while blindly sample the prior and return the best fit.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>data to condition the model on</p> required <code>guesses</code> <p>number of evals to use for finding the seed</p> <code>None</code> <code>fixed</code> <p>keyed dict of parameters to be fixed instead of estimating a seed</p> <code>{}</code> <p>Returns:</p> Type Description <code>(dict, float)</code> <p>tuple of dict of seed params and log density at this position</p> Source code in <code>litmus/models.py</code> <pre><code>def find_seed(self, data, guesses=None, fixed={}) -&gt; (dict, float):\n    \"\"\"\n    Find a good initial seed. Unless otherwise over-written, while blindly sample the prior and return the best fit.\n    :param data: data to condition the model on\n    :param guesses: number of evals to use for finding the seed\n    :param fixed: keyed dict of parameters to be fixed instead of estimating a seed\n    :return: tuple of dict of seed params and log density at this position\n    \"\"\"\n\n    if len(fixed.keys()) == len(self.paramnames()): return fixed, self.log_density(fixed, data)\n\n    if guesses == None: guesses = 50 * 2 ** len(self.free_params())\n\n    samples = self.prior_sample(num_samples=guesses)\n\n    if fixed != {}: samples = _utils.dict_extend(samples | fixed)\n\n    ll = self.log_density(samples, data)\n    i = ll.argmax()\n\n    out = _utils.dict_divide(samples)[i]\n    return out, ll.max()\n</code></pre>"},{"location":"models/#litmus.models.GP_simple","title":"<code>GP_simple(prior_ranges=None, **kwargs)</code>","text":"<p>               Bases: <code>stats_model</code></p> <p>An example of how to construct your own stats_model in the simplest form. Requirements are to:     1. Set a default prior range for all parameters used in model_function     2. Define a numpyro generative model model_function You can add / adjust methods as required, but these are the only main steps</p> Source code in <code>litmus/models.py</code> <pre><code>def __init__(self, prior_ranges=None, **kwargs):\n    self._default_prior_ranges = {\n        'lag': _default_config['lag'],\n        'logtau': _default_config['logtau'],\n        'logamp': _default_config['logamp'],\n        'rel_amp': _default_config['rel_amp'],\n        'mean': _default_config['mean'],\n        'rel_mean': _default_config['rel_mean'],\n    }\n    self._protected_keys = ['basekernel']\n    super().__init__(prior_ranges=prior_ranges, **kwargs)\n\n    self.basekernel: tinygp.kernels.quasisep = kwargs[\n        'basekernel'] if 'basekernel' in kwargs.keys() else tinygp.kernels.quasisep.Exp\n    \"\"\"The gaussian kernel\"\"\"\n</code></pre>"},{"location":"models/#litmus.models.GP_simple.basekernel","title":"<code>basekernel: tinygp.kernels.quasisep = kwargs['basekernel'] if 'basekernel' in kwargs.keys() else tinygp.kernels.quasisep.Exp</code>  <code>instance-attribute</code>","text":"<p>The gaussian kernel</p>"},{"location":"models/#litmus.models.GP_simple.prior","title":"<code>prior() -&gt; list[float, float, float, float, float, float]</code>","text":"Source code in <code>litmus/models.py</code> <pre><code>def prior(self) -&gt; list[float, float, float, float, float, float]:\n    # Sample distributions\n    lag = quickprior(self, 'lag')\n\n    logtau = quickprior(self, 'logtau')\n    logamp = quickprior(self, 'logamp')\n\n    rel_amp = quickprior(self, 'rel_amp')\n    mean = quickprior(self, 'mean')\n    rel_mean = quickprior(self, 'rel_mean')\n\n    return lag, logtau, logamp, rel_amp, mean, rel_mean\n</code></pre>"},{"location":"models/#litmus.models.GP_simple.model_function","title":"<code>model_function(data) -&gt; None</code>","text":"Source code in <code>litmus/models.py</code> <pre><code>def model_function(self, data) -&gt; None:\n    lag, logtau, logamp, rel_amp, mean, rel_mean = self.prior()\n\n    T, Y, E, bands = [data[key] for key in ['T', 'Y', 'E', 'bands']]\n\n    # Conversions to gp-friendly form\n    amp, tau = jnp.exp(logamp), jnp.exp(logtau)\n\n    diag = jnp.square(E)\n\n    delays = jnp.array([0, lag])\n    amps = jnp.array([amp, rel_amp * amp])\n    means = jnp.array([mean, mean + rel_mean])\n\n    T_delayed = T - delays[bands]\n    I = T_delayed.argsort()\n\n    # Build and sample GP\n\n    gp = gpw.build_gp(T_delayed[I], Y[I], diag[I], bands[I], tau, amps, means, basekernel=self.basekernel)\n    numpyro.sample(\"Y\", gp.numpyro_dist(), obs=Y[I])\n</code></pre>"},{"location":"models/#litmus.models.GP_simple.find_seed","title":"<code>find_seed(data, guesses=None, fixed={}) -&gt; (float, dict[str, float])</code>","text":"Source code in <code>litmus/models.py</code> <pre><code>def find_seed(self, data, guesses=None, fixed={}) -&gt; (float, dict[str, float]):\n\n    # -------------------------\n    # Setup\n    T, Y, E, bands = [data[key] for key in ['T', 'Y', 'E', 'bands']]\n\n    T1, Y1, E1 = T[bands == 0], Y[bands == 0], E[bands == 0]\n    T2, Y2, E2 = T[bands == 1], Y[bands == 1], E[bands == 1]\n\n    # If not specified, use roughly 4 per epoch of main lightcurve\n    if guesses is None: guesses = int(np.array(self.prior_ranges['lag']).ptp() / np.median(np.diff(T1))) * 4\n\n    check_fixed = self.params_inprior(fixed)\n    if False in check_fixed:\n        self.msg_err(\"Warning! Tried to fix seed params at values that lie outside of prior range:\")\n        for [key, val] in check_fixed.items():\n            if val == False: self.msg_err('\\t%s' % key)\n        self.msg_err(\"This may be overwritten\")\n\n    # -------------------------\n    # Estimate Correlation Timescale\n    if 'logtau' not in fixed.keys():\n        from litmus.ICCF_working import correlfunc_jax_vmapped\n        approx_season = np.diff(T1).max()\n\n        if approx_season &gt; np.median(np.diff(T1)) * 5:\n            span = approx_season\n        else:\n            span = np.ptp(T1) * 0.1\n\n        autolags = jnp.linspace(-span, span, 1024)\n        autocorrel = correlfunc_jax_vmapped(autolags, T1, Y1, T1, Y1, 1024)\n\n        autolags, autocorrel = np.array(autolags), np.array(autocorrel)\n\n        # Trim to positive values and take a linear regression\n        autolags = autolags[autocorrel &gt; 0]\n        autocorrel = autocorrel[autocorrel &gt; 0]\n        autocorrel = np.log(autocorrel)\n        autocorrel[autolags &lt; 0] *= -1\n\n        autolags -= autolags.mean(),\n        autocorrel -= autocorrel.mean()\n\n        tau = (autolags * autolags).sum() / (autolags * autocorrel).sum()\n        tau = abs(tau)\n        # tau *= np.exp(1)\n    else:\n        tau = 1\n\n    # -------------------------\n    # Estimate mean &amp; variances\n    Y1bar, Y2bar = np.average(Y1, weights=E1 ** -2), np.average(Y2, weights=E2 ** -2)\n    Y1var, Y2var = np.average((Y1 - Y1bar) ** 2, weights=E1 ** -2), np.average((Y2 - Y2bar) ** 2, weights=E2 ** -2)\n\n    # -------------------------\n\n    out = {\n        'lag': 0.0,\n        'logtau': np.log(tau),\n        'logamp': np.log(Y1var) / 2,\n        'rel_amp': np.sqrt(Y2var / Y1var),\n        'mean': Y1bar,\n        'rel_mean': Y2bar - Y1bar,\n    }\n\n    out |= fixed\n\n    # -------------------------\n    # For any extended models, randomly sample\n    extended_keys = [key for key in self.free_params() if key not in out.keys()]\n    if len(extended_keys) != 0:\n        self.msg_debug(\"In GP_simple.find_seed(), using blind sampeling to find guesses for \", *extended_keys)\n        extended_samps = _utils.dict_extend(self.prior_sample(guesses) | out)\n        extended_lls = self.log_density(extended_samps, data)\n        out = _utils.dict_divide(extended_samps)[np.argmax(extended_keys)]\n\n    # -------------------------\n    # Where estimates are outside prior range, round down\n    isgood = self.params_inprior(out)\n    r = 0.01\n    isgood['lag'] = True\n    for key in out.keys():\n        if not isgood[key]:\n            if out[key] &lt; self.prior_ranges[key][0]:\n                out[key] = self.prior_ranges[key][0] + r * np.ptp(self.prior_ranges[key])\n            else:\n                out[key] = self.prior_ranges[key][1] - r * np.ptp(self.prior_ranges[key])\n\n    # -------------------------\n    # Estimate lag with a sweep if not in fixed\n    if 'lag' not in fixed.keys():\n        lag_fits = np.linspace(*self.prior_ranges['lag'], guesses, endpoint=False)\n        out |= {'lag': lag_fits}\n    else:\n        out |= {'lag': fixed['lag']}\n\n    # -------------------------\n    # Get log likelihoods and return best value\n    out = _utils.dict_extend(out)\n\n    lls = self.log_density(params=out, data=data)\n    if _utils.dict_dim(out)[1] &gt; 1:\n        i = lls.argmax()\n        ll_out = lls[i]\n        out = {key: out[key][i] for key in out.keys()}\n        self.msg_debug(\"In find seed, sample no %i is best /w LL %.2f at lag %.2f\" % (i, ll_out, out['lag']))\n    else:\n        ll_out = float(lls)\n\n    return out, ll_out\n</code></pre>"},{"location":"models/#litmus.models.GP_simple_null","title":"<code>GP_simple_null(prior_ranges=None, **kwargs)</code>","text":"<p>               Bases: <code>GP_simple</code></p> <p>A variant of GP_simple for uncoupled gaussian processes, equivalent to lag-&gt;infty in GP_simple. Used for null hypothesis testing through model comparison.</p> Source code in <code>litmus/models.py</code> <pre><code>def __init__(self, prior_ranges=None, **kwargs):\n    self._default_prior_ranges = {\n        'lag': [0.0, 0.0]\n    }\n    super().__init__()\n</code></pre>"},{"location":"models/#litmus.models.GP_simple_null.lc_to_data","title":"<code>lc_to_data(lc_1: lightcurve, lc_2: lightcurve) -&gt; dict</code>","text":"Source code in <code>litmus/models.py</code> <pre><code>def lc_to_data(self, lc_1: lightcurve, lc_2: lightcurve) -&gt; dict:\n    return super().lc_to_data(lc_1, lc_2) | {\n        'T1': lc_1.T,\n        'Y1': lc_1.Y,\n        'E1': lc_1.E,\n        'T2': lc_2.T,\n        'Y2': lc_2.Y,\n        'E2': lc_2.E,\n    }\n</code></pre>"},{"location":"models/#litmus.models.GP_simple_null.model_function","title":"<code>model_function(data) -&gt; None</code>","text":"Source code in <code>litmus/models.py</code> <pre><code>def model_function(self, data) -&gt; None:\n    lag, logtau, logamp, rel_amp, mean, rel_mean = self.prior()\n\n    T1, Y1, E1 = [data[key] for key in ['T1', 'Y1', 'E1']]\n    T2, Y2, E2 = [data[key] for key in ['T2', 'Y2', 'E2']]\n\n    # Conversions to gp-friendly form\n    amp, tau = jnp.exp(logamp), jnp.exp(logtau)\n\n    diag1, diag2 = jnp.square(E1), jnp.square(E2)\n\n    # amps = jnp.array([amp, rel_amp * amp])\n    # means = jnp.array([mean, mean + rel_mean])\n    Y1 -= mean\n    Y2 -= (mean + rel_mean)\n\n    # Build and sample GP\n    kernel1 = tinygp.kernels.quasisep.Exp(scale=tau, sigma=amp)\n    kernel2 = tinygp.kernels.quasisep.Exp(scale=tau, sigma=amp * rel_amp)\n\n    gp1 = GaussianProcess(kernel1, T1, diag=diag1)\n    gp2 = GaussianProcess(kernel2, T2, diag=diag2)\n    numpyro.sample(\"Y1\", gp1.numpyro_dist(), obs=Y1)\n    numpyro.sample(\"Y2\", gp2.numpyro_dist(), obs=Y2)\n</code></pre>"},{"location":"models/#litmus.models.whitenoise_null","title":"<code>whitenoise_null(prior_ranges=None, **kwargs)</code>","text":"<p>               Bases: <code>GP_simple_null</code></p> Source code in <code>litmus/models.py</code> <pre><code>def __init__(self, prior_ranges=None, **kwargs):\n    self._default_prior_ranges = {\n        'lag': [0.0, 0.0]\n    }\n    super().__init__()\n</code></pre>"},{"location":"models/#litmus.models.whitenoise_null.model_function","title":"<code>model_function(data) -&gt; None</code>","text":"Source code in <code>litmus/models.py</code> <pre><code>def model_function(self, data) -&gt; None:\n    lag, logtau, logamp, rel_amp, mean, rel_mean = self.prior()\n\n    T1, Y1, E1 = [data[key] for key in ['T1', 'Y1', 'E1']]\n    T2, Y2, E2 = [data[key] for key in ['T2', 'Y2', 'E2']]\n\n    # Conversions to gp-friendly form\n    amp, tau = jnp.exp(logamp), jnp.exp(logtau)\n\n    diag1, diag2 = jnp.square(E1), jnp.square(E2)\n\n    # amps = jnp.array([amp, rel_amp * amp])\n    # means = jnp.array([mean, mean + rel_mean])\n    Y1 -= mean\n\n    # Build and sample GP\n    kernel1 = tinygp.kernels.quasisep.Exp(scale=tau, sigma=amp)\n\n    gp1 = GaussianProcess(kernel1, T1, diag=diag1)\n    numpyro.sample(\"Y1\", gp1.numpyro_dist(), obs=Y1)\n\n    with numpyro.plate(\"whitenoise\", len(Y2)):\n        numpyro.sample('Y2',\n                       dist.Normal(mean + rel_mean, jnp.sqrt((amp * rel_amp) ** 2 + E2 ** 2)),\n                       obs=Y2)\n</code></pre>"},{"location":"models/#litmus.models.GP_simple_normalprior","title":"<code>GP_simple_normalprior(prior_ranges=None, **kwargs)</code>","text":"<p>               Bases: <code>GP_simple</code></p> Source code in <code>litmus/models.py</code> <pre><code>def __init__(self, prior_ranges=None, **kwargs):\n    self._default_prior_ranges = {\n        'lag': [1.0, 1000.0]\n    }\n    self._protected_keys = ['mu_lagpred', 'sig_lagpred']\n    super().__init__()\n\n    # Default values for hbeta AGN @ ~44 dex lum, from R-L relations\n    self.mu_lagpred = 1.44 * np.log(10)  # ~28 days, from McDougall et al 2025a\n    self.sig_lagpred = np.log(10) * 0.24  # ~1.75 dex, from McDougall et al 2025a\n</code></pre>"},{"location":"models/#litmus.models.GP_simple_normalprior.mu_lagpred","title":"<code>mu_lagpred = 1.44 * np.log(10)</code>  <code>instance-attribute</code>","text":""},{"location":"models/#litmus.models.GP_simple_normalprior.sig_lagpred","title":"<code>sig_lagpred = np.log(10) * 0.24</code>  <code>instance-attribute</code>","text":""},{"location":"models/#litmus.models.GP_simple_normalprior.prior","title":"<code>prior() -&gt; (float, float, float, float, float, float)</code>","text":"Source code in <code>litmus/models.py</code> <pre><code>def prior(self) -&gt; (float, float, float, float, float, float):\n\n    tform = numpyro.distributions.transforms.ExpTransform()\n    tformed_dist = numpyro.distributions.TransformedDistribution(\n        dist.TruncatedNormal(self.mu_lagpred, self.sig_lagpred, low=jnp.log(self.prior_ranges['lag'][0]),\n                             high=jnp.log(self.prior_ranges['lag'][1])), [tform, ]\n    )\n    lag = numpyro.sample('lag', tformed_dist)\n    masked_model = numpyro.handlers.substitute(super().prior, {'lag': lag})\n    with numpyro.handlers.block(hide=['lag']):\n        _, logtau, logamp, rel_amp, mean, rel_mean = masked_model()\n    return lag, logtau, logamp, rel_amp, mean, rel_mean\n</code></pre>"},{"location":"models/#litmus.models.GP_simple_normalprior.uncon_grad_lag","title":"<code>uncon_grad_lag(params) -&gt; float</code>","text":"Source code in <code>litmus/models.py</code> <pre><code>def uncon_grad_lag(self, params) -&gt; float:\n\n    from numpyro.infer.util import transform_fn\n\n    if 'lag' not in self.paramnames(): return 0\n    if np.ptp(self.prior_ranges['lag']) == 0: return 0\n\n    tform = numpyro.distributions.transforms.ExpTransform()\n    lagdist = numpyro.distributions.TransformedDistribution(\n        dist.TruncatedNormal(self.mu_lagpred, self.sig_lagpred, low=jnp.log(self.prior_ranges['lag'][0]),\n                             high=jnp.log(self.prior_ranges['lag'][1])), [tform, ]\n    )\n    lag_con = params['lag']\n\n    transforms = {\"lag\": numpyro.distributions.biject_to(lagdist.support)}\n\n    def tform(x):\n        out = transform_fn(transforms, params | {'lag': x}, invert=True)['lag']\n        return out\n\n    tform = jax.grad(tform)\n    out = np.log(abs(tform(lag_con)))\n    return out\n</code></pre>"},{"location":"models/#litmus.models.quickprior","title":"<code>quickprior(targ, key)</code>","text":"Source code in <code>litmus/models.py</code> <pre><code>def quickprior(targ, key):\n    p = targ.prior_ranges[key]\n    distrib = dist.Uniform(float(p[0]), float(p[1])) if p[0] != p[1] else dist.Delta(float(p[0]))\n    out = numpyro.sample(key, distrib)\n    return out\n</code></pre>"},{"location":"examples/Custom_statsmodel_example/Custom_statsmodel_example/","title":"Custom <code>stats_model</code> Example","text":"<p>Using <code>LITMUS</code> but finding that the existing stats models just aren't what you're after: Good news, <code>LITMUS</code>'s modular design means you can write your own stats model or extend an existing one as easily as you like. In this example we'll show an example of adding PyRoa-style uncertainty calibration from an imaginary survey with two photometric telescopes. In this example we work in the format of a notebook, but in actuality you'd likely write your extension modules in a <code>.py</code> file</p> <pre><code>from litmus.models import _default_config, stats_model, quickprior\nimport litmus.gp_working as gpw\nfrom litmus import *\nfrom litmus._utils import dict_extend\n\nimport numpyro\nimport numpyro.distributions as dist\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport jax.numpy as jnp\n</code></pre>"},{"location":"examples/Custom_statsmodel_example/Custom_statsmodel_example/#an-imaginary-survey","title":"An Imaginary Survey","text":"<p>First up, we need to actually generate our fake data. Let's imagine a ficticious (but reasonable) scenario like this: 1. Like OzDES, we have multi-year parallel observations of OzDES spectroscopy and DES-like phometry data for the first three years 2. Oops! Something went wrong and we lost the last three years of photometry! 3. Good news! A second survey (Let's call it DESOz) happened to observe the same target for the last three years! 4. Oh no! DESOZ has some unaccounted source of error $E_\\mathrm{calib}$ that isn't accounted for in its measurement uncertainties!</p> <p>This example may be a little unrealisitc, but it has the vague shape of a practical application and so we'll proceed anyway. First up, let's simulate the imaginary data, per usual equipped with all the true underlying values.</p> <p>First, use the <code>mocks</code> module to get some OzDES/DES-like data:</p> <pre><code>seed = 0\nmock_underlying = litmus.mocks.mock(seed=seed, lag=300, tau=500, maxtime=360 * 6, E=[0.01, 0.1])\nphot_lc_1, spec_lc = mock_underlying.lc_1, mock_underlying.lc_2\n</code></pre> <p>Now, we'll trim the photometry lightcurve, <code>phot_lc_1</code>, to the first three years and generate a second mock with a slightly higher measurement uncertainty, which we'll then trim to the second three years. We'll then bring the \"recorded\" uncertainty back down. This means that the <code>phot_lc_2</code> now has its measurement uncertainty under-estimated, which we can see by how the corresponding points scatter about the true underlying lightcurve:</p> <pre><code>phot_lc_1 = phot_lc_1.trimmed_copy(0.0, 360 * 3)\n\nE_calib_true = 0.2\nmock_2 = litmus.mocks.mock(seed=seed, lag=300, tau=500, maxtime=360 * 6,\n                           E=[np.sqrt(0.01 ** 2 + E_calib_true ** 2), 0.1])\nphot_lc_2 = mock_2.lc_1.trimmed_copy(360 * 3, 360 * 6)\nphot_lc_2.E = np.sqrt(phot_lc_2.E ** 2 - E_calib_true ** 2)\n\nphot_lc = phot_lc_1.concatenate(phot_lc_2)\n\n#-----------\nfig, ax = plt.subplots(figsize=(8, 5))\nphot_lc_1.plot(axis=ax, c='tab:blue', capsize=2, show=False, label=\"DES Photometry\")\nphot_lc_2.plot(axis=ax, c='tab:orange', capsize=2, show=False, label=\"DESOz Photometry\")\nspec_lc.plot(axis=ax, c='tab:purple', capsize=2, show=False, label=\"Spectroscopy (300 day lag)\")\nplt.plot(mock_underlying.lc.T, mock_underlying.lc.Y, alpha=0.25, lw=2, c='k', label=\"True Photometry\")\nplt.legend()\nplt.ylabel(\"Signal Strength (Arb)\")\nplt.xlabel(\"Time (Days)\")\nplt.show()\n</code></pre> <p></p>"},{"location":"examples/Custom_statsmodel_example/Custom_statsmodel_example/#the-stats","title":"The Stats","text":"<p>Before we code a statistical model, we need to know what that model looks like exactly. In this case, it's a relatively simple change to the normal <code>GP_simple</code> model, but with one addition: we need <code>E_calib</code> to be some unknown factor near unity by which we correct the DESOz measurement errors. As a Bayesian model we need some sort of prior, and given we now that <code>E_calib</code> is close to zero an exponential distribution makes sense</p> <p>\\begin{equation} \\pi(E_\\mathrm{calib}) \\propto \\exp(lambda\\timesE_\\mathrm{calib}) \\end{equation}</p> <p>Then we'd up-scale our uncertainties by this amount, remembering that we add errors in quadrature instead of linearly: \\begin{equation} E_i = \\sqrt{E_i^0 + E_\\mathrm{calib}}  \\;\\; \\forall \\;\\; i \\in {i_\\mathrm{DESOz}} \\end{equation} In NumPyro this looks something like:</p> <pre><code>E_calib = numpyro.sample('E_calib',numpyro.distributions.Exponential(0, lam))\nE = jnp.sqrt(E**2 + jnp.where(survey==\"DESOz\", E_calib, 0.0)**2)\n\n...Typical GP sampling stuf...\n</code></pre> <p>Most of <code>LITMUS</code>'s models by default use uniform distributions, or at least distributions with hard boundaries, so we're playing a bit fast and loose with convention here. Still, for an ad-hoc example we can be a little shotgun with convention.</p> <p>We need know we need to keep track of which survey generated each measurement in each lightcurve, so let's staple those onto the lightcurve objects now:</p> <pre><code>phot_lc.survey = [*[\"DES\"] * sum(phot_lc.T &lt; 360 * 3), *[\"DESOz\"] * sum(phot_lc.T &gt;= 360 * 3)]\nspec_lc.survey = [\"OzDES\"] * spec_lc.N\n</code></pre>"},{"location":"examples/Custom_statsmodel_example/Custom_statsmodel_example/#building-the-stats_model-model","title":"Building the <code>stats_model</code> Model","text":"<p>The minimum amount of things you need to build a <code>stats_model</code> in litmus is: 1. A way of turning lightcurves into data (<code>.lc_to_data()</code>), 2. A prior (<code>.prior()</code>), with default ranges (<code>._default_prior_ranges</code>), and 3. A model likelihood function (<code>.model_function(data)</code>)</p> <p>Our model, lets call it <code>GP_simple_Ecalib</code> is an extension to the GP_simple model, so it inherits most of the bells and whistles out of the box. We only really need to specify the bits that change. Of the functions here, most are copy-pasted from the original <code>GP_simple</code> source-code with only a few extra lines added, but you can see how the moving parts work.</p> <pre><code>class GP_simple_Ecalib(litmus.models.GP_simple):\n    def __init__(self, prior_ranges=None, **kwargs):\n        self._default_prior_ranges = {\n            'E_calib': [0, 1.0],\n        }\n        super().__init__(prior_ranges=prior_ranges, **kwargs)\n\n    # ----------------------------------\n\n    def lc_to_data(self, lc_1: lightcurve, lc_2: lightcurve) -&gt; dict:\n        T = jnp.array([*lc_1.T, *lc_2.T])\n        Y = jnp.array([*lc_1.Y, *lc_2.Y])\n        E = jnp.array([*lc_1.E, *lc_2.E])\n        bands = jnp.array([*np.zeros(lc_1.N), *np.ones(lc_2.N)]).astype(int)\n        survey = np.array([*lc_1.survey, *lc_2.survey])  # &lt;---------- New!\n        survey = jnp.where(survey == \"DESOz\", 1, 0)\n\n        I = T.argsort()\n\n        T, Y, E, bands = T[I], Y[I], E[I], bands[I]\n\n        data = {'T': T,\n                'Y': Y,\n                'E': E,\n                'bands': bands,\n                \"survey\": survey  # &lt;---------- New! \n                }\n\n        return data\n\n    def prior(self):\n        lag, logtau, logamp, rel_amp, mean, rel_mean = super().prior()\n        _, lam = self.prior_ranges['E_calib']  # &lt;---------- New!\n        E_calib = numpyro.sample('E_calib', dist.Exponential(lam))  # &lt;---------- New!\n        return lag, logtau, logamp, rel_amp, mean, rel_mean, E_calib\n\n    def model_function(self, data):\n        lag, logtau, logamp, rel_amp, mean, rel_mean, E_calib = self.prior()\n\n        T, Y, E, bands, survey = [data[key] for key in ['T', 'Y', 'E', 'bands', \"survey\"]]\n\n        E = jnp.sqrt(E ** 2 + jnp.where(survey == 1, E_calib, 1.0) ** 2)  # &lt;---- The magic line!\n\n        # Conversions to gp-friendly form\n        amp, tau = jnp.exp(logamp), jnp.exp(logtau)\n\n        diag = jnp.square(E)\n\n        delays = jnp.array([0, lag])\n        amps = jnp.array([amp, rel_amp * amp])\n        means = jnp.array([mean, mean + rel_mean])\n\n        T_delayed = T - delays[bands]\n        I = T_delayed.argsort()\n\n        # Build and sample GP\n\n        gp = gpw.build_gp(T_delayed[I], Y[I], diag[I], bands[I], tau, amps, means, basekernel=self.basekernel)\n        numpyro.sample(\"Y\", gp.numpyro_dist(), obs=Y[I])\n</code></pre> <p>Now let's create an instance of this model and test to see if it actually works. First, we'll generate a bunch of samples to make sure it understands its prior properly:</p> <pre><code>lam = 4.0\nmodel_instance = GP_simple_Ecalib(prior_ranges={\"E_calib\":[0, lam]}, verbose=False)\nprior_samples = model_instance.prior_sample(1024 * 32)\n\n#--------\nX = prior_samples['E_calib'].sort()\nplt.plot(X, lam*np.exp(-X*lam), c= 'navy', lw=2, label = \"Expected Prior\")\nplt.hist(prior_samples['E_calib'], bins=64, density=True, color = 'orchid', alpha = 0.5, label = \"Prior Samples\")\nplt.title(\"Samples Drawn from Prior Distribution of $E_\\mathrm{calib}$\")\nplt.xlabel(\"$E_\\mathrm{calib}$\"), plt.ylabel(\"Prior Density\")\nplt.legend(), plt.tight_layout()\nplt.grid()\nplt.xlim(0,1.0)\nplt.show()\n</code></pre> <p></p> <p>Let's also make sure that our statistical model has enough constraining power to actually measure $E_\\mathrm{calib}$, and confirm that <code>.model_function()</code> and <code>.lc_to_data()</code> are working properly. Because we chose a vague prior the posterior density is heavily likelihood dominated and, thankfully, near-gaussian which means we can safely use the <code>hessian_scan</code>. </p> <pre><code># Convert lightcurves to data with .lc_to_data()\ndata = model_instance.lc_to_data(phot_lc, spec_lc)\n\n# Create a bunch of test samples with all parameters except E_calib fixed at truth\nE_calib_test = np.linspace(0.0, 1, 1024)\ntest_params = dict_extend(mock_underlying.params(), {'E_calib': E_calib_test})\n\n# Calculate prior, likelihood and log density\nlog_prior = model_instance.log_prior(test_params, data)\nlog_density = model_instance.log_density(test_params, data)\nlog_likelihood = log_density - log_prior\n\n#--------------------------------\n# Plot!\nf, [a1, a2] = plt.subplots(2, 1, figsize=(8, 4), sharex=True)\na1.set_yscale('log')\n\nfor a in [a1, a2]:\n    a.plot(test_params['E_calib'], np.exp(log_prior - log_prior.max()), label=\"Log Prior\", c = 'navy')\n    a.plot(test_params['E_calib'], np.exp(log_density - log_density.max()), label=\"Log Density\", c= 'midnightblue')\n    a.plot(test_params['E_calib'], np.exp(log_likelihood - log_likelihood.max()), label=\"Log likelihood\", c = 'salmon')\n    a.axvline(E_calib_true, ls='--', label='Truth, E_calib = %.2f' % E_calib_true)\n    a.set_yticklabels([])\n    a.grid()\n    if a is a1: a.legend(loc='center right')\n\nplt.xlim(E_calib_test.min(), E_calib_test.max())\na1.set_title(\"Log Density, Conditional\")\na2.set_title(\"Density, Conditional\")\n\nf.supxlabel(\"$E_\\mathrm{calib}$\")\nf.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>Enough validation and testing. Let's see how it performs with actual fitting via the <code>hessian_scan</code>. By default, everything should perform nice and smoothly without any alteration. Let's see how we've done:</p> <pre><code>my_fitter = litmus.fitting_methods.hessian_scan(model_instance, Nlags=64, verbose=False)\nmy_fitter.fit(phot_lc, spec_lc)\n</code></pre> <p>Now slotting this into a plotter and firing off the contours, we find that we've succeeded in constraining the calibration uncertainty! Notice that $E_\\mathrm{calib}$ and $\\ln \\vert \\tau \\vert$ (<code>E_calib</code> and <code>logtau</code>) are slightly correlated. If we hadn't included this error calibration, we would have ended up with a model that: 1. Drastically under-estimated the timescale of variation to try and wiggle through the extra noise 2. Had a markedly lower model evidence, obscuring the strength of our lag recovery</p> <pre><code>lt = LITMUS(my_fitter)\nlt.plot_parameters(show=False, truth=mock_underlying.params() | {'E_calib': E_calib_true},\n                   params=[\"lag\", \"logtau\", \"E_calib\"])\nplt.show()\n</code></pre> <pre><code>Warning! LITMUS object built on pre-run fitting_procedure. May have unexpected behaviour.\n</code></pre> <p></p> <p>This is a pretty simple example of setting up a custom <code>stats_model</code>, but by and large even more complicated applications will follow these processes. Some things not covered here are: 1. Updating the <code>.find_seed()</code> function to make common sense a-priori guesses for any new parameters you introduce. By default  <code>GP_simple</code> or other models will blindly sample the prior until they get a decent hit for these 2. Updating <code>._gen_lightcurves()</code> to properly predict the underlying lightcurves. This is very similar to updating <code>model_function</code> but taking in an argument <code>Tpred</code> for time and returning a covariance matrix. See the source code for <code>GP_simple</code> and its null hypothesis variants for examples 3. If you decide to use a non-uniform prior for <code>lag</code>, you'll need to make sure it still has hard boundaries as defined in <code>prior_ranges</code> and will need to update <code>.uncon_grad_lag()</code> so that fitting methods understand how to transit between the constrained and unconstrained domains. See <code>GP_simple_normalprior</code> for an example.</p>"},{"location":"examples/JAVELIN_HeScan_Comparison/JAVELIN_HeScan_Comparison/","title":"JAVELIN Hessian Scan Comparison","text":"<p>In this example we use  To begin, we'll import all the relevant modules:</p> <pre><code>from litmus import *\nfrom litmus._utils import dict_extend\nimport jax\njax.config.update(\"jax_enable_x64\", True)\nplt.rcParams['mathtext.fontset'] = 'stix'\n</code></pre> <pre><code>An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n</code></pre> <p>Now we'll use the <code>litmus.mocks</code> module to generate some mock data:</p> <pre><code>mock = mocks.mock(lag=180, cadende=[10, 30], E=[0.15, 0.25], tau=200)\nmock = mock(10)\nlc_1, lc_2 = mock.lc_1, mock.lc_2\nmock.plot(show=True)\nplt.show()\n</code></pre> <p></p> <p>Now we create a stats model (specifically <code>models.GP_simple</code>, which models the response as an un-smoothed damped random walk), and set our prior ranges. In this case we'll numerical costs low by assuming we already know the signal mean's and relative amplitudes.</p> <pre><code>model = models.GP_simple()\nmodel.set_priors({\n    'lag': [0, 800],\n    'mean': [0.0, 0.0],\n    'rel_mean': [0.0, 0.0],\n    'rel_amp': [1.00, 1.00],\n})\n</code></pre> <p>To get an idea of why aliasing happens, let's use <code>model</code> to evaluate the log-density in a slice over the lag prior with the other parameters fixed. Notice the multimodal behaviour within the (highlighted) aliasing seasons?</p> <pre><code># Turn lightcurves into model-friendly data\ndata = model.lc_to_data(lc_1, lc_2)\n\n# Set up a span of lags and calculate the model log-density conditioned on the data\nlagplot = np.linspace(*model.prior_ranges['lag'], 1024)\nLLs = model.log_density(dict_extend(mock.params(), {'lag': lagplot}), data)\n\n# Plot likelihood and true lag\nplt.figure(figsize=(8,4))\nplt.plot(lagplot, (LLs - LLs.max()))\nplt.axvline(mock.lag, ls='--', c='k')\n\n# Highlight seasons\nif mock.season != 0:\n    tmax = model.prior_ranges['lag'][1]\n    nyears = int(tmax // (mock.season * 2) + 1)\n    for i in range(nyears):\n        plt.axvspan((i + 1 / 2 - 1 / 4) * mock.season * 2, (i + 1 - 1 / 4) * mock.season * 2,\n                    ymin=0, ymax=1, alpha=0.25, color='royalblue',\n                    zorder=-10,\n                    label=\"Aliasing Seasons\" if i == 0 else None)\n\nplt.xlim(0, model.prior_ranges['lag'][1])\nplt.xlabel(\"Lag $\\Delta t$ (days)\")\nplt.ylabel(\"Log Density $\\mathcal{L(\\Delta t \\\\vert \\phi)}\\pi(\\Delta t \\\\vert \\phi)$\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>That's all the mocks and statistics set up, now we set up two fitting method. Firstly, <code>meth_1</code> will be a <code>JAVELIKE</code>, i.e. an implementation of the AEIS, while <code>meth_2</code> will be <code>LITMUS</code>'s very own Hessian Scan. I've set the <code>debug</code> flag to <code>False</code> for each method for the sake of brevity, which otherwise would give granular detail about their inner workings, while I've kept the <code>verbose</code> flag, which outputs key information about the run progress, set to true. </p> <pre><code>meth_1 = fitting_methods.JAVELIKE(model,\n                                  verbose=False,\n                                  debug=False,\n                                  num_warmup=5_000,\n                                  num_samples=100_000 // 512,\n                                  num_chains=512\n                                  )\n\nmeth_2 = fitting_methods.hessian_scan(model,\n                                      verbose=False,\n                                      debug=False,\n                                      Nlags=64,\n                                      precondition=\"half-eig\",\n                                      reverse=False\n                                      )\n</code></pre> <p>Now we can run each method. We've already set the fitting procedures up, all that's required now is to run them! In this example I run <code>prefit()</code> and <code>fit()</code> seperately, though this is not strictly necessary.</p> <p>I then load the fitting methods into a <code>LITMUS</code> object, which is a wrapper for generating nice fancy plots and data outputs. In this case, I save the chains with <code>save_chain</code> and generate the methods' respective lag posterior distributions with <code>lag_plot</code>:</p> <pre><code>for meth, name in zip([meth_1, meth_2], [\"javelin\", 'hessian']):\n    meth.prefit(lc_1, lc_2)\n    meth.fit(lc_1, lc_2)\n\n    lt = LITMUS(meth)\n    lt.lag_plot()\n</code></pre> <pre><code>Plots!\njavelin\n\n\nTried to get 99840 sub-samples from chain of 50000 total samples. Tried to get 99840 sub-samples from chain of 50000 total samples. Warning! LITMUS object built on pre-run fitting_procedure. May have unexpected behaviour.\n\n\n\n\n\n\nWARNING:chainconsumer:Parameter lag in chain Lightcurves %i-%i is not constrained\n\n\nhessian\n\n\nWarning! LITMUS object built on pre-run fitting_procedure. May have unexpected behaviour.\n</code></pre> <p></p> <p></p> <p>There's one immediately obvious difference between the two contours: the <code>JAVELIN</code>-like <code>meth_1</code> has summoned a bimodal distribution out of nowhere, while the hessian scan of <code>meth_2</code> has not.</p>"},{"location":"examples/Model_comparison_example/Model_comparison_example/","title":"Model Comparison Example","text":"<p>One of <code>LITMUS</code>'s core features is that it can not only constrain the lag, but use Bayesian model comparison to determine how good of a fit that actually is, i.e. whether the recovere lag is significant. In this example we show how to get model evidences and see if / when we actually trust our recovered lags.</p> <p>Per usual, start with some imports:</p> <pre><code>import matplotlib.pyplot as plt\n\nfrom litmus import *\n</code></pre> <p>Then define the different models. <code>model_alt</code> is our alternative hypothesis, i.e. \"The signal encodes a lag and we have seen it\". The other two models are null hypothesese, with <code>model_null</code> being \"there is information in the signal but we did not measure a lag\", and <code>model_whitenoise</code> is \"the response signal is so choppy we can't measure anything at all\". For cleanness of the notebook I'm setting al of these to <code>verbose=False</code> to suppress their progress messages:</p> <pre><code>model_alt = litmus.models.GP_simple(verbose=False)\nmodel_null = litmus.models.GP_simple_null(verbose=False)\nmodel_whitenoise = litmus.models.whitenoise_null(verbose=False)\n\nMODELS = [model_alt, model_null, model_whitenoise]\n</code></pre> <p>This is the code used to generate the evidence examples in the <code>LITMUS</code> paper, and here I've got a switch, <code>vague</code>, to flick between two sets of mock signals. If <code>vague=False</code> we generate some nice crisp signals with good cadence, easy to observe timescales of variation and nice measurement uncertainties. If <code>vague=True</code> the uncertainties get larger and the timescale is shortened to make it harder to track the variability. In either case, the procedure here is to generate three mocks, each corresponding to a \"true\" case of one of the model's we're testing. 1. <code>mock</code> has a true lag of $540 \\mathrm{d}$. 2. <code>mock1_null</code> has no true lag, but both driving and response lightcurves have good signal-to-noise. This corresponds to a lag beyond our prior, or a response that's become physically decoupled. 3. <code>mock1_whitenoise</code> encodes no lag and has a response that is degraded to the point of being white noise.</p> <pre><code>vague = False\n\nif not vague:\n    mock1 = litmus.mocks.mock(1, lag=540, tau=200, E=[0.01, 0.1])\n    mock2 = mock1.copy(seed=5)\n    mock3 = litmus.mocks.mock(8, lag=180, tau=0.001, E=[0.01, 0.1])\nelse:\n    mock1 = litmus.mocks.mock(3, lag=540, tau=50, E=[0.2, 0.5])\n    mock2 = mock1.copy(seed=5)\n    mock3 = litmus.mocks.mock(8, lag=180, tau=0.001, E=[0.2, 0.5])\n\nmock1_null, mock1_whitenoise = mock1.copy(), mock1.copy()\nmock1_null.swap_response(mock2)\nmock1_whitenoise.swap_response(mock3)\n\nMOCKS = [mock1, mock1_null, mock1_whitenoise]\nmock1.name = \"Positive\"\nmock1_null.name = \"Negative\"\nmock1_whitenoise.name = \"Noise\"\n\n#----------------------------\nfig, axes = plt.subplots(len(MOCKS), 1, sharex=True, sharey=True)\nfor mock, ax in zip(MOCKS, axes):\n    mock.plot(axis=ax, show=False,\n              true_args={'c': ['navy', 'orchid'], 'label': [None, None]},\n              series_args={'c': ['navy', 'orchid'], 'label': [\"Continuum\", \"Response\"]}\n              )\n    ax.grid()\nfig.supxlabel(\"Time (Days)\"), fig.supylabel(\"Signal (Arb Units)\")\naxes[0].set_title(\"True Lag Response\"), axes[1].set_title(\"Decoupled Response\"), axes[2].set_title(\n    \"White Noise Response\")\naxes[0].legend()\nfig.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>Now nothing to it but to do it. We're going to use the <code>SVI_scan</code> to fit all of the models and calculate their evidences in a 3x3 grid. Notice I've lowered <code>grid_bunching</code> to <code>0.25</code> to make sure the test lags are spread out enough to get a good evidence integral:</p> <pre><code>Z = []\nFITTERS = []\nfor mock in MOCKS:\n    for model in MODELS:\n        fitter = litmus.fitting_methods.SVI_scan(model, Nlags=128, precondition=\"diag\", ELBO_Nsteps=512,\n                                                 grid_bunching=0.25, verbose=False)\n        fitter.name = mock.name + \" \" + model.name\n        fitter.fit(mock.lc_1, mock.lc_2)\n\n        FITTERS.append(fitter)\n\n        Z.append(fitter.get_evidence())\n</code></pre> <p>Before we actually use these fits to do the model comparison, let's see why this is necessary. In addition to getting evidences, <code>LITMUS</code> fitters can also generate MCMC-like chains for parameter constraint. Grabbing the fitters that fit for the alternative model, i.e. the ones that fit for lags, we can plot their constraints on the lag.</p> <p>When we do, we can see that all three mock cases produced what looked like a clear and precise lag, even though only 1/3 of them actually had a real underlying lag to recover. The second and third recoveries are false positives:</p> <pre><code>fig = plt.figure(figsize=(8,4))\nfor i, c in enumerate(['#4169E1','#DA70D6','#FFA07A']):\n    samples = FITTERS[3 * i].get_samples(50_000)\n    plt.hist(samples['lag'], label = FITTERS[3 * i].name, alpha=1., color = c, bins=256, density=True)\nplt.xlim(*model_alt.prior_ranges['lag'])\nplt.axvline(mock1_whitenoise.lag, ls='--', color='navy', label = \"True Lag For Encoded Case\")\nplt.gca().set_yticklabels([])\nplt.ylabel(\"Posterior Density\")\nplt.xlabel(\"Recovered Lag, Days\")\nplt.grid()\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>So how do we identify this failure state? Evidence ratios! We need to know not only what lag was recovered, but whether the model at large does a better job of explaining our data than a null hypothesis. Once you've run a fitter, getting model evidences is as simple as calling <code>fitter.get_evidence()</code>.</p> <p>Evidences in <code>LITMUS</code> are returned in linear form, i.e. not logarithmically, so we need to convert back. Traditionally log-evidences are written in log base 10, and we'll work with that here. Bayes ratios only make sense when comparing models as applied to the same data set, so here we'll plot the relative evidence (i.e. difference in evidence compared to the simplest model) for the 3 different models within each mock.  </p> <p>Plotting these we immediately see that the model with the highest evidence is the one correct one. For the signal with a lag encoded, the alternative model wins out by about $1.7 \\mathrm{dex}$, or a factor of about 50, while the decoupled signal strongly favours the decoupled model, and the white noise response signal has both GP response models disfavoured by an enormous margin.</p> <p>Through evidence-based model comparison we've seperated out the one signal that actually has an encoded lag from the two other false positives.</p> <pre><code>LZ = np.log10(np.array(Z)[:, 0]).reshape(len(MOCKS), len(MODELS))\nLZ = np.round(LZ, 2)\n\nplt.figure(figsize=(8,4))\n\n\nfor i, row in enumerate(LZ - np.tile(LZ[:, 2], [len(MODELS), 1]).T):\n    plt.bar((np.arange(3) + 3 * i)[0], row[0], color='royalblue', label=\"Lag Detected Model\" if i == 0 else None)\n    plt.bar((np.arange(3) + 3 * i)[1], row[1], color='orchid', label=\"Decoupled Model\" if i == 0 else None)\n\nplt.axvspan(2, 5, alpha=0.25, color='royalblue')\nplt.axhline(y=0, color='black')\n\nplt.xlabel(\"Mock Ground Truth\")\nplt.ylabel(\"$\\log_{10}$ of Evidence Ratio vs White Noise Model\")\n\nplt.grid()\nplt.gca().set_xticks([0.5, 3.5, 6.5])\nplt.gca().set_xticklabels([\"Lag Encoded Mock\", \"Decoupled Mock\", \"White Noise\"])\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p>"},{"location":"examples/lightcurve_gentest/lightcurve_gentest/","title":"Constrained Light Curve Example","text":"<p>In this example we show how you can use posterior sample chains to generate nice visualisations of constrained lightcurves. Per usual we begin by importing some modules:</p> <pre><code>import litmus\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom litmus._utils import dict_extend\n</code></pre> <pre><code>An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n</code></pre> <p>Now generate some mock signals a'la OzDES data:</p> <pre><code>mock = litmus.mocks.mock(lag=180,\n                         E=[0.1, 0.25],\n                         tau=400)\nlc_1, lc_2 = mock.lc_1, mock.lc_2\nmock.plot()\nplt.show()\n</code></pre> <p></p> <p>Now create a <code>LITMUS</code> model, specifically the <code>GP_simple</code> model. We'll leave the priors at their defaults for <code>lag</code> and <code>logtau</code>, but restrict the others at their true values to make fitting easier for this simple example.</p> <p>While we're at it, we'll convert the two lightcurves into the right format for the model's data input, and define the time span we want to track the lightcurves over, and make the change of setting the model's logging to <code>verbose=True</code> so we can track its progress.</p> <pre><code>model = litmus.models.GP_simple(verbose=False,\n                                debug=False,\n                                prior_ranges=mock.params() | {'lag': [0, 1_000], 'logtau': [np.log(10), np.log(10000)]})\ndata = model.lc_to_data(lc_1, lc_2)\nTpred = np.linspace(-1000, 2500, 512)\n</code></pre> <p>Light curve constrain works via the stats model itself. We feed in the data to condition the model and some model paramters for constraining the model. Because we're working with a mock, we already know the underlying true model paramters, and can get them from <code>mock.params()</code>:</p> <pre><code>p = mock.params()\npred1_true, pred2_true = model.make_lightcurves(data, params=p, Tpred=Tpred, num_samples=1)\n</code></pre> <p>Because we're going to run the code again we'll wrap the plot generation in a function:</p> <pre><code>def make_plot(pred1, pred2):\n    print(\"Predictions Clear. Plotting\")\n    c0 = 'midnightblue'\n    c1, c2 = 'navy', 'orchid'\n\n    f, (a1, a2) = plt.subplots(2, 1, sharex=True, figsize=(8, 6))\n    lc_1.plot(axis=a1, show=False, c=c1, capsize=2, label=\"Measurements\")\n    lc_2.plot(axis=a2, show=False, c=c2, capsize=2, label=\"Measurements\")\n\n    a1.plot(mock.lc.T, mock.lc.Y, c=c0, lw=0.5, zorder=-6, label='True Signal')\n    a2.plot(mock.lc.T + mock.lag, mock.lc.Y, c=c0, lw=0.5, zorder=-6)\n\n    a1.fill_between(pred1.T, pred1.Y - pred1.E, pred1.Y + pred1.E, alpha=0.25, color=c1,\n                    label=\"Constrained Lightcurve, Continuum\")\n    a2.fill_between(pred2.T, pred2.Y - pred2.E, pred2.Y + pred2.E, alpha=0.25, color=c2,\n                    label=\"Constrained Lightcurve, Response\")\n    a1.fill_between(pred1.T, pred1.Y - 2 * pred1.E, pred1.Y + 2 * pred1.E, alpha=0.125, color=c1)\n    a2.fill_between(pred2.T, pred2.Y - 2 * pred2.E, pred2.Y + 2 * pred2.E, alpha=0.125, color=c2)\n\n    r = 0.1\n    a1.fill_between(pred1.T, pred1.Y - 2 * pred1.E - r, pred1.Y + 2 * pred1.E + r, alpha=1.0, color='w', zorder=-9)\n    a2.fill_between(pred2.T, pred2.Y - 2 * pred2.E - r, pred2.Y + 2 * pred2.E + r, alpha=1.0, color='w', zorder=-9)\n\n    f.supxlabel(\"Time (Days)\")\n    f.supylabel(\"Signal (Arb Units)\")\n\n    for a in a1, a2:\n        a.grid()\n        a.set_yticklabels([])\n        a.set_ylim(-3, 3)\n\n        if mock.season != 0:\n            tmax = Tpred.max()\n            nyears = int(tmax // (mock.season * 2) + 1)\n            for i in range(nyears):\n                a.axvspan((i + 1 / 2 - 1 / 2) * mock.season * 2, (i + 1 - 1 / 2) * mock.season * 2,\n                          ymin=0, ymax=1, alpha=0.125, color='royalblue',\n                          zorder=-10,\n                          label=None)\n            a.legend()\n\n    a1.set_xlim(0, 2000)\n    f.tight_layout()\n\n    print(\"Plots done\")\n\n    return (f)\n</code></pre> <p>And run it:</p> <pre><code>f = make_plot(pred1_true, pred2_true)\nf.suptitle(\"Constrained Lightcurves at true values\")\nf.tight_layout()\nf.show()\n</code></pre> <pre><code>Predictions Clear. Plotting\nPlots done\n</code></pre> <p></p> <p>That's not all though, we can also feed in an MCMC-like chain of parameters to have <code>model.make_lightcurve</code> marginalize over the posterior. First we'll need to actually set the </p> <pre><code>my_fitter = litmus.fitting_methods.hessian_scan(model, verbose=False, debug=False, grid_bunching = 0.25, Nlags = 128)\nmy_fitter.fit(lc_1, lc_2)\np_varied = my_fitter.get_samples()\n</code></pre> <p>We'll wrap a <code>LITMUS</code> plotter around this method to check what the parameters are doing and make sure we have a nice result. We can notice that, while the parameters are well constrained, the timescale skews slightly above the $\\log\\vert400 \\mathrm{d} \\vert=5.99$ of the underlying mock signal:</p> <pre><code>lt = litmus.LITMUS(my_fitter)\nlt.plot_parameters(prior_extents = True)\nplt.show()\n</code></pre> <pre><code>Warning! LITMUS object built on pre-run fitting_procedure. May have unexpected behaviour. WARNING:chainconsumer:Parameter lag in chain Lightcurves %i-%i is not constrained\n</code></pre> <p></p> <p>What does this mean for the constraints on the lightcurve? Well, two effects are present:  1. The uncertainty in the lag smudges the response lightcurve laterally 2. The skewing of the timescale towards larger values means our predictions are smoothed out The net result is that the final constrained lightcurve is smoother and blurrier in the time-axis when we account for our measurement uncertainties. How do we test this with <code>LITMUS</code>? Simple: we <code>.get_samples()</code> from <code>my_fitter</code> to get an MCMC-like chain and feed this to <code>make_lightcurves</code> before plotting: </p> <pre><code>p_varied = my_fitter.get_samples(N=512)\npred1_vary, pred2_vary = model.make_lightcurves(data, params=p_varied, Tpred=Tpred, num_samples=64)\nf = make_plot(pred1_vary, pred2_vary)\nf.suptitle(\"Constrained lightcurve at varied / fit parameters\")\nf.tight_layout()\nf.show()\n</code></pre> <pre><code>Predictions Clear. Plotting\nPlots done\n</code></pre> <p></p> <p>We can see that the plots are smoothed out by the uncertainty in $\\Delta t$ and $\\tau$, which skews somewhat towards longer timescales.</p>"}]}